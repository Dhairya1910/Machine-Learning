{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70cd4fe3",
   "metadata": {},
   "source": [
    "# Full fledge neural net with hyperparameter and droput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6565dba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "442fe161",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNeuralNetwork:\n",
    "    def __init__(self,input_size,output_size,hidden_size,epoches=1000,learning_rate=0.001,dropout=0.5,hidden_layers=2):\n",
    "        self.input_size = input_size \n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size \n",
    "        self.learning_rate = learning_rate \n",
    "        self.dropout = dropout \n",
    "        self.hidden_layers = hidden_layers \n",
    "        self.epoches = epoches\n",
    "\n",
    "        self.hidden_weights = [np.random.randn(self.input_size,self.hidden_size)* np.square(2/self.input_size)]\n",
    "        self.hidden_bias = [np.zeros((1,self.hidden_size))]\n",
    "        \n",
    "        for _ in range(self.hidden_layers): \n",
    "            self.hidden_weights.append(np.random.randn(self.hidden_size,self.hidden_size) * np.square(2/self.input_size))\n",
    "            self.hidden_bias.append(np.zeros((1,self.hidden_size)))\n",
    "        \n",
    "        self.output_weight = np.random.randn(self.hidden_size,self.output_size)\n",
    "        self.output_bias = np.zeros((1,self.output_size))\n",
    "\n",
    "    def sigmoid(self,z):\n",
    "        return (1/(1+np.exp(-z)))\n",
    "    \n",
    "    def derivative_sigmoid(self,z):\n",
    "        s = self.sigmoid(z)\n",
    "        return s * (1-s)\n",
    "    \n",
    "    def Relu(self,z):\n",
    "        return np.maximum(0,z)\n",
    "\n",
    "    def derivative_relu(self,z):\n",
    "        return (z > 0 ).astype(float)\n",
    "    \n",
    "    def compute_loss(self,preds,Y):\n",
    "        preds = np.clip(preds,1e-8,1-1e-8) \n",
    "        return -np.mean(Y*np.log(preds)+(1-Y)*np.log(1-preds)) \n",
    "    \n",
    "    def Dropout(self,A):\n",
    "        mask = np.random.rand(*A.shape) < self.dropout\n",
    "        return (mask * A ) / self.dropout\n",
    "    \n",
    "    def ForwardPropagation(self,X):\n",
    "        self.Activations = [X]\n",
    "        self.hidden_Z = []\n",
    "\n",
    "        for l in range(self.hidden_layers):\n",
    "            Z = np.dot(self.Activations[l],self.hidden_weights[l]) + self.hidden_bias[l] \n",
    "            self.hidden_Z.append(Z)\n",
    "            A = self.Relu(Z) \n",
    "            self.Activations.append(self.Dropout(A))\n",
    "\n",
    "        self.output_Z = np.dot(self.Activations[-1],self.output_weight) + self.output_bias \n",
    "        self.output_A = self.sigmoid(self.output_Z) \n",
    "        return self.output_A\n",
    "\n",
    "    def BackPropagation(self,X,Y):\n",
    "        m = X.shape[0]\n",
    "\n",
    "        error = self.output_A - Y \n",
    "        Dwo = 1/m * np.dot(self.Activations[-1].T,error)\n",
    "        Dbo = 1/m * np.sum(error) \n",
    "\n",
    "        self.output_weight =  self.output_weight - self.learning_rate * Dwo\n",
    "        self.output_bias = self.output_bias - self.learning_rate * Dbo \n",
    "\n",
    "        for l in reversed(range(self.hidden_layers)):\n",
    "            \n",
    "            da = np.dot(error,self.output_weight.T if l == self.hidden_layers-1 else self.hidden_weights[l+1].T)\n",
    "            dz = da * self.derivative_relu(self.hidden_Z[l]) # final errors \n",
    "\n",
    "            dw = 1/m * np.dot(self.Activations[l].T,dz)\n",
    "            db = 1/m * np.sum(dz)\n",
    "            error = dz \n",
    "\n",
    "            self.hidden_weights[l] = self.hidden_weights[l] - self.learning_rate * dw \n",
    "            self.hidden_bias[l] = self.hidden_bias[l] - self.learning_rate * db  \n",
    "\n",
    "    def fit(self,X,Y):\n",
    "        Y = np.array(Y).reshape(-1,1)\n",
    "        for epoch in range(self.epoches):\n",
    "                preds_F = self.ForwardPropagation(X)\n",
    "                loss = self.compute_loss(preds_F,Y)\n",
    "                print(f\"epoch : {epoch+1}, loss : {loss}\")\n",
    "                self.BackPropagation(X,Y)\n",
    "\n",
    "    def predict(self,X):\n",
    "        return (self.ForwardPropagation(X) > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17900ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.62160</td>\n",
       "      <td>8.6661</td>\n",
       "      <td>-2.8073</td>\n",
       "      <td>-0.44699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.1674</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.6383</td>\n",
       "      <td>1.9242</td>\n",
       "      <td>0.10645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.5228</td>\n",
       "      <td>-4.0112</td>\n",
       "      <td>-3.59440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.4552</td>\n",
       "      <td>4.5718</td>\n",
       "      <td>-0.98880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variance  skewness  curtosis  entropy  class\n",
       "0   3.62160    8.6661   -2.8073 -0.44699      0\n",
       "1   4.54590    8.1674   -2.4586 -1.46210      0\n",
       "2   3.86600   -2.6383    1.9242  0.10645      0\n",
       "3   3.45660    9.5228   -4.0112 -3.59440      0\n",
       "4   0.32924   -4.4552    4.5718 -0.98880      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "dataset = pd.read_csv(\"../BankNote_Authentication.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05df4745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1029, 1)\n"
     ]
    }
   ],
   "source": [
    "Y = np.array(dataset['class']).reshape(-1,1)\n",
    "X = np.array(dataset.drop(columns=['class']))\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y)\n",
    "\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd1d14ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1, loss : 1.2792504663329718\n",
      "epoch : 2, loss : 1.2314376243041032\n",
      "epoch : 3, loss : 1.3006223038444984\n",
      "epoch : 4, loss : 1.2203110734415454\n",
      "epoch : 5, loss : 1.2142525152371864\n",
      "epoch : 6, loss : 1.246200205045712\n",
      "epoch : 7, loss : 1.1720346168953786\n",
      "epoch : 8, loss : 1.1939521298107185\n",
      "epoch : 9, loss : 1.0952274519207967\n",
      "epoch : 10, loss : 1.1894615967985167\n",
      "epoch : 11, loss : 1.1486202222728061\n",
      "epoch : 12, loss : 1.202539944648462\n",
      "epoch : 13, loss : 1.1924041366507938\n",
      "epoch : 14, loss : 1.136154391755598\n",
      "epoch : 15, loss : 1.104259753562609\n",
      "epoch : 16, loss : 1.1954462641977706\n",
      "epoch : 17, loss : 1.1985720021074073\n",
      "epoch : 18, loss : 1.2444686869124832\n",
      "epoch : 19, loss : 1.120915316805712\n",
      "epoch : 20, loss : 1.1283629043358083\n",
      "epoch : 21, loss : 1.0416028277844622\n",
      "epoch : 22, loss : 1.1445422197371147\n",
      "epoch : 23, loss : 1.0461343925890583\n",
      "epoch : 24, loss : 1.165013224330579\n",
      "epoch : 25, loss : 1.0859687496153203\n",
      "epoch : 26, loss : 1.1383344666600572\n",
      "epoch : 27, loss : 1.1725162633968889\n",
      "epoch : 28, loss : 1.0990721433205672\n",
      "epoch : 29, loss : 1.1038831883879243\n",
      "epoch : 30, loss : 1.0937500903192068\n",
      "epoch : 31, loss : 1.0628678060038705\n",
      "epoch : 32, loss : 1.0750062830283467\n",
      "epoch : 33, loss : 1.0601151249916605\n",
      "epoch : 34, loss : 1.0329264577579251\n",
      "epoch : 35, loss : 1.0974784893498968\n",
      "epoch : 36, loss : 1.0117591293699084\n",
      "epoch : 37, loss : 1.0116676480870919\n",
      "epoch : 38, loss : 1.00408516200888\n",
      "epoch : 39, loss : 1.025442872256076\n",
      "epoch : 40, loss : 1.024724243884307\n",
      "epoch : 41, loss : 1.0248846918247774\n",
      "epoch : 42, loss : 1.037198156338879\n",
      "epoch : 43, loss : 1.0001625822544307\n",
      "epoch : 44, loss : 0.9651798363613403\n",
      "epoch : 45, loss : 0.9615162041428197\n",
      "epoch : 46, loss : 1.066409822395709\n",
      "epoch : 47, loss : 1.0283986336142672\n",
      "epoch : 48, loss : 0.9798713218175659\n",
      "epoch : 49, loss : 0.9577055940636093\n",
      "epoch : 50, loss : 1.0520588795109578\n",
      "epoch : 51, loss : 0.975510682401494\n",
      "epoch : 52, loss : 0.994778332157757\n",
      "epoch : 53, loss : 0.9771143525170568\n",
      "epoch : 54, loss : 0.9337564361513522\n",
      "epoch : 55, loss : 0.943988519788593\n",
      "epoch : 56, loss : 0.9216419989532776\n",
      "epoch : 57, loss : 0.923166090892266\n",
      "epoch : 58, loss : 0.9935900747742812\n",
      "epoch : 59, loss : 0.9512240158235531\n",
      "epoch : 60, loss : 0.9718983255711857\n",
      "epoch : 61, loss : 0.9716136280935468\n",
      "epoch : 62, loss : 0.9609403802510518\n",
      "epoch : 63, loss : 0.9729729706953342\n",
      "epoch : 64, loss : 0.9810312273266195\n",
      "epoch : 65, loss : 0.8755778710740171\n",
      "epoch : 66, loss : 0.9298911272750197\n",
      "epoch : 67, loss : 0.9157148614973954\n",
      "epoch : 68, loss : 1.001268268500489\n",
      "epoch : 69, loss : 0.8805404766434579\n",
      "epoch : 70, loss : 0.9564217136954628\n",
      "epoch : 71, loss : 0.9107437359739194\n",
      "epoch : 72, loss : 0.9411538564017229\n",
      "epoch : 73, loss : 0.957455329920905\n",
      "epoch : 74, loss : 0.9507768673907275\n",
      "epoch : 75, loss : 0.930029576294686\n",
      "epoch : 76, loss : 0.8989464444165733\n",
      "epoch : 77, loss : 1.0053950474598907\n",
      "epoch : 78, loss : 0.9434781617957301\n",
      "epoch : 79, loss : 0.8698583486419245\n",
      "epoch : 80, loss : 0.9364935353230044\n",
      "epoch : 81, loss : 0.9049519799267726\n",
      "epoch : 82, loss : 0.8786422897995448\n",
      "epoch : 83, loss : 0.8860472138119452\n",
      "epoch : 84, loss : 0.901487554257293\n",
      "epoch : 85, loss : 0.9336924788526151\n",
      "epoch : 86, loss : 0.8895807913774931\n",
      "epoch : 87, loss : 0.9471474623692028\n",
      "epoch : 88, loss : 0.9683934018625847\n",
      "epoch : 89, loss : 0.9161343076258747\n",
      "epoch : 90, loss : 0.9182583640652959\n",
      "epoch : 91, loss : 0.9128369378230111\n",
      "epoch : 92, loss : 0.8918875855162095\n",
      "epoch : 93, loss : 0.9401738622755264\n",
      "epoch : 94, loss : 0.8745623741872136\n",
      "epoch : 95, loss : 0.8849527162295554\n",
      "epoch : 96, loss : 0.8375352057534359\n",
      "epoch : 97, loss : 0.8650507203744948\n",
      "epoch : 98, loss : 0.9481763752881884\n",
      "epoch : 99, loss : 0.8582929259251504\n",
      "epoch : 100, loss : 0.9605222235265741\n",
      "epoch : 101, loss : 0.8890754784462436\n",
      "epoch : 102, loss : 0.8374090734680979\n",
      "epoch : 103, loss : 0.8507480020940656\n",
      "epoch : 104, loss : 0.8325615674135722\n",
      "epoch : 105, loss : 0.8501191726491893\n",
      "epoch : 106, loss : 0.8832795834678765\n",
      "epoch : 107, loss : 0.8965122378954737\n",
      "epoch : 108, loss : 0.8290084475895315\n",
      "epoch : 109, loss : 0.8988901292903011\n",
      "epoch : 110, loss : 0.8625449673350752\n",
      "epoch : 111, loss : 0.9230922936594665\n",
      "epoch : 112, loss : 0.8939495788753076\n",
      "epoch : 113, loss : 0.8852333347668538\n",
      "epoch : 114, loss : 0.9078600332598564\n",
      "epoch : 115, loss : 0.8492244896005416\n",
      "epoch : 116, loss : 0.8226076606931368\n",
      "epoch : 117, loss : 0.8465105722589467\n",
      "epoch : 118, loss : 0.8887726571457152\n",
      "epoch : 119, loss : 0.8810827136646385\n",
      "epoch : 120, loss : 0.9182265621776409\n",
      "epoch : 121, loss : 0.8800653657145656\n",
      "epoch : 122, loss : 0.8600705746210744\n",
      "epoch : 123, loss : 0.9257303881960838\n",
      "epoch : 124, loss : 0.8714499660014634\n",
      "epoch : 125, loss : 0.8454809171501365\n",
      "epoch : 126, loss : 0.8605633008381202\n",
      "epoch : 127, loss : 0.8770582058126489\n",
      "epoch : 128, loss : 0.8655293812054945\n",
      "epoch : 129, loss : 0.8253546636495518\n",
      "epoch : 130, loss : 0.8122673391765282\n",
      "epoch : 131, loss : 0.8432550702498649\n",
      "epoch : 132, loss : 0.8788144615830862\n",
      "epoch : 133, loss : 0.8191245692982995\n",
      "epoch : 134, loss : 0.8340832079407375\n",
      "epoch : 135, loss : 0.8379473060791619\n",
      "epoch : 136, loss : 0.8237886762640011\n",
      "epoch : 137, loss : 0.8370394605582325\n",
      "epoch : 138, loss : 0.8947915301696673\n",
      "epoch : 139, loss : 0.8405974320049852\n",
      "epoch : 140, loss : 0.8578213871867043\n",
      "epoch : 141, loss : 0.8448049456752075\n",
      "epoch : 142, loss : 0.8103772212489865\n",
      "epoch : 143, loss : 0.8358084492549964\n",
      "epoch : 144, loss : 0.8818682619703888\n",
      "epoch : 145, loss : 0.8294370541588609\n",
      "epoch : 146, loss : 0.8082703593502296\n",
      "epoch : 147, loss : 0.7992057455099371\n",
      "epoch : 148, loss : 0.8378213848889738\n",
      "epoch : 149, loss : 0.7933694988899538\n",
      "epoch : 150, loss : 0.8199596152143266\n",
      "epoch : 151, loss : 0.8592797239905635\n",
      "epoch : 152, loss : 0.8261962784976982\n",
      "epoch : 153, loss : 0.7977001772585477\n",
      "epoch : 154, loss : 0.8088420405488715\n",
      "epoch : 155, loss : 0.861330709021378\n",
      "epoch : 156, loss : 0.8222437865796492\n",
      "epoch : 157, loss : 0.8237167226304337\n",
      "epoch : 158, loss : 0.7914659502115041\n",
      "epoch : 159, loss : 0.7922411760932312\n",
      "epoch : 160, loss : 0.8131789970920521\n",
      "epoch : 161, loss : 0.8283908626093676\n",
      "epoch : 162, loss : 0.7813078423947809\n",
      "epoch : 163, loss : 0.8044394322297599\n",
      "epoch : 164, loss : 0.8302105554629319\n",
      "epoch : 165, loss : 0.8072801065300396\n",
      "epoch : 166, loss : 0.8366318638701747\n",
      "epoch : 167, loss : 0.8229948088288678\n",
      "epoch : 168, loss : 0.7904864561727191\n",
      "epoch : 169, loss : 0.7983921708747783\n",
      "epoch : 170, loss : 0.7892832812053814\n",
      "epoch : 171, loss : 0.8322791058441343\n",
      "epoch : 172, loss : 0.8183080202429005\n",
      "epoch : 173, loss : 0.7772590848128058\n",
      "epoch : 174, loss : 0.7951713364151941\n",
      "epoch : 175, loss : 0.8191336787518818\n",
      "epoch : 176, loss : 0.8656434386553831\n",
      "epoch : 177, loss : 0.8061063316737879\n",
      "epoch : 178, loss : 0.8098886109847532\n",
      "epoch : 179, loss : 0.8063967901445739\n",
      "epoch : 180, loss : 0.8288999241579953\n",
      "epoch : 181, loss : 0.818541923895122\n",
      "epoch : 182, loss : 0.833414949592497\n",
      "epoch : 183, loss : 0.8159622333101365\n",
      "epoch : 184, loss : 0.8235938039794451\n",
      "epoch : 185, loss : 0.8599183657562414\n",
      "epoch : 186, loss : 0.8381661305976486\n",
      "epoch : 187, loss : 0.7988135807382867\n",
      "epoch : 188, loss : 0.8195126024480118\n",
      "epoch : 189, loss : 0.8100662523272117\n",
      "epoch : 190, loss : 0.7897211060693962\n",
      "epoch : 191, loss : 0.8694590394044589\n",
      "epoch : 192, loss : 0.7888273070048082\n",
      "epoch : 193, loss : 0.801751177452004\n",
      "epoch : 194, loss : 0.8168234611231792\n",
      "epoch : 195, loss : 0.7806973465962055\n",
      "epoch : 196, loss : 0.7977691949323304\n",
      "epoch : 197, loss : 0.8083031356930394\n",
      "epoch : 198, loss : 0.832605117113727\n",
      "epoch : 199, loss : 0.8500247370812138\n",
      "epoch : 200, loss : 0.8077505751673475\n",
      "epoch : 201, loss : 0.7785828903162316\n",
      "epoch : 202, loss : 0.8384373558673491\n",
      "epoch : 203, loss : 0.7953570955030798\n",
      "epoch : 204, loss : 0.8328144027269743\n",
      "epoch : 205, loss : 0.8073602016840387\n",
      "epoch : 206, loss : 0.7622379862334322\n",
      "epoch : 207, loss : 0.7951748592678846\n",
      "epoch : 208, loss : 0.7992105796578522\n",
      "epoch : 209, loss : 0.7966901283882581\n",
      "epoch : 210, loss : 0.8401524968783117\n",
      "epoch : 211, loss : 0.8043763401126268\n",
      "epoch : 212, loss : 0.8045210074074602\n",
      "epoch : 213, loss : 0.8061278073101802\n",
      "epoch : 214, loss : 0.7784375566565356\n",
      "epoch : 215, loss : 0.7639032075268404\n",
      "epoch : 216, loss : 0.8173490403774848\n",
      "epoch : 217, loss : 0.8128844297418504\n",
      "epoch : 218, loss : 0.7914376004233695\n",
      "epoch : 219, loss : 0.77416146034018\n",
      "epoch : 220, loss : 0.7976737956756891\n",
      "epoch : 221, loss : 0.7892827865992661\n",
      "epoch : 222, loss : 0.8026776613411591\n",
      "epoch : 223, loss : 0.7851458453481266\n",
      "epoch : 224, loss : 0.8075237578217321\n",
      "epoch : 225, loss : 0.734091021846533\n",
      "epoch : 226, loss : 0.7884320913227424\n",
      "epoch : 227, loss : 0.7528293348760705\n",
      "epoch : 228, loss : 0.7531580885572652\n",
      "epoch : 229, loss : 0.8326954267864738\n",
      "epoch : 230, loss : 0.8080988093299203\n",
      "epoch : 231, loss : 0.8027221919445077\n",
      "epoch : 232, loss : 0.8746274109237827\n",
      "epoch : 233, loss : 0.7636489283268263\n",
      "epoch : 234, loss : 0.7984121439724816\n",
      "epoch : 235, loss : 0.7721168312617674\n",
      "epoch : 236, loss : 0.7680899924366646\n",
      "epoch : 237, loss : 0.7944189822967279\n",
      "epoch : 238, loss : 0.7970373203553828\n",
      "epoch : 239, loss : 0.7826436058618608\n",
      "epoch : 240, loss : 0.7930410135708859\n",
      "epoch : 241, loss : 0.7868430125846194\n",
      "epoch : 242, loss : 0.7689404233077398\n",
      "epoch : 243, loss : 0.7969428421235835\n",
      "epoch : 244, loss : 0.7858333005696232\n",
      "epoch : 245, loss : 0.7482872581910154\n",
      "epoch : 246, loss : 0.7387385989963372\n",
      "epoch : 247, loss : 0.7682912542453907\n",
      "epoch : 248, loss : 0.7767351443132098\n",
      "epoch : 249, loss : 0.7835843277147511\n",
      "epoch : 250, loss : 0.7586943221757183\n",
      "epoch : 251, loss : 0.7889696638536848\n",
      "epoch : 252, loss : 0.7645615993891872\n",
      "epoch : 253, loss : 0.7666043410805197\n",
      "epoch : 254, loss : 0.7840003033895491\n",
      "epoch : 255, loss : 0.7854485845595628\n",
      "epoch : 256, loss : 0.781912392106016\n",
      "epoch : 257, loss : 0.7916680105732936\n",
      "epoch : 258, loss : 0.8142180183162726\n",
      "epoch : 259, loss : 0.7912109519978142\n",
      "epoch : 260, loss : 0.7821138092674721\n",
      "epoch : 261, loss : 0.8118428781909849\n",
      "epoch : 262, loss : 0.7720917366499039\n",
      "epoch : 263, loss : 0.7861155026030626\n",
      "epoch : 264, loss : 0.7639272568147849\n",
      "epoch : 265, loss : 0.8009611579486993\n",
      "epoch : 266, loss : 0.7699584751840773\n",
      "epoch : 267, loss : 0.7938693595364996\n",
      "epoch : 268, loss : 0.7676376219928362\n",
      "epoch : 269, loss : 0.7966500440064399\n",
      "epoch : 270, loss : 0.7637656229878033\n",
      "epoch : 271, loss : 0.8038681401498331\n",
      "epoch : 272, loss : 0.7647265000338158\n",
      "epoch : 273, loss : 0.7768379701184678\n",
      "epoch : 274, loss : 0.7409038363414989\n",
      "epoch : 275, loss : 0.7800535597601643\n",
      "epoch : 276, loss : 0.7662182630077224\n",
      "epoch : 277, loss : 0.7704216463074925\n",
      "epoch : 278, loss : 0.7251455468663972\n",
      "epoch : 279, loss : 0.7420567847474723\n",
      "epoch : 280, loss : 0.7487190724127146\n",
      "epoch : 281, loss : 0.7545905178603345\n",
      "epoch : 282, loss : 0.8308539972077856\n",
      "epoch : 283, loss : 0.7904825692811411\n",
      "epoch : 284, loss : 0.7154430629984758\n",
      "epoch : 285, loss : 0.761478581012768\n",
      "epoch : 286, loss : 0.7349855079554395\n",
      "epoch : 287, loss : 0.7546699677589569\n",
      "epoch : 288, loss : 0.7880314357647145\n",
      "epoch : 289, loss : 0.7604852766791161\n",
      "epoch : 290, loss : 0.8006049547238764\n",
      "epoch : 291, loss : 0.699476397498799\n",
      "epoch : 292, loss : 0.7862327992227381\n",
      "epoch : 293, loss : 0.7896812515447388\n",
      "epoch : 294, loss : 0.7895392121628121\n",
      "epoch : 295, loss : 0.8460757393123173\n",
      "epoch : 296, loss : 0.7829699568055808\n",
      "epoch : 297, loss : 0.7618228859720653\n",
      "epoch : 298, loss : 0.7574750269968532\n",
      "epoch : 299, loss : 0.7108302127854536\n",
      "epoch : 300, loss : 0.7263889339656652\n",
      "epoch : 301, loss : 0.7110883595697621\n",
      "epoch : 302, loss : 0.7710454156056528\n",
      "epoch : 303, loss : 0.8086184032115279\n",
      "epoch : 304, loss : 0.7457975097243263\n",
      "epoch : 305, loss : 0.741756341840671\n",
      "epoch : 306, loss : 0.7643646646851916\n",
      "epoch : 307, loss : 0.7611408215635016\n",
      "epoch : 308, loss : 0.772396282817422\n",
      "epoch : 309, loss : 0.7265610663387964\n",
      "epoch : 310, loss : 0.7180291286002467\n",
      "epoch : 311, loss : 0.7143507509790641\n",
      "epoch : 312, loss : 0.7302451206871032\n",
      "epoch : 313, loss : 0.7428014920063383\n",
      "epoch : 314, loss : 0.7478365587464418\n",
      "epoch : 315, loss : 0.7156017470504864\n",
      "epoch : 316, loss : 0.7563675115618281\n",
      "epoch : 317, loss : 0.7302965708273952\n",
      "epoch : 318, loss : 0.7197713706438437\n",
      "epoch : 319, loss : 0.7331600192137254\n",
      "epoch : 320, loss : 0.7220650555547716\n",
      "epoch : 321, loss : 0.6986459053181607\n",
      "epoch : 322, loss : 0.7928488830342105\n",
      "epoch : 323, loss : 0.7303863856532976\n",
      "epoch : 324, loss : 0.7585166547366272\n",
      "epoch : 325, loss : 0.7593617622579396\n",
      "epoch : 326, loss : 0.7622283126472917\n",
      "epoch : 327, loss : 0.7403247877310405\n",
      "epoch : 328, loss : 0.736220276957046\n",
      "epoch : 329, loss : 0.7061628543912399\n",
      "epoch : 330, loss : 0.7180263116972322\n",
      "epoch : 331, loss : 0.7183620362307434\n",
      "epoch : 332, loss : 0.7085657565204081\n",
      "epoch : 333, loss : 0.7603983082155934\n",
      "epoch : 334, loss : 0.7597910817766117\n",
      "epoch : 335, loss : 0.7763716188166229\n",
      "epoch : 336, loss : 0.7219553221316172\n",
      "epoch : 337, loss : 0.7214841347302164\n",
      "epoch : 338, loss : 0.7666292300996704\n",
      "epoch : 339, loss : 0.716140172142803\n",
      "epoch : 340, loss : 0.7712975965266271\n",
      "epoch : 341, loss : 0.6699578871361176\n",
      "epoch : 342, loss : 0.7389332388844508\n",
      "epoch : 343, loss : 0.7672308566143505\n",
      "epoch : 344, loss : 0.7577439433013101\n",
      "epoch : 345, loss : 0.7693662672196804\n",
      "epoch : 346, loss : 0.7684033073773542\n",
      "epoch : 347, loss : 0.7711154477504013\n",
      "epoch : 348, loss : 0.74900138686344\n",
      "epoch : 349, loss : 0.7572062199683367\n",
      "epoch : 350, loss : 0.7598239628568046\n",
      "epoch : 351, loss : 0.735568227111539\n",
      "epoch : 352, loss : 0.6836198276879529\n",
      "epoch : 353, loss : 0.6925998474074111\n",
      "epoch : 354, loss : 0.739324041376875\n",
      "epoch : 355, loss : 0.7577486128674937\n",
      "epoch : 356, loss : 0.7205550366856016\n",
      "epoch : 357, loss : 0.7224090328979157\n",
      "epoch : 358, loss : 0.7367717314650358\n",
      "epoch : 359, loss : 0.7541780945383783\n",
      "epoch : 360, loss : 0.7796491837029255\n",
      "epoch : 361, loss : 0.7424976140655859\n",
      "epoch : 362, loss : 0.7404160244457056\n",
      "epoch : 363, loss : 0.7135795511793162\n",
      "epoch : 364, loss : 0.6754675996487828\n",
      "epoch : 365, loss : 0.7253382067461698\n",
      "epoch : 366, loss : 0.7490362368188807\n",
      "epoch : 367, loss : 0.7310398824845464\n",
      "epoch : 368, loss : 0.709975591179189\n",
      "epoch : 369, loss : 0.7650613818317318\n",
      "epoch : 370, loss : 0.7343589329057091\n",
      "epoch : 371, loss : 0.7337899690898999\n",
      "epoch : 372, loss : 0.7314097401323589\n",
      "epoch : 373, loss : 0.7413937900915648\n",
      "epoch : 374, loss : 0.7004275454629806\n",
      "epoch : 375, loss : 0.7221827247415861\n",
      "epoch : 376, loss : 0.6808864561905561\n",
      "epoch : 377, loss : 0.714667668951522\n",
      "epoch : 378, loss : 0.7242876616367218\n",
      "epoch : 379, loss : 0.7214218614390357\n",
      "epoch : 380, loss : 0.7336333566580997\n",
      "epoch : 381, loss : 0.7019619344306625\n",
      "epoch : 382, loss : 0.7468916930231017\n",
      "epoch : 383, loss : 0.7177756339663843\n",
      "epoch : 384, loss : 0.7007737217331045\n",
      "epoch : 385, loss : 0.7280446958074267\n",
      "epoch : 386, loss : 0.7007555174193896\n",
      "epoch : 387, loss : 0.7068385356030057\n",
      "epoch : 388, loss : 0.721051170550255\n",
      "epoch : 389, loss : 0.7123210584764033\n",
      "epoch : 390, loss : 0.7904213225498778\n",
      "epoch : 391, loss : 0.7169835726660373\n",
      "epoch : 392, loss : 0.7000978130042772\n",
      "epoch : 393, loss : 0.7119391711895864\n",
      "epoch : 394, loss : 0.7226996616077851\n",
      "epoch : 395, loss : 0.7221252194248442\n",
      "epoch : 396, loss : 0.7305151403611645\n",
      "epoch : 397, loss : 0.7257146770085556\n",
      "epoch : 398, loss : 0.7109428679334365\n",
      "epoch : 399, loss : 0.6815229552114435\n",
      "epoch : 400, loss : 0.7094925765359011\n",
      "epoch : 401, loss : 0.746458951409203\n",
      "epoch : 402, loss : 0.6879525037214577\n",
      "epoch : 403, loss : 0.6642259396354053\n",
      "epoch : 404, loss : 0.7186493602924655\n",
      "epoch : 405, loss : 0.7501416479214356\n",
      "epoch : 406, loss : 0.7412871644611116\n",
      "epoch : 407, loss : 0.7138997856767706\n",
      "epoch : 408, loss : 0.7351037934453987\n",
      "epoch : 409, loss : 0.7346352312871801\n",
      "epoch : 410, loss : 0.741994886667242\n",
      "epoch : 411, loss : 0.7002239693214772\n",
      "epoch : 412, loss : 0.7003129956010145\n",
      "epoch : 413, loss : 0.7460392779981346\n",
      "epoch : 414, loss : 0.7044319323090148\n",
      "epoch : 415, loss : 0.6861123821361066\n",
      "epoch : 416, loss : 0.729938264762756\n",
      "epoch : 417, loss : 0.7362207627991999\n",
      "epoch : 418, loss : 0.7265396694297055\n",
      "epoch : 419, loss : 0.7378429224673041\n",
      "epoch : 420, loss : 0.7077027724737874\n",
      "epoch : 421, loss : 0.6993235695830595\n",
      "epoch : 422, loss : 0.731225967367565\n",
      "epoch : 423, loss : 0.718281507221083\n",
      "epoch : 424, loss : 0.7135791357652206\n",
      "epoch : 425, loss : 0.7195757735836065\n",
      "epoch : 426, loss : 0.7115197001861651\n",
      "epoch : 427, loss : 0.732257386430701\n",
      "epoch : 428, loss : 0.7347989165711105\n",
      "epoch : 429, loss : 0.7001388231137091\n",
      "epoch : 430, loss : 0.7202403038965038\n",
      "epoch : 431, loss : 0.76909788563903\n",
      "epoch : 432, loss : 0.7650920490418397\n",
      "epoch : 433, loss : 0.7195863421995943\n",
      "epoch : 434, loss : 0.6803372811396416\n",
      "epoch : 435, loss : 0.6990927249202479\n",
      "epoch : 436, loss : 0.7341428666823\n",
      "epoch : 437, loss : 0.711224352245161\n",
      "epoch : 438, loss : 0.7364042997651957\n",
      "epoch : 439, loss : 0.6935814968749724\n",
      "epoch : 440, loss : 0.7955097514321304\n",
      "epoch : 441, loss : 0.7030987459453513\n",
      "epoch : 442, loss : 0.6985162395453993\n",
      "epoch : 443, loss : 0.7107854675475891\n",
      "epoch : 444, loss : 0.7027942952158676\n",
      "epoch : 445, loss : 0.7160030970909661\n",
      "epoch : 446, loss : 0.6997125253353826\n",
      "epoch : 447, loss : 0.6910761041429834\n",
      "epoch : 448, loss : 0.7133064036923172\n",
      "epoch : 449, loss : 0.7444124153329906\n",
      "epoch : 450, loss : 0.7035055830300997\n",
      "epoch : 451, loss : 0.6915308555986347\n",
      "epoch : 452, loss : 0.69668000061371\n",
      "epoch : 453, loss : 0.6558885611256454\n",
      "epoch : 454, loss : 0.7357727083629139\n",
      "epoch : 455, loss : 0.6703830446358052\n",
      "epoch : 456, loss : 0.6990277624642832\n",
      "epoch : 457, loss : 0.7279193776207461\n",
      "epoch : 458, loss : 0.7349444022298739\n",
      "epoch : 459, loss : 0.6855660848904173\n",
      "epoch : 460, loss : 0.7406365075631369\n",
      "epoch : 461, loss : 0.7544116984910316\n",
      "epoch : 462, loss : 0.6724691154415914\n",
      "epoch : 463, loss : 0.6995292752398904\n",
      "epoch : 464, loss : 0.7112832708511109\n",
      "epoch : 465, loss : 0.6587671376758459\n",
      "epoch : 466, loss : 0.7092144611625577\n",
      "epoch : 467, loss : 0.6877293964273947\n",
      "epoch : 468, loss : 0.7107031659853308\n",
      "epoch : 469, loss : 0.7016496422304147\n",
      "epoch : 470, loss : 0.7218806761529484\n",
      "epoch : 471, loss : 0.7391429421587801\n",
      "epoch : 472, loss : 0.6718291385426595\n",
      "epoch : 473, loss : 0.7322343954268596\n",
      "epoch : 474, loss : 0.6734084090295587\n",
      "epoch : 475, loss : 0.7147092345025647\n",
      "epoch : 476, loss : 0.7151326841296867\n",
      "epoch : 477, loss : 0.708691849553954\n",
      "epoch : 478, loss : 0.7112391120993748\n",
      "epoch : 479, loss : 0.7089742561285246\n",
      "epoch : 480, loss : 0.7122024202118549\n",
      "epoch : 481, loss : 0.7214775934126235\n",
      "epoch : 482, loss : 0.716254463021877\n",
      "epoch : 483, loss : 0.6893707584652179\n",
      "epoch : 484, loss : 0.638096079781832\n",
      "epoch : 485, loss : 0.699462844380688\n",
      "epoch : 486, loss : 0.6807355039438299\n",
      "epoch : 487, loss : 0.7374943864391593\n",
      "epoch : 488, loss : 0.7050712897453374\n",
      "epoch : 489, loss : 0.6818706805546113\n",
      "epoch : 490, loss : 0.7348907942954415\n",
      "epoch : 491, loss : 0.7053378951669208\n",
      "epoch : 492, loss : 0.6988148503936475\n",
      "epoch : 493, loss : 0.6576760031661695\n",
      "epoch : 494, loss : 0.7430407102472522\n",
      "epoch : 495, loss : 0.6636594147358782\n",
      "epoch : 496, loss : 0.6957069600857394\n",
      "epoch : 497, loss : 0.686838036604738\n",
      "epoch : 498, loss : 0.6821048726379486\n",
      "epoch : 499, loss : 0.7080701340400328\n",
      "epoch : 500, loss : 0.6559833601746163\n",
      "epoch : 501, loss : 0.6975440465896815\n",
      "epoch : 502, loss : 0.7148548850871346\n",
      "epoch : 503, loss : 0.6831406189885363\n",
      "epoch : 504, loss : 0.7025295818313594\n",
      "epoch : 505, loss : 0.7272956644730705\n",
      "epoch : 506, loss : 0.6660872010042751\n",
      "epoch : 507, loss : 0.6946091882092695\n",
      "epoch : 508, loss : 0.6581487269081342\n",
      "epoch : 509, loss : 0.7702057636021674\n",
      "epoch : 510, loss : 0.6922503233600728\n",
      "epoch : 511, loss : 0.6916216794057221\n",
      "epoch : 512, loss : 0.6812110631571056\n",
      "epoch : 513, loss : 0.6702274349346856\n",
      "epoch : 514, loss : 0.7273781468137728\n",
      "epoch : 515, loss : 0.7092711914229136\n",
      "epoch : 516, loss : 0.6932009878315732\n",
      "epoch : 517, loss : 0.7096494799895439\n",
      "epoch : 518, loss : 0.6638753688002996\n",
      "epoch : 519, loss : 0.6991077359851567\n",
      "epoch : 520, loss : 0.6819403428677685\n",
      "epoch : 521, loss : 0.7252954507446185\n",
      "epoch : 522, loss : 0.6723174211587771\n",
      "epoch : 523, loss : 0.6876944470615777\n",
      "epoch : 524, loss : 0.6717952615763694\n",
      "epoch : 525, loss : 0.7407386291198766\n",
      "epoch : 526, loss : 0.7082734919807034\n",
      "epoch : 527, loss : 0.6720017558716225\n",
      "epoch : 528, loss : 0.6841358098708866\n",
      "epoch : 529, loss : 0.6526773039266491\n",
      "epoch : 530, loss : 0.6552454258308145\n",
      "epoch : 531, loss : 0.6494855875117971\n",
      "epoch : 532, loss : 0.673920883036663\n",
      "epoch : 533, loss : 0.6824270241143093\n",
      "epoch : 534, loss : 0.6936871862219952\n",
      "epoch : 535, loss : 0.7091823271457321\n",
      "epoch : 536, loss : 0.695462095370722\n",
      "epoch : 537, loss : 0.7054387844285097\n",
      "epoch : 538, loss : 0.6544029836969051\n",
      "epoch : 539, loss : 0.6901456800682085\n",
      "epoch : 540, loss : 0.7092576433194189\n",
      "epoch : 541, loss : 0.653267056996565\n",
      "epoch : 542, loss : 0.6884137646452165\n",
      "epoch : 543, loss : 0.6978375331772596\n",
      "epoch : 544, loss : 0.6815246613625797\n",
      "epoch : 545, loss : 0.6586020446188018\n",
      "epoch : 546, loss : 0.660206028266201\n",
      "epoch : 547, loss : 0.6616362214035475\n",
      "epoch : 548, loss : 0.7095000425113992\n",
      "epoch : 549, loss : 0.6557550940310822\n",
      "epoch : 550, loss : 0.7570737401716168\n",
      "epoch : 551, loss : 0.667396201007371\n",
      "epoch : 552, loss : 0.6536473053509301\n",
      "epoch : 553, loss : 0.6901196986167429\n",
      "epoch : 554, loss : 0.6912706622473473\n",
      "epoch : 555, loss : 0.6576616506925833\n",
      "epoch : 556, loss : 0.7027033437054756\n",
      "epoch : 557, loss : 0.7447009934300909\n",
      "epoch : 558, loss : 0.707312121477392\n",
      "epoch : 559, loss : 0.6510477489009006\n",
      "epoch : 560, loss : 0.7013366554071082\n",
      "epoch : 561, loss : 0.6857388493417502\n",
      "epoch : 562, loss : 0.666572671569008\n",
      "epoch : 563, loss : 0.6367013960571188\n",
      "epoch : 564, loss : 0.6916001631570211\n",
      "epoch : 565, loss : 0.6944120796221424\n",
      "epoch : 566, loss : 0.6675923732047069\n",
      "epoch : 567, loss : 0.6731216190875099\n",
      "epoch : 568, loss : 0.6457215616570336\n",
      "epoch : 569, loss : 0.6676675196918187\n",
      "epoch : 570, loss : 0.7318629819210737\n",
      "epoch : 571, loss : 0.6227117773763318\n",
      "epoch : 572, loss : 0.6675964377637444\n",
      "epoch : 573, loss : 0.6969615496128191\n",
      "epoch : 574, loss : 0.6867696876213298\n",
      "epoch : 575, loss : 0.7193306053789592\n",
      "epoch : 576, loss : 0.7084723374104256\n",
      "epoch : 577, loss : 0.6888593365940084\n",
      "epoch : 578, loss : 0.7013240218580507\n",
      "epoch : 579, loss : 0.7408523775717438\n",
      "epoch : 580, loss : 0.6625334541764458\n",
      "epoch : 581, loss : 0.7256657748640394\n",
      "epoch : 582, loss : 0.7373452778686761\n",
      "epoch : 583, loss : 0.6528405844217027\n",
      "epoch : 584, loss : 0.6599604229746591\n",
      "epoch : 585, loss : 0.6512243051101702\n",
      "epoch : 586, loss : 0.7002330679742108\n",
      "epoch : 587, loss : 0.6773367611656912\n",
      "epoch : 588, loss : 0.7362194300289956\n",
      "epoch : 589, loss : 0.6572025056342562\n",
      "epoch : 590, loss : 0.6540485272077299\n",
      "epoch : 591, loss : 0.6850228804062414\n",
      "epoch : 592, loss : 0.7196186434731655\n",
      "epoch : 593, loss : 0.7053901220897246\n",
      "epoch : 594, loss : 0.7212555511593215\n",
      "epoch : 595, loss : 0.6699594276119734\n",
      "epoch : 596, loss : 0.7444007026612389\n",
      "epoch : 597, loss : 0.6480690397727517\n",
      "epoch : 598, loss : 0.6492187156651679\n",
      "epoch : 599, loss : 0.6678983145867234\n",
      "epoch : 600, loss : 0.7496730570258261\n",
      "epoch : 601, loss : 0.6405943944480313\n",
      "epoch : 602, loss : 0.7546751603457101\n",
      "epoch : 603, loss : 0.6608308406373391\n",
      "epoch : 604, loss : 0.6668031400499355\n",
      "epoch : 605, loss : 0.7415054994801608\n",
      "epoch : 606, loss : 0.674843882753091\n",
      "epoch : 607, loss : 0.6414002041942315\n",
      "epoch : 608, loss : 0.6604376034311372\n",
      "epoch : 609, loss : 0.6400588344592666\n",
      "epoch : 610, loss : 0.6826346318305456\n",
      "epoch : 611, loss : 0.653154137461885\n",
      "epoch : 612, loss : 0.6731106933723553\n",
      "epoch : 613, loss : 0.6940942478318359\n",
      "epoch : 614, loss : 0.7229377349964019\n",
      "epoch : 615, loss : 0.7460775228724943\n",
      "epoch : 616, loss : 0.6683053388813905\n",
      "epoch : 617, loss : 0.697474312909996\n",
      "epoch : 618, loss : 0.6386492784589739\n",
      "epoch : 619, loss : 0.6476419298703628\n",
      "epoch : 620, loss : 0.6773413261287107\n",
      "epoch : 621, loss : 0.6765824411964197\n",
      "epoch : 622, loss : 0.6731423182179637\n",
      "epoch : 623, loss : 0.6323400484142538\n",
      "epoch : 624, loss : 0.6877647069232283\n",
      "epoch : 625, loss : 0.6601002540673558\n",
      "epoch : 626, loss : 0.6659411656608276\n",
      "epoch : 627, loss : 0.6882419086286277\n",
      "epoch : 628, loss : 0.655601809704447\n",
      "epoch : 629, loss : 0.6348140676882468\n",
      "epoch : 630, loss : 0.6955434145290139\n",
      "epoch : 631, loss : 0.7419353977508177\n",
      "epoch : 632, loss : 0.6889276961402773\n",
      "epoch : 633, loss : 0.6311315361797822\n",
      "epoch : 634, loss : 0.6958272119000468\n",
      "epoch : 635, loss : 0.6883206563625194\n",
      "epoch : 636, loss : 0.654236929017462\n",
      "epoch : 637, loss : 0.6505989900035908\n",
      "epoch : 638, loss : 0.660461625437726\n",
      "epoch : 639, loss : 0.7215077328170809\n",
      "epoch : 640, loss : 0.6423089558582143\n",
      "epoch : 641, loss : 0.6822453585629871\n",
      "epoch : 642, loss : 0.6580996754257995\n",
      "epoch : 643, loss : 0.6600692712869562\n",
      "epoch : 644, loss : 0.7048186954695743\n",
      "epoch : 645, loss : 0.6988628713718844\n",
      "epoch : 646, loss : 0.6468600626342833\n",
      "epoch : 647, loss : 0.6840429086798278\n",
      "epoch : 648, loss : 0.6929921885493303\n",
      "epoch : 649, loss : 0.7076013479594976\n",
      "epoch : 650, loss : 0.6518889279700187\n",
      "epoch : 651, loss : 0.6732437553571367\n",
      "epoch : 652, loss : 0.661342047411624\n",
      "epoch : 653, loss : 0.7409205265468157\n",
      "epoch : 654, loss : 0.680463967471282\n",
      "epoch : 655, loss : 0.6601871347789782\n",
      "epoch : 656, loss : 0.6931285608151059\n",
      "epoch : 657, loss : 0.6195782420795074\n",
      "epoch : 658, loss : 0.6869237780308729\n",
      "epoch : 659, loss : 0.6843675417865004\n",
      "epoch : 660, loss : 0.6770443851210254\n",
      "epoch : 661, loss : 0.6598122171930861\n",
      "epoch : 662, loss : 0.6421936709304804\n",
      "epoch : 663, loss : 0.6684556023314464\n",
      "epoch : 664, loss : 0.6479738601122128\n",
      "epoch : 665, loss : 0.6821123393772107\n",
      "epoch : 666, loss : 0.663696626256936\n",
      "epoch : 667, loss : 0.6773011498567099\n",
      "epoch : 668, loss : 0.6329450320480923\n",
      "epoch : 669, loss : 0.6403388472011785\n",
      "epoch : 670, loss : 0.6876577792210473\n",
      "epoch : 671, loss : 0.7235883971321264\n",
      "epoch : 672, loss : 0.6202686526480358\n",
      "epoch : 673, loss : 0.6573950889398019\n",
      "epoch : 674, loss : 0.6725810265268447\n",
      "epoch : 675, loss : 0.6560112276765445\n",
      "epoch : 676, loss : 0.6558662054038944\n",
      "epoch : 677, loss : 0.6516295955438299\n",
      "epoch : 678, loss : 0.6785600892928371\n",
      "epoch : 679, loss : 0.6661134003169562\n",
      "epoch : 680, loss : 0.6641551278559003\n",
      "epoch : 681, loss : 0.636498753742175\n",
      "epoch : 682, loss : 0.675581813283254\n",
      "epoch : 683, loss : 0.6436295071033339\n",
      "epoch : 684, loss : 0.6784560061372459\n",
      "epoch : 685, loss : 0.6572635725697764\n",
      "epoch : 686, loss : 0.6588131902430513\n",
      "epoch : 687, loss : 0.669137949551907\n",
      "epoch : 688, loss : 0.7451092637329358\n",
      "epoch : 689, loss : 0.6667818650439908\n",
      "epoch : 690, loss : 0.6636676778941322\n",
      "epoch : 691, loss : 0.6869393452387944\n",
      "epoch : 692, loss : 0.6223232493193148\n",
      "epoch : 693, loss : 0.6465939119981919\n",
      "epoch : 694, loss : 0.6637872715514801\n",
      "epoch : 695, loss : 0.673969327784582\n",
      "epoch : 696, loss : 0.6690708420223309\n",
      "epoch : 697, loss : 0.6960710591833159\n",
      "epoch : 698, loss : 0.6956154860512158\n",
      "epoch : 699, loss : 0.6204571135548156\n",
      "epoch : 700, loss : 0.6780072048434037\n",
      "epoch : 701, loss : 0.6504267288292099\n",
      "epoch : 702, loss : 0.7016974756236573\n",
      "epoch : 703, loss : 0.6928735304696728\n",
      "epoch : 704, loss : 0.6527773704768814\n",
      "epoch : 705, loss : 0.6728488405650009\n",
      "epoch : 706, loss : 0.7027593913960155\n",
      "epoch : 707, loss : 0.6787799373958351\n",
      "epoch : 708, loss : 0.648089046647591\n",
      "epoch : 709, loss : 0.6891710786532407\n",
      "epoch : 710, loss : 0.7015051309296413\n",
      "epoch : 711, loss : 0.680887525270466\n",
      "epoch : 712, loss : 0.6380455113579073\n",
      "epoch : 713, loss : 0.6879485177561347\n",
      "epoch : 714, loss : 0.6877909378836642\n",
      "epoch : 715, loss : 0.6247857172538828\n",
      "epoch : 716, loss : 0.6603454378806543\n",
      "epoch : 717, loss : 0.6531516771899237\n",
      "epoch : 718, loss : 0.6201666951260315\n",
      "epoch : 719, loss : 0.6734077595020124\n",
      "epoch : 720, loss : 0.6815681777947474\n",
      "epoch : 721, loss : 0.6100353616595471\n",
      "epoch : 722, loss : 0.6191944451067132\n",
      "epoch : 723, loss : 0.6734765642728974\n",
      "epoch : 724, loss : 0.6632739202205904\n",
      "epoch : 725, loss : 0.6965992997902651\n",
      "epoch : 726, loss : 0.6490919219447532\n",
      "epoch : 727, loss : 0.6923488211686262\n",
      "epoch : 728, loss : 0.6534953982413073\n",
      "epoch : 729, loss : 0.6363011530161925\n",
      "epoch : 730, loss : 0.6662555228774079\n",
      "epoch : 731, loss : 0.6148427252329471\n",
      "epoch : 732, loss : 0.6522082761442302\n",
      "epoch : 733, loss : 0.6715564641982942\n",
      "epoch : 734, loss : 0.6184718381759413\n",
      "epoch : 735, loss : 0.6434943070235549\n",
      "epoch : 736, loss : 0.6908588774660399\n",
      "epoch : 737, loss : 0.675210562227813\n",
      "epoch : 738, loss : 0.639553700344914\n",
      "epoch : 739, loss : 0.6832312747178214\n",
      "epoch : 740, loss : 0.6301791220830352\n",
      "epoch : 741, loss : 0.6578357679105575\n",
      "epoch : 742, loss : 0.6933219004195762\n",
      "epoch : 743, loss : 0.6458934612769824\n",
      "epoch : 744, loss : 0.6736988310116837\n",
      "epoch : 745, loss : 0.6611175688794609\n",
      "epoch : 746, loss : 0.6644843413882187\n",
      "epoch : 747, loss : 0.6972814447651537\n",
      "epoch : 748, loss : 0.6252914169815588\n",
      "epoch : 749, loss : 0.6462811519294298\n",
      "epoch : 750, loss : 0.6600232853098855\n",
      "epoch : 751, loss : 0.6912154635287595\n",
      "epoch : 752, loss : 0.6580386208265679\n",
      "epoch : 753, loss : 0.6672115767647758\n",
      "epoch : 754, loss : 0.6173825754681849\n",
      "epoch : 755, loss : 0.6403328576981457\n",
      "epoch : 756, loss : 0.6744740491679916\n",
      "epoch : 757, loss : 0.6889112004612417\n",
      "epoch : 758, loss : 0.614107967323367\n",
      "epoch : 759, loss : 0.6660000714171762\n",
      "epoch : 760, loss : 0.6433283000959077\n",
      "epoch : 761, loss : 0.6782674817271885\n",
      "epoch : 762, loss : 0.6901979551262639\n",
      "epoch : 763, loss : 0.6562834588191471\n",
      "epoch : 764, loss : 0.6252148506204502\n",
      "epoch : 765, loss : 0.6402525698232735\n",
      "epoch : 766, loss : 0.6346782684121582\n",
      "epoch : 767, loss : 0.6964294413268338\n",
      "epoch : 768, loss : 0.6701779478247225\n",
      "epoch : 769, loss : 0.6578634191064057\n",
      "epoch : 770, loss : 0.6184679760200521\n",
      "epoch : 771, loss : 0.6727641099056216\n",
      "epoch : 772, loss : 0.6305201386815789\n",
      "epoch : 773, loss : 0.6684277919196007\n",
      "epoch : 774, loss : 0.6729489836798442\n",
      "epoch : 775, loss : 0.6241411766120378\n",
      "epoch : 776, loss : 0.635551881881621\n",
      "epoch : 777, loss : 0.6638023260231238\n",
      "epoch : 778, loss : 0.6399437634019649\n",
      "epoch : 779, loss : 0.7003807419557135\n",
      "epoch : 780, loss : 0.6232828862268391\n",
      "epoch : 781, loss : 0.6706912904260022\n",
      "epoch : 782, loss : 0.6488750054874034\n",
      "epoch : 783, loss : 0.6664145434844869\n",
      "epoch : 784, loss : 0.6361808845349919\n",
      "epoch : 785, loss : 0.6663944859884484\n",
      "epoch : 786, loss : 0.6167508672560427\n",
      "epoch : 787, loss : 0.6733598411075903\n",
      "epoch : 788, loss : 0.6578227757484092\n",
      "epoch : 789, loss : 0.6601558913873321\n",
      "epoch : 790, loss : 0.6597308730194652\n",
      "epoch : 791, loss : 0.6804404369070075\n",
      "epoch : 792, loss : 0.6722919274428598\n",
      "epoch : 793, loss : 0.6357495922952484\n",
      "epoch : 794, loss : 0.6082247703684058\n",
      "epoch : 795, loss : 0.6603081168318753\n",
      "epoch : 796, loss : 0.6940854975193363\n",
      "epoch : 797, loss : 0.6578190716931103\n",
      "epoch : 798, loss : 0.6222222050679526\n",
      "epoch : 799, loss : 0.6071675676335753\n",
      "epoch : 800, loss : 0.6138862651692016\n",
      "epoch : 801, loss : 0.6489044818450521\n",
      "epoch : 802, loss : 0.6799690052642992\n",
      "epoch : 803, loss : 0.7030843177544976\n",
      "epoch : 804, loss : 0.6317905944459198\n",
      "epoch : 805, loss : 0.6541613647465339\n",
      "epoch : 806, loss : 0.6345729824317907\n",
      "epoch : 807, loss : 0.6506994587156574\n",
      "epoch : 808, loss : 0.6409064249515466\n",
      "epoch : 809, loss : 0.649518831010857\n",
      "epoch : 810, loss : 0.6996441585758318\n",
      "epoch : 811, loss : 0.666152188987801\n",
      "epoch : 812, loss : 0.6967969373772467\n",
      "epoch : 813, loss : 0.6400890084022802\n",
      "epoch : 814, loss : 0.6160409534753337\n",
      "epoch : 815, loss : 0.6103970632958978\n",
      "epoch : 816, loss : 0.6736220295502808\n",
      "epoch : 817, loss : 0.6406054023414975\n",
      "epoch : 818, loss : 0.6299774407341978\n",
      "epoch : 819, loss : 0.648260104191354\n",
      "epoch : 820, loss : 0.6312594851766097\n",
      "epoch : 821, loss : 0.670652456073563\n",
      "epoch : 822, loss : 0.6104264780992907\n",
      "epoch : 823, loss : 0.6203371089950571\n",
      "epoch : 824, loss : 0.6560009540979698\n",
      "epoch : 825, loss : 0.6436725389432933\n",
      "epoch : 826, loss : 0.6770237710863065\n",
      "epoch : 827, loss : 0.6553955532787579\n",
      "epoch : 828, loss : 0.6899221371604923\n",
      "epoch : 829, loss : 0.6988706626654061\n",
      "epoch : 830, loss : 0.6268021076685534\n",
      "epoch : 831, loss : 0.63814887155031\n",
      "epoch : 832, loss : 0.6743569644142274\n",
      "epoch : 833, loss : 0.6826580374146571\n",
      "epoch : 834, loss : 0.6446223789893589\n",
      "epoch : 835, loss : 0.7087464747647264\n",
      "epoch : 836, loss : 0.6283816973909317\n",
      "epoch : 837, loss : 0.6566934650549807\n",
      "epoch : 838, loss : 0.6958215745722973\n",
      "epoch : 839, loss : 0.6362211429209685\n",
      "epoch : 840, loss : 0.6399031688219602\n",
      "epoch : 841, loss : 0.6463469793283044\n",
      "epoch : 842, loss : 0.6044524020602564\n",
      "epoch : 843, loss : 0.6205995876928273\n",
      "epoch : 844, loss : 0.6552455162265561\n",
      "epoch : 845, loss : 0.6538211958686933\n",
      "epoch : 846, loss : 0.6178343341916985\n",
      "epoch : 847, loss : 0.657833277089081\n",
      "epoch : 848, loss : 0.7046176309673352\n",
      "epoch : 849, loss : 0.6124579206130727\n",
      "epoch : 850, loss : 0.6009372132434412\n",
      "epoch : 851, loss : 0.6400635459896551\n",
      "epoch : 852, loss : 0.61959238243541\n",
      "epoch : 853, loss : 0.6378108917165527\n",
      "epoch : 854, loss : 0.6534027709271649\n",
      "epoch : 855, loss : 0.6283521879114149\n",
      "epoch : 856, loss : 0.6138388348338797\n",
      "epoch : 857, loss : 0.6136959610316491\n",
      "epoch : 858, loss : 0.653469773296181\n",
      "epoch : 859, loss : 0.649859761072667\n",
      "epoch : 860, loss : 0.6387130456033331\n",
      "epoch : 861, loss : 0.6485652428857847\n",
      "epoch : 862, loss : 0.6998559501746584\n",
      "epoch : 863, loss : 0.6375083294085694\n",
      "epoch : 864, loss : 0.6773713281066976\n",
      "epoch : 865, loss : 0.663116653104752\n",
      "epoch : 866, loss : 0.6218873622116817\n",
      "epoch : 867, loss : 0.6539795652826751\n",
      "epoch : 868, loss : 0.626557335282732\n",
      "epoch : 869, loss : 0.6383249574593651\n",
      "epoch : 870, loss : 0.6652385900494421\n",
      "epoch : 871, loss : 0.6494740647294569\n",
      "epoch : 872, loss : 0.6429748582405733\n",
      "epoch : 873, loss : 0.6461244369666351\n",
      "epoch : 874, loss : 0.6627037255288187\n",
      "epoch : 875, loss : 0.6096982054157254\n",
      "epoch : 876, loss : 0.6728710172660606\n",
      "epoch : 877, loss : 0.6408401214542515\n",
      "epoch : 878, loss : 0.6327182822374829\n",
      "epoch : 879, loss : 0.6544009175836676\n",
      "epoch : 880, loss : 0.6332661176599593\n",
      "epoch : 881, loss : 0.6513711192117155\n",
      "epoch : 882, loss : 0.6145498683642624\n",
      "epoch : 883, loss : 0.6588634566446658\n",
      "epoch : 884, loss : 0.6193626254367954\n",
      "epoch : 885, loss : 0.594876543043888\n",
      "epoch : 886, loss : 0.6846713113276652\n",
      "epoch : 887, loss : 0.6659951437962773\n",
      "epoch : 888, loss : 0.6082590359492764\n",
      "epoch : 889, loss : 0.6273345811846588\n",
      "epoch : 890, loss : 0.6220433270818485\n",
      "epoch : 891, loss : 0.5946627397447458\n",
      "epoch : 892, loss : 0.5916902838313206\n",
      "epoch : 893, loss : 0.6558838899826885\n",
      "epoch : 894, loss : 0.6553798846920849\n",
      "epoch : 895, loss : 0.6191321573010939\n",
      "epoch : 896, loss : 0.6287031839457297\n",
      "epoch : 897, loss : 0.623115560408433\n",
      "epoch : 898, loss : 0.5896332383149748\n",
      "epoch : 899, loss : 0.5670780913503617\n",
      "epoch : 900, loss : 0.6593890324039554\n",
      "epoch : 901, loss : 0.6174992057222218\n",
      "epoch : 902, loss : 0.7282336736142493\n",
      "epoch : 903, loss : 0.5894784059883\n",
      "epoch : 904, loss : 0.6737969617180216\n",
      "epoch : 905, loss : 0.666493045035366\n",
      "epoch : 906, loss : 0.6106462770443066\n",
      "epoch : 907, loss : 0.6001240383323093\n",
      "epoch : 908, loss : 0.6354785341799698\n",
      "epoch : 909, loss : 0.613720818459113\n",
      "epoch : 910, loss : 0.6539022062148595\n",
      "epoch : 911, loss : 0.609459712091912\n",
      "epoch : 912, loss : 0.6535885862909105\n",
      "epoch : 913, loss : 0.5748511577263592\n",
      "epoch : 914, loss : 0.6429972536974697\n",
      "epoch : 915, loss : 0.6740594506802686\n",
      "epoch : 916, loss : 0.625256708389616\n",
      "epoch : 917, loss : 0.614318936490228\n",
      "epoch : 918, loss : 0.6647546156648741\n",
      "epoch : 919, loss : 0.603509264407084\n",
      "epoch : 920, loss : 0.6112442417771243\n",
      "epoch : 921, loss : 0.5898556549284326\n",
      "epoch : 922, loss : 0.6576127680406662\n",
      "epoch : 923, loss : 0.6068160882850976\n",
      "epoch : 924, loss : 0.6120711775011394\n",
      "epoch : 925, loss : 0.6473587004762797\n",
      "epoch : 926, loss : 0.6062598788825289\n",
      "epoch : 927, loss : 0.5817343205190937\n",
      "epoch : 928, loss : 0.6362363573812561\n",
      "epoch : 929, loss : 0.6151196894715268\n",
      "epoch : 930, loss : 0.6755312828149428\n",
      "epoch : 931, loss : 0.6381213549503184\n",
      "epoch : 932, loss : 0.6485139567624479\n",
      "epoch : 933, loss : 0.605768103945855\n",
      "epoch : 934, loss : 0.5919213256794312\n",
      "epoch : 935, loss : 0.6192637498483531\n",
      "epoch : 936, loss : 0.6466554372380277\n",
      "epoch : 937, loss : 0.5856336070259758\n",
      "epoch : 938, loss : 0.644979818201146\n",
      "epoch : 939, loss : 0.6681277578481324\n",
      "epoch : 940, loss : 0.6270433901341989\n",
      "epoch : 941, loss : 0.6359079777584973\n",
      "epoch : 942, loss : 0.6508183386328458\n",
      "epoch : 943, loss : 0.6203530990798872\n",
      "epoch : 944, loss : 0.5966378924538189\n",
      "epoch : 945, loss : 0.6260138592841261\n",
      "epoch : 946, loss : 0.6312600848852304\n",
      "epoch : 947, loss : 0.6199487884573739\n",
      "epoch : 948, loss : 0.6310735416546412\n",
      "epoch : 949, loss : 0.6194183890103871\n",
      "epoch : 950, loss : 0.6600736203482522\n",
      "epoch : 951, loss : 0.6452399936573817\n",
      "epoch : 952, loss : 0.6336204426939803\n",
      "epoch : 953, loss : 0.6586166846136382\n",
      "epoch : 954, loss : 0.6570701483929546\n",
      "epoch : 955, loss : 0.6356397400329536\n",
      "epoch : 956, loss : 0.5758231095169987\n",
      "epoch : 957, loss : 0.6479046059050922\n",
      "epoch : 958, loss : 0.6595433134962614\n",
      "epoch : 959, loss : 0.5763588766650879\n",
      "epoch : 960, loss : 0.6663015050458664\n",
      "epoch : 961, loss : 0.6564553539097594\n",
      "epoch : 962, loss : 0.6090247576561115\n",
      "epoch : 963, loss : 0.5733590934931457\n",
      "epoch : 964, loss : 0.6337837170501951\n",
      "epoch : 965, loss : 0.6234979939657604\n",
      "epoch : 966, loss : 0.717715520839307\n",
      "epoch : 967, loss : 0.633910058485571\n",
      "epoch : 968, loss : 0.5840289552684481\n",
      "epoch : 969, loss : 0.6183154175664396\n",
      "epoch : 970, loss : 0.6466225553801509\n",
      "epoch : 971, loss : 0.6433642783396568\n",
      "epoch : 972, loss : 0.6570081080934639\n",
      "epoch : 973, loss : 0.658025559998286\n",
      "epoch : 974, loss : 0.6667972489256943\n",
      "epoch : 975, loss : 0.6247705831435186\n",
      "epoch : 976, loss : 0.6099355048289125\n",
      "epoch : 977, loss : 0.6162336658985097\n",
      "epoch : 978, loss : 0.6163940605555113\n",
      "epoch : 979, loss : 0.6055992095657216\n",
      "epoch : 980, loss : 0.6545378927011037\n",
      "epoch : 981, loss : 0.6075377046073159\n",
      "epoch : 982, loss : 0.6391364380133101\n",
      "epoch : 983, loss : 0.6106178024937275\n",
      "epoch : 984, loss : 0.7265030287997456\n",
      "epoch : 985, loss : 0.6821254662337964\n",
      "epoch : 986, loss : 0.6807557947698825\n",
      "epoch : 987, loss : 0.6570183459408605\n",
      "epoch : 988, loss : 0.641047559829039\n",
      "epoch : 989, loss : 0.6526850509517683\n",
      "epoch : 990, loss : 0.6256681346502785\n",
      "epoch : 991, loss : 0.5941606214156285\n",
      "epoch : 992, loss : 0.6105362052391218\n",
      "epoch : 993, loss : 0.5885563272879449\n",
      "epoch : 994, loss : 0.6388590982770754\n",
      "epoch : 995, loss : 0.5708452516000339\n",
      "epoch : 996, loss : 0.6352851324775854\n",
      "epoch : 997, loss : 0.6280066504221044\n",
      "epoch : 998, loss : 0.6296971374288877\n",
      "epoch : 999, loss : 0.6762052787808515\n",
      "epoch : 1000, loss : 0.6712976232530588\n"
     ]
    }
   ],
   "source": [
    "model = DeepNeuralNetwork(\n",
    "    input_size=4,\n",
    "    output_size=1,\n",
    "    hidden_size=16,\n",
    "    epoches=1000,\n",
    "    learning_rate=0.001,\n",
    "    dropout=0.8,\n",
    "    hidden_layers=4\n",
    ")\n",
    "\n",
    "model.fit(X_train,Y_train)\n",
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb3dda5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.83      0.77       188\n",
      "           1       0.74      0.60      0.66       155\n",
      "\n",
      "    accuracy                           0.73       343\n",
      "   macro avg       0.73      0.71      0.72       343\n",
      "weighted avg       0.73      0.73      0.72       343\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "cmf = confusion_matrix(Y_test,preds)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(cmf,annot=True,fmt='d')\n",
    "plt.xlabel(\"True\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.show()\n",
    "print(classification_report(Y_test,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cff45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 2 1 3]\n"
     ]
    }
   ],
   "source": [
    "print(np.random.randint())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
