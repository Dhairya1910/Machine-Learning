{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d8bec25",
   "metadata": {},
   "source": [
    "Creating deep neural network from scratch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "321d8650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edc2584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 , loss = 8.232338193396552\n",
      "epoch 1 , loss = 8.08798465632776\n",
      "epoch 2 , loss = 7.565019416640727\n",
      "epoch 3 , loss = 7.058355811231796\n",
      "epoch 4 , loss = 6.530961429400374\n",
      "epoch 5 , loss = 5.986086579555838\n",
      "epoch 6 , loss = 5.461520261290671\n",
      "epoch 7 , loss = 4.970496454557526\n",
      "epoch 8 , loss = 4.571269413531469\n",
      "epoch 9 , loss = 4.368456644109925\n",
      "epoch 10 , loss = 4.299496742338502\n",
      "epoch 11 , loss = 4.27749408940406\n",
      "epoch 12 , loss = 4.265499494036222\n",
      "epoch 13 , loss = 4.258255934056616\n",
      "epoch 14 , loss = 4.201605157562372\n",
      "epoch 15 , loss = 4.013299519880076\n",
      "epoch 16 , loss = 3.904934385197339\n",
      "epoch 17 , loss = 3.825783422971112\n",
      "epoch 18 , loss = 3.7482605075875166\n",
      "epoch 19 , loss = 3.672281688873849\n",
      "epoch 20 , loss = 3.5977830100610118\n",
      "epoch 21 , loss = 3.5247123478571325\n",
      "epoch 22 , loss = 3.4530246328495355\n",
      "epoch 23 , loss = 3.3826791413918222\n",
      "epoch 24 , loss = 3.3136379880266547\n",
      "epoch 25 , loss = 3.245865291176835\n",
      "epoch 26 , loss = 3.179326708222854\n",
      "epoch 27 , loss = 3.1139891709584786\n",
      "epoch 28 , loss = 3.0498207291523256\n",
      "epoch 29 , loss = 2.986790452670351\n",
      "epoch 30 , loss = 2.9248683654736207\n",
      "epoch 31 , loss = 2.8640253973751064\n",
      "epoch 32 , loss = 2.804233345997193\n",
      "epoch 33 , loss = 2.7499504907174246\n",
      "epoch 34 , loss = 2.7035863044428927\n",
      "epoch 35 , loss = 2.646395903500128\n",
      "epoch 36 , loss = 2.5901700284510514\n",
      "epoch 37 , loss = 2.534880320256366\n",
      "epoch 38 , loss = 2.4805007471692906\n",
      "epoch 39 , loss = 2.427006897983924\n",
      "epoch 40 , loss = 2.3743755839602176\n",
      "epoch 41 , loss = 2.315424291989628\n",
      "epoch 42 , loss = 2.0833243014843794\n",
      "epoch 43 , loss = 1.8710367518230782\n",
      "epoch 44 , loss = 1.6227487953301405\n",
      "epoch 45 , loss = 1.4451154079422632\n",
      "epoch 46 , loss = 1.3151197558686132\n",
      "epoch 47 , loss = 1.1939280772515493\n",
      "epoch 48 , loss = 1.135758524000356\n",
      "epoch 49 , loss = 1.0280642730437533\n",
      "epoch 50 , loss = 0.9340713903158955\n",
      "epoch 51 , loss = 0.8809726025096991\n",
      "epoch 52 , loss = 0.7913740672899586\n",
      "epoch 53 , loss = 0.7117433627651535\n",
      "epoch 54 , loss = 0.6624613524650377\n",
      "epoch 55 , loss = 0.6043781024284851\n",
      "epoch 56 , loss = 0.5454010639539931\n",
      "epoch 57 , loss = 0.49649856177049556\n",
      "epoch 58 , loss = 0.470010181336824\n",
      "epoch 59 , loss = 0.43311218041409777\n",
      "epoch 60 , loss = 0.4008849790183523\n",
      "epoch 61 , loss = 0.37454085823218974\n",
      "epoch 62 , loss = 0.35751634137665356\n",
      "epoch 63 , loss = 0.3396854172251632\n",
      "epoch 64 , loss = 0.32202942375825466\n",
      "epoch 65 , loss = 0.3072112237448488\n",
      "epoch 66 , loss = 0.294481616305966\n",
      "epoch 67 , loss = 0.28611646112090006\n",
      "epoch 68 , loss = 0.27632630246194984\n",
      "epoch 69 , loss = 0.26704757024343656\n",
      "epoch 70 , loss = 0.25891911738991513\n",
      "epoch 71 , loss = 0.2516890925454405\n",
      "epoch 72 , loss = 0.24605019855454377\n",
      "epoch 73 , loss = 0.24077379130502996\n",
      "epoch 74 , loss = 0.23510415801223003\n",
      "epoch 75 , loss = 0.22997558995007555\n",
      "epoch 76 , loss = 0.22528838204990012\n",
      "epoch 77 , loss = 0.22097283760854453\n",
      "epoch 78 , loss = 0.21749970461834287\n",
      "epoch 79 , loss = 0.21404750417363097\n",
      "epoch 80 , loss = 0.21044242799731408\n",
      "epoch 81 , loss = 0.20709084978991008\n",
      "epoch 82 , loss = 0.20395604815007704\n",
      "epoch 83 , loss = 0.20101039328976084\n",
      "epoch 84 , loss = 0.19830860161094685\n",
      "epoch 85 , loss = 0.19608943801544068\n",
      "epoch 86 , loss = 0.1935335141731318\n",
      "epoch 87 , loss = 0.1911417254709824\n",
      "epoch 88 , loss = 0.18887440047318604\n",
      "epoch 89 , loss = 0.18671168288390683\n",
      "epoch 90 , loss = 0.18484221423309116\n",
      "epoch 91 , loss = 0.18317871408890926\n",
      "epoch 92 , loss = 0.18128038802332574\n",
      "epoch 93 , loss = 0.1794680851784899\n",
      "epoch 94 , loss = 0.17777595257240222\n",
      "epoch 95 , loss = 0.17633917498665908\n",
      "epoch 96 , loss = 0.17469705028649937\n",
      "epoch 97 , loss = 0.17330301253281852\n",
      "epoch 98 , loss = 0.17194855317146066\n",
      "epoch 99 , loss = 0.17045750954595912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DeepNeuralNetwork:\n",
    "    def __init__(self,input_size,hidden_size,output_size,epoches=100,learning_rate=0.01,activation_function=\"RELU\",hidden_layers=2): \n",
    "        self.input_size = input_size # input size or number of parameters \n",
    "        self.hidden_size = hidden_size # number of neurons in hidden layers \n",
    "        self.output_size = output_size # number of neuron in output layer\n",
    "        self.epoches = epoches # number of iterations \n",
    "        self.learning_rate = learning_rate # learning rate \n",
    "        self.activation_function = activation_function # Activations \n",
    "        self.hidden_layer = hidden_layers # number of hidden layers \n",
    "\n",
    "        \"\"\"initializing weights and bias and Output variable\"\"\"\n",
    "        self.WH =[np.random.randn(self.input_size,self.hidden_size)]\n",
    "        self.BH = [np.zeros((1,self.hidden_size))] \n",
    "\n",
    "        for _ in range(self.hidden_layer-1):\n",
    "            self.WH.append(np.random.randn(hidden_size,hidden_size))\n",
    "            self.BH.append(np.zeros((1,self.hidden_size)))\n",
    "\n",
    "        self.WO = np.random.randn(self.hidden_size,self.output_size)\n",
    "        self.BO = np.zeros((1,self.output_size))\n",
    "\n",
    "        \n",
    "    def RELU(self,z):\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    def sigmoid(self,z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def derivative_sigmoid(self,z):\n",
    "        s = self.sigmoid(z)\n",
    "        return s * (1-s)\n",
    "    \n",
    "    def derivative_relu(self,z):\n",
    "        return (z>0).astype(float) \n",
    "        \n",
    "    \n",
    "    def log_loss(self,preds,Y):\n",
    "        preds = np.clip(preds, 1e-7, 1 - 1e-7)\n",
    "        return -np.mean(Y*np.log(preds)+(1-Y)*np.log(1-preds))\n",
    "    \n",
    "    def Forward(self,X):\n",
    "        A = X \n",
    "        self.AH = [A]\n",
    "        self.z = []\n",
    "        for l in range(self.hidden_layer):\n",
    "            self.z.append(np.dot(A,self.WH[l]) + self.BH[l])\n",
    "            A = self.RELU(self.z[l])\n",
    "            self.AH.append(A)\n",
    "        \n",
    "        self.z.append(np.dot(A,self.WO) + self.BO)\n",
    "        self.AO = self.sigmoid(self.z[-1])\n",
    "        return self.AO\n",
    "    \n",
    "    def BackPropagation(self,X,Y):\n",
    "        m,n = X.shape\n",
    "        error = self.AO - Y\n",
    "\n",
    "        dwo = 1/ m * np.dot(self.AH[-1].T,error)  \n",
    "        dbo = 1/m * np.sum(error,axis=0,keepdims=True)\n",
    "\n",
    "        self.WO = self.WO - self.learning_rate * dwo \n",
    "        self.BO = self.BO - self.learning_rate * dbo\n",
    "\n",
    "        for l in reversed(range(self.hidden_layer)):\n",
    "            da = np.dot(error,self.WO.T if l == self.hidden_layer -1 else self.WH[l+1].T)\n",
    "            dz = da * self.derivative_relu(self.z[l])\n",
    "            dw = 1/m * np.dot(self.AH[l].T,dz)\n",
    "            db = 1/m * np.sum(dz,axis=0,keepdims=True)\n",
    "\n",
    "            self.WH[l] = self.WH[l] - self.learning_rate * dw \n",
    "            self.BH[l] = self.BH[l] - self.learning_rate * db\n",
    "            error = dz \n",
    "\n",
    "    def fit(self,X,Y):\n",
    "        Y = np.array(Y).reshape(-1,1)\n",
    "        for epoch in range(self.epoches):\n",
    "            a = self.Forward(X)\n",
    "            print(f\"epoch {epoch} , loss = {self.log_loss(a,Y)}\")\n",
    "            self.BackPropagation(X,Y)\n",
    "\n",
    "    def predict(self,X):\n",
    "        return (self.Forward(X) > 0.5 ).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "model = DeepNeuralNetwork(\n",
    "    input_size=2,\n",
    "    hidden_size=16,\n",
    "    output_size=1,\n",
    "    epoches=100,\n",
    "    learning_rate=0.001,\n",
    "    hidden_layers=4\n",
    ")\n",
    "\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]]) \n",
    "Y = np.array([0,0,0,1])\n",
    "model.fit(X,Y)\n",
    "model.predict(X)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
