{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70cd4fe3",
   "metadata": {},
   "source": [
    "# Full fledge neural net with hyperparameter and droput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6565dba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "442fe161",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNeuralNetwork:\n",
    "    def __init__(self,input_size,output_size,hidden_size,epoches=1000,learning_rate=0.001,dropout=0.5,hidden_layers=2):\n",
    "        self.input_size = input_size \n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size \n",
    "        self.learning_rate = learning_rate \n",
    "        self.dropout = dropout \n",
    "        self.hidden_layers = hidden_layers \n",
    "        self.epoches = epoches\n",
    "\n",
    "        self.hidden_weights = [np.random.randn(self.input_size,self.hidden_size)* np.square(2/self.input_size)]\n",
    "        self.hidden_bias = [np.zeros((1,self.hidden_size))]\n",
    "        \n",
    "        self.Hidden_Gradient_weights = [np.zeros((self.input_size,self.hidden_size))]\n",
    "        self.Hidden_Gradient_bias = [np.zeros((1,self.hidden_size))]\n",
    "\n",
    "        for _ in range(self.hidden_layers): \n",
    "            self.hidden_weights.append(np.random.randn(self.hidden_size,self.hidden_size) * np.square(2/self.input_size))\n",
    "            self.hidden_bias.append(np.zeros((1,self.hidden_size)))\n",
    "            self.Hidden_Gradient_weights.append(np.zeros((self.hidden_size,self.hidden_size)))\n",
    "            self.Hidden_Gradient_bias.append(np.zeros((1,self.hidden_size)))\n",
    "         \n",
    "\n",
    "\n",
    "        self.output_weight = np.random.randn(self.hidden_size,self.output_size)\n",
    "        self.Output_Gradient_weights = np.zeros((self.hidden_size,self.output_size))\n",
    "        self.output_bias = np.zeros((1,self.output_size))\n",
    "        self.Output_Gradient_bias = np.zeros((1,self.output_size))\n",
    "\n",
    "\n",
    "    def sigmoid(self,z):\n",
    "        return (1/(1+np.exp(-z)))\n",
    "    \n",
    "    def derivative_sigmoid(self,z):\n",
    "        s = self.sigmoid(z)\n",
    "        return s * (1-s)\n",
    "    \n",
    "    def Relu(self,z):\n",
    "        return np.maximum(0,z)\n",
    "\n",
    "    def derivative_relu(self,z):\n",
    "        return (z > 0 ).astype(float)\n",
    "    \n",
    "    def compute_loss(self,preds,Y):\n",
    "        preds = np.clip(preds,1e-8,1-1e-8) \n",
    "        return -np.mean(Y*np.log(preds)+(1-Y)*np.log(1-preds)) \n",
    "    \n",
    "    def Dropout(self,A):\n",
    "        mask = np.random.rand(*A.shape) < self.dropout\n",
    "        return (mask * A ) / self.dropout\n",
    "\n",
    "    \n",
    "    def ForwardPropagation(self,X):\n",
    "        self.Activations = [X]\n",
    "        self.hidden_Z = []\n",
    "\n",
    "        for l in range(self.hidden_layers):\n",
    "            Z = np.dot(self.Activations[l],self.hidden_weights[l]) + self.hidden_bias[l] \n",
    "            self.hidden_Z.append(Z)\n",
    "            A = self.Relu(Z) \n",
    "            self.Activations.append(self.Dropout(A))\n",
    "\n",
    "        self.output_Z = np.dot(self.Activations[-1],self.output_weight) + self.output_bias \n",
    "        self.output_A = self.sigmoid(self.output_Z) \n",
    "        return self.output_A\n",
    "\n",
    "    def BackPropagation(self,X,Y):\n",
    "        m = X.shape[0]\n",
    "        alpha = 1e-8\n",
    "        error = self.output_A - Y \n",
    "        Dwo = 1/m * np.dot(self.Activations[-1].T,error)\n",
    "        Dbo = 1/m * np.sum(error,keepdims=True,axis=0) \n",
    "\n",
    "        G = self.Output_Gradient_weights + np.square(Dwo)\n",
    "        self.Output_Gradient_weights = G \n",
    "\n",
    "        G = self.Output_Gradient_bias + np.square(Dbo)\n",
    "        self.Output_Gradient_bias = G \n",
    "\n",
    "\n",
    "        self.output_weight =  self.output_weight - (self.learning_rate /  (self.Output_Gradient_weights + alpha)) * Dwo\n",
    "        self.output_bias = self.output_bias - (self.learning_rate /  (self.Output_Gradient_bias + alpha)) * Dbo\n",
    "        for l in reversed(range(self.hidden_layers)):\n",
    "            \n",
    "            da = np.dot(error,self.output_weight.T if l == self.hidden_layers-1 else self.hidden_weights[l+1].T)\n",
    "            dz = da * self.derivative_relu(self.hidden_Z[l]) # final errors \n",
    "            dw = 1/m * np.dot(self.Activations[l].T,dz)\n",
    "            db = 1/m * np.sum(dz,keepdims=True,axis=0)\n",
    "            error = dz \n",
    "\n",
    "            G = self.Hidden_Gradient_weights[l] +  np.square(dw)\n",
    "            self.Hidden_Gradient_weights[l] = G \n",
    "            G = self.Hidden_Gradient_bias[l] + np.square(db)\n",
    "            self.Hidden_Gradient_bias[l] = G \n",
    "\n",
    "            self.hidden_weights[l] = self.hidden_weights[l] - self.learning_rate / np.sqrt(self.Hidden_Gradient_weights[l]+alpha) * dw \n",
    "            self.hidden_bias[l] = self.hidden_bias[l] - self.learning_rate / np.sqrt(self.Hidden_Gradient_bias[l]+alpha) * db  \n",
    "\n",
    "    def fit(self,X,Y):\n",
    "        Y = np.array(Y).reshape(-1,1)\n",
    "        for epoch in range(self.epoches):\n",
    "                preds_F = self.ForwardPropagation(X)\n",
    "                loss = self.compute_loss(preds_F,Y)\n",
    "                print(f\"epoch : {epoch+1}, loss : {loss}\")\n",
    "                self.BackPropagation(X,Y)\n",
    "\n",
    "    def predict(self,X):\n",
    "        return (self.ForwardPropagation(X) > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17900ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.62160</td>\n",
       "      <td>8.6661</td>\n",
       "      <td>-2.8073</td>\n",
       "      <td>-0.44699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.1674</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.6383</td>\n",
       "      <td>1.9242</td>\n",
       "      <td>0.10645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.5228</td>\n",
       "      <td>-4.0112</td>\n",
       "      <td>-3.59440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.4552</td>\n",
       "      <td>4.5718</td>\n",
       "      <td>-0.98880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variance  skewness  curtosis  entropy  class\n",
       "0   3.62160    8.6661   -2.8073 -0.44699      0\n",
       "1   4.54590    8.1674   -2.4586 -1.46210      0\n",
       "2   3.86600   -2.6383    1.9242  0.10645      0\n",
       "3   3.45660    9.5228   -4.0112 -3.59440      0\n",
       "4   0.32924   -4.4552    4.5718 -0.98880      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "dataset = pd.read_csv(\"../BankNote_Authentication.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05df4745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1029, 1)\n"
     ]
    }
   ],
   "source": [
    "Y = np.array(dataset['class']).reshape(-1,1)\n",
    "X = np.array(dataset.drop(columns=['class']))\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y)\n",
    "\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd1d14ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1, loss : 0.9360051771934128\n",
      "epoch : 2, loss : 0.9101914752367453\n",
      "epoch : 3, loss : 0.9270095905568606\n",
      "epoch : 4, loss : 0.889682918066547\n",
      "epoch : 5, loss : 0.8568119345054153\n",
      "epoch : 6, loss : 0.8777776457450067\n",
      "epoch : 7, loss : 0.8929454044874003\n",
      "epoch : 8, loss : 0.8731943406975908\n",
      "epoch : 9, loss : 0.8402459383264033\n",
      "epoch : 10, loss : 0.8335838556182874\n",
      "epoch : 11, loss : 0.8642463300619395\n",
      "epoch : 12, loss : 0.8357350750848817\n",
      "epoch : 13, loss : 0.8257039181420649\n",
      "epoch : 14, loss : 0.8306871472638974\n",
      "epoch : 15, loss : 0.8408984995431141\n",
      "epoch : 16, loss : 0.7835223766862123\n",
      "epoch : 17, loss : 0.7999785626496767\n",
      "epoch : 18, loss : 0.8146386405494963\n",
      "epoch : 19, loss : 0.7926720015440032\n",
      "epoch : 20, loss : 0.8190287838325742\n",
      "epoch : 21, loss : 0.8020725880485338\n",
      "epoch : 22, loss : 0.7790585021378248\n",
      "epoch : 23, loss : 0.7838561544002728\n",
      "epoch : 24, loss : 0.7960905940138663\n",
      "epoch : 25, loss : 0.8259501936090766\n",
      "epoch : 26, loss : 0.7740830990524643\n",
      "epoch : 27, loss : 0.7959514628608549\n",
      "epoch : 28, loss : 0.8044626225106202\n",
      "epoch : 29, loss : 0.8026380894914131\n",
      "epoch : 30, loss : 0.797964232695944\n",
      "epoch : 31, loss : 0.7799519846891783\n",
      "epoch : 32, loss : 0.7511642918674359\n",
      "epoch : 33, loss : 0.7884500916030392\n",
      "epoch : 34, loss : 0.8225134867800802\n",
      "epoch : 35, loss : 0.786476708119349\n",
      "epoch : 36, loss : 0.7830683806033315\n",
      "epoch : 37, loss : 0.7675575525149245\n",
      "epoch : 38, loss : 0.7926040215566099\n",
      "epoch : 39, loss : 0.7625813867630895\n",
      "epoch : 40, loss : 0.7540163848046182\n",
      "epoch : 41, loss : 0.7458668727341394\n",
      "epoch : 42, loss : 0.7696029560351981\n",
      "epoch : 43, loss : 0.76333608749574\n",
      "epoch : 44, loss : 0.7907781972995395\n",
      "epoch : 45, loss : 0.7978470955674881\n",
      "epoch : 46, loss : 0.7940728599636977\n",
      "epoch : 47, loss : 0.7545034660764954\n",
      "epoch : 48, loss : 0.743675620464458\n",
      "epoch : 49, loss : 0.728929390516195\n",
      "epoch : 50, loss : 0.7768789100190239\n",
      "epoch : 51, loss : 0.7320712821815247\n",
      "epoch : 52, loss : 0.7794572999893757\n",
      "epoch : 53, loss : 0.765505693793235\n",
      "epoch : 54, loss : 0.7637583085820098\n",
      "epoch : 55, loss : 0.7564625663347766\n",
      "epoch : 56, loss : 0.752635690691074\n",
      "epoch : 57, loss : 0.7426917808056003\n",
      "epoch : 58, loss : 0.7321574466055413\n",
      "epoch : 59, loss : 0.732537860656212\n",
      "epoch : 60, loss : 0.7300289486430355\n",
      "epoch : 61, loss : 0.7625163702456688\n",
      "epoch : 62, loss : 0.7192751221789773\n",
      "epoch : 63, loss : 0.7683898233215098\n",
      "epoch : 64, loss : 0.7662186499059792\n",
      "epoch : 65, loss : 0.7601610802168913\n",
      "epoch : 66, loss : 0.731836909799856\n",
      "epoch : 67, loss : 0.7856246026067184\n",
      "epoch : 68, loss : 0.7781902811220013\n",
      "epoch : 69, loss : 0.7363614279982457\n",
      "epoch : 70, loss : 0.7625500475318848\n",
      "epoch : 71, loss : 0.7437811765848721\n",
      "epoch : 72, loss : 0.7325514911495955\n",
      "epoch : 73, loss : 0.7165358019705808\n",
      "epoch : 74, loss : 0.7669753010707764\n",
      "epoch : 75, loss : 0.7376971415904462\n",
      "epoch : 76, loss : 0.7039748896938012\n",
      "epoch : 77, loss : 0.729581649425399\n",
      "epoch : 78, loss : 0.7868069153249229\n",
      "epoch : 79, loss : 0.7160817764817671\n",
      "epoch : 80, loss : 0.7551840505009991\n",
      "epoch : 81, loss : 0.7388250279500534\n",
      "epoch : 82, loss : 0.7576520867857467\n",
      "epoch : 83, loss : 0.7178995451880279\n",
      "epoch : 84, loss : 0.7588721835929313\n",
      "epoch : 85, loss : 0.7307480935438199\n",
      "epoch : 86, loss : 0.7280146981353861\n",
      "epoch : 87, loss : 0.7738065237592349\n",
      "epoch : 88, loss : 0.7607785153363762\n",
      "epoch : 89, loss : 0.7158385368960244\n",
      "epoch : 90, loss : 0.7279168639367714\n",
      "epoch : 91, loss : 0.7228471182168611\n",
      "epoch : 92, loss : 0.7605252342130853\n",
      "epoch : 93, loss : 0.7517311610355298\n",
      "epoch : 94, loss : 0.7596725181095405\n",
      "epoch : 95, loss : 0.7413201649263499\n",
      "epoch : 96, loss : 0.7592116542848202\n",
      "epoch : 97, loss : 0.7115541132220276\n",
      "epoch : 98, loss : 0.7261640086537268\n",
      "epoch : 99, loss : 0.7402161930863044\n",
      "epoch : 100, loss : 0.6998932688585445\n",
      "epoch : 101, loss : 0.7264021673802629\n",
      "epoch : 102, loss : 0.6897626110150382\n",
      "epoch : 103, loss : 0.7211414290132323\n",
      "epoch : 104, loss : 0.7329231403853658\n",
      "epoch : 105, loss : 0.7057320195048382\n",
      "epoch : 106, loss : 0.7276262757692459\n",
      "epoch : 107, loss : 0.7008123143138729\n",
      "epoch : 108, loss : 0.716808861976829\n",
      "epoch : 109, loss : 0.724974539231321\n",
      "epoch : 110, loss : 0.7216103151201554\n",
      "epoch : 111, loss : 0.7237598919591764\n",
      "epoch : 112, loss : 0.7402648475322887\n",
      "epoch : 113, loss : 0.7138299393447158\n",
      "epoch : 114, loss : 0.7041859763547533\n",
      "epoch : 115, loss : 0.7024460141225793\n",
      "epoch : 116, loss : 0.6974429734818837\n",
      "epoch : 117, loss : 0.7254461289109588\n",
      "epoch : 118, loss : 0.7235229435657172\n",
      "epoch : 119, loss : 0.7144343587151636\n",
      "epoch : 120, loss : 0.6862643666929766\n",
      "epoch : 121, loss : 0.7168689046703962\n",
      "epoch : 122, loss : 0.7085160340793217\n",
      "epoch : 123, loss : 0.7302591889934934\n",
      "epoch : 124, loss : 0.7294428224635578\n",
      "epoch : 125, loss : 0.7022326682679757\n",
      "epoch : 126, loss : 0.7231472509368795\n",
      "epoch : 127, loss : 0.7169346358093015\n",
      "epoch : 128, loss : 0.6923213054468396\n",
      "epoch : 129, loss : 0.725190116086993\n",
      "epoch : 130, loss : 0.6935795977958831\n",
      "epoch : 131, loss : 0.687850515278909\n",
      "epoch : 132, loss : 0.7291442507915331\n",
      "epoch : 133, loss : 0.6934412705469679\n",
      "epoch : 134, loss : 0.7279681664339314\n",
      "epoch : 135, loss : 0.7073548782569848\n",
      "epoch : 136, loss : 0.7145286554104765\n",
      "epoch : 137, loss : 0.7072733297519866\n",
      "epoch : 138, loss : 0.7207772758024414\n",
      "epoch : 139, loss : 0.7012846994537317\n",
      "epoch : 140, loss : 0.7227297661468512\n",
      "epoch : 141, loss : 0.6863418043477332\n",
      "epoch : 142, loss : 0.6906971145105419\n",
      "epoch : 143, loss : 0.7308942993410604\n",
      "epoch : 144, loss : 0.7113353163062363\n",
      "epoch : 145, loss : 0.6867927858420626\n",
      "epoch : 146, loss : 0.7229479765235903\n",
      "epoch : 147, loss : 0.7325922425063278\n",
      "epoch : 148, loss : 0.7381101341732899\n",
      "epoch : 149, loss : 0.6796277022961976\n",
      "epoch : 150, loss : 0.7050802725311807\n",
      "epoch : 151, loss : 0.7014244137000482\n",
      "epoch : 152, loss : 0.6774492735641259\n",
      "epoch : 153, loss : 0.7222336069915648\n",
      "epoch : 154, loss : 0.6985251260804584\n",
      "epoch : 155, loss : 0.7445770528970457\n",
      "epoch : 156, loss : 0.7355110783415078\n",
      "epoch : 157, loss : 0.6706830539925356\n",
      "epoch : 158, loss : 0.6818047767875665\n",
      "epoch : 159, loss : 0.6904527212869013\n",
      "epoch : 160, loss : 0.7091276670556753\n",
      "epoch : 161, loss : 0.6939545847473318\n",
      "epoch : 162, loss : 0.6972715601134443\n",
      "epoch : 163, loss : 0.723140305597525\n",
      "epoch : 164, loss : 0.6833503015181236\n",
      "epoch : 165, loss : 0.6918113805077505\n",
      "epoch : 166, loss : 0.7183970551301075\n",
      "epoch : 167, loss : 0.700798322129962\n",
      "epoch : 168, loss : 0.7156561068329502\n",
      "epoch : 169, loss : 0.6849159553763978\n",
      "epoch : 170, loss : 0.6992175747923007\n",
      "epoch : 171, loss : 0.720384099222055\n",
      "epoch : 172, loss : 0.7141049032999996\n",
      "epoch : 173, loss : 0.728514911823929\n",
      "epoch : 174, loss : 0.7188317252118708\n",
      "epoch : 175, loss : 0.7127214723458515\n",
      "epoch : 176, loss : 0.7024253436170147\n",
      "epoch : 177, loss : 0.6800445849012771\n",
      "epoch : 178, loss : 0.6907962038681459\n",
      "epoch : 179, loss : 0.687234979221116\n",
      "epoch : 180, loss : 0.6809701644879008\n",
      "epoch : 181, loss : 0.6615145074599404\n",
      "epoch : 182, loss : 0.6826860858411369\n",
      "epoch : 183, loss : 0.6513052947236599\n",
      "epoch : 184, loss : 0.6968541056080337\n",
      "epoch : 185, loss : 0.6837658790975727\n",
      "epoch : 186, loss : 0.7128641972106512\n",
      "epoch : 187, loss : 0.6806198861497429\n",
      "epoch : 188, loss : 0.7174093696521456\n",
      "epoch : 189, loss : 0.6664816922429133\n",
      "epoch : 190, loss : 0.6662981195879775\n",
      "epoch : 191, loss : 0.6808433823468447\n",
      "epoch : 192, loss : 0.6987104371829773\n",
      "epoch : 193, loss : 0.6700158182679224\n",
      "epoch : 194, loss : 0.6690681312023625\n",
      "epoch : 195, loss : 0.6878122472469912\n",
      "epoch : 196, loss : 0.6829206905020057\n",
      "epoch : 197, loss : 0.6904896193700556\n",
      "epoch : 198, loss : 0.6982020710974097\n",
      "epoch : 199, loss : 0.6591945218185575\n",
      "epoch : 200, loss : 0.7016235475947588\n",
      "epoch : 201, loss : 0.6848371401908058\n",
      "epoch : 202, loss : 0.6955870101796431\n",
      "epoch : 203, loss : 0.6963171716961064\n",
      "epoch : 204, loss : 0.6861612964921213\n",
      "epoch : 205, loss : 0.6964409344317664\n",
      "epoch : 206, loss : 0.6703322348152643\n",
      "epoch : 207, loss : 0.7011933302047142\n",
      "epoch : 208, loss : 0.6731573809466613\n",
      "epoch : 209, loss : 0.6385054875307768\n",
      "epoch : 210, loss : 0.700502654506492\n",
      "epoch : 211, loss : 0.719807746922654\n",
      "epoch : 212, loss : 0.7221875404141713\n",
      "epoch : 213, loss : 0.6736090373629372\n",
      "epoch : 214, loss : 0.643783952319198\n",
      "epoch : 215, loss : 0.6604693587490537\n",
      "epoch : 216, loss : 0.7072836819251112\n",
      "epoch : 217, loss : 0.7022637116855357\n",
      "epoch : 218, loss : 0.6884474876310688\n",
      "epoch : 219, loss : 0.6767260782320722\n",
      "epoch : 220, loss : 0.7113208372600025\n",
      "epoch : 221, loss : 0.6722358858788845\n",
      "epoch : 222, loss : 0.6957883218960507\n",
      "epoch : 223, loss : 0.6326298659110485\n",
      "epoch : 224, loss : 0.6711692951620024\n",
      "epoch : 225, loss : 0.6739490884221013\n",
      "epoch : 226, loss : 0.6638960894366316\n",
      "epoch : 227, loss : 0.6666291220218344\n",
      "epoch : 228, loss : 0.6963697585445506\n",
      "epoch : 229, loss : 0.6563876894698186\n",
      "epoch : 230, loss : 0.6362527107459155\n",
      "epoch : 231, loss : 0.678048335456082\n",
      "epoch : 232, loss : 0.6475522981255202\n",
      "epoch : 233, loss : 0.6550363608857468\n",
      "epoch : 234, loss : 0.6714772982025986\n",
      "epoch : 235, loss : 0.6826803449408253\n",
      "epoch : 236, loss : 0.6515567767312881\n",
      "epoch : 237, loss : 0.6331262575465811\n",
      "epoch : 238, loss : 0.6953400553946238\n",
      "epoch : 239, loss : 0.6726211362215483\n",
      "epoch : 240, loss : 0.6452655707970194\n",
      "epoch : 241, loss : 0.6666393417532995\n",
      "epoch : 242, loss : 0.6450482958648173\n",
      "epoch : 243, loss : 0.6814602294534062\n",
      "epoch : 244, loss : 0.6698894242497595\n",
      "epoch : 245, loss : 0.6816912226061931\n",
      "epoch : 246, loss : 0.6795182592514812\n",
      "epoch : 247, loss : 0.6415475177781785\n",
      "epoch : 248, loss : 0.6720308593985598\n",
      "epoch : 249, loss : 0.6866792010485763\n",
      "epoch : 250, loss : 0.6458978599123324\n",
      "epoch : 251, loss : 0.6696056538676384\n",
      "epoch : 252, loss : 0.663223574348019\n",
      "epoch : 253, loss : 0.6302132289404557\n",
      "epoch : 254, loss : 0.683858311530977\n",
      "epoch : 255, loss : 0.6746656216363687\n",
      "epoch : 256, loss : 0.6258160258927603\n",
      "epoch : 257, loss : 0.6663570152281766\n",
      "epoch : 258, loss : 0.687135517438618\n",
      "epoch : 259, loss : 0.6809981536373045\n",
      "epoch : 260, loss : 0.6801908230334572\n",
      "epoch : 261, loss : 0.7005609088671857\n",
      "epoch : 262, loss : 0.6420985773796868\n",
      "epoch : 263, loss : 0.6559101108554893\n",
      "epoch : 264, loss : 0.6512353688483156\n",
      "epoch : 265, loss : 0.670832949329055\n",
      "epoch : 266, loss : 0.6603036115762251\n",
      "epoch : 267, loss : 0.6375917767971911\n",
      "epoch : 268, loss : 0.6473869185349914\n",
      "epoch : 269, loss : 0.6469411806452828\n",
      "epoch : 270, loss : 0.6825115088415706\n",
      "epoch : 271, loss : 0.6792862993718446\n",
      "epoch : 272, loss : 0.6970905902466389\n",
      "epoch : 273, loss : 0.669364996880579\n",
      "epoch : 274, loss : 0.6459574947607226\n",
      "epoch : 275, loss : 0.6463868784294086\n",
      "epoch : 276, loss : 0.6677609960864294\n",
      "epoch : 277, loss : 0.6518241163505114\n",
      "epoch : 278, loss : 0.6770989429657351\n",
      "epoch : 279, loss : 0.708137137754112\n",
      "epoch : 280, loss : 0.6597442162191588\n",
      "epoch : 281, loss : 0.6472993015683338\n",
      "epoch : 282, loss : 0.6860983704974095\n",
      "epoch : 283, loss : 0.622116436806637\n",
      "epoch : 284, loss : 0.6665023964069124\n",
      "epoch : 285, loss : 0.6522682892974933\n",
      "epoch : 286, loss : 0.6719615419438273\n",
      "epoch : 287, loss : 0.655095004926491\n",
      "epoch : 288, loss : 0.6534671571200089\n",
      "epoch : 289, loss : 0.6637806568182957\n",
      "epoch : 290, loss : 0.6382845279087161\n",
      "epoch : 291, loss : 0.6190015468808509\n",
      "epoch : 292, loss : 0.6847044235280428\n",
      "epoch : 293, loss : 0.6566089208267454\n",
      "epoch : 294, loss : 0.6586007052599357\n",
      "epoch : 295, loss : 0.646295658479037\n",
      "epoch : 296, loss : 0.6793981163651539\n",
      "epoch : 297, loss : 0.6430811888029103\n",
      "epoch : 298, loss : 0.6687327089319408\n",
      "epoch : 299, loss : 0.6549881960176511\n",
      "epoch : 300, loss : 0.676077482270734\n",
      "epoch : 301, loss : 0.6549970160827207\n",
      "epoch : 302, loss : 0.6716065190909446\n",
      "epoch : 303, loss : 0.662523512692747\n",
      "epoch : 304, loss : 0.6498649670994312\n",
      "epoch : 305, loss : 0.6615780353255565\n",
      "epoch : 306, loss : 0.67697564427703\n",
      "epoch : 307, loss : 0.6943901527096351\n",
      "epoch : 308, loss : 0.6522421221847767\n",
      "epoch : 309, loss : 0.6213788200758522\n",
      "epoch : 310, loss : 0.6817248459724187\n",
      "epoch : 311, loss : 0.6357525672821609\n",
      "epoch : 312, loss : 0.6427750908574338\n",
      "epoch : 313, loss : 0.6579100250623526\n",
      "epoch : 314, loss : 0.6290651169652978\n",
      "epoch : 315, loss : 0.6647664592209894\n",
      "epoch : 316, loss : 0.6475519118570396\n",
      "epoch : 317, loss : 0.6125502416361004\n",
      "epoch : 318, loss : 0.6317479488568559\n",
      "epoch : 319, loss : 0.6350275188215958\n",
      "epoch : 320, loss : 0.6476344606874094\n",
      "epoch : 321, loss : 0.6388953080974814\n",
      "epoch : 322, loss : 0.6487739272667444\n",
      "epoch : 323, loss : 0.6578478710392762\n",
      "epoch : 324, loss : 0.6418619078331805\n",
      "epoch : 325, loss : 0.6521859161903808\n",
      "epoch : 326, loss : 0.6826908454104065\n",
      "epoch : 327, loss : 0.6690036053800281\n",
      "epoch : 328, loss : 0.6980054714832897\n",
      "epoch : 329, loss : 0.6678658471814175\n",
      "epoch : 330, loss : 0.666497499696117\n",
      "epoch : 331, loss : 0.6579487997315273\n",
      "epoch : 332, loss : 0.6605109170269182\n",
      "epoch : 333, loss : 0.6592801550609068\n",
      "epoch : 334, loss : 0.6485282046759087\n",
      "epoch : 335, loss : 0.6394660818027493\n",
      "epoch : 336, loss : 0.6262571196206004\n",
      "epoch : 337, loss : 0.6062162749310073\n",
      "epoch : 338, loss : 0.6620791254539401\n",
      "epoch : 339, loss : 0.6432530854716865\n",
      "epoch : 340, loss : 0.6650806204272929\n",
      "epoch : 341, loss : 0.6652809516342294\n",
      "epoch : 342, loss : 0.6430059493030663\n",
      "epoch : 343, loss : 0.6384631938991091\n",
      "epoch : 344, loss : 0.6377063053092445\n",
      "epoch : 345, loss : 0.6474457975544933\n",
      "epoch : 346, loss : 0.6374886169654214\n",
      "epoch : 347, loss : 0.6044064741677658\n",
      "epoch : 348, loss : 0.6433327214119781\n",
      "epoch : 349, loss : 0.6037216426898994\n",
      "epoch : 350, loss : 0.631988106701604\n",
      "epoch : 351, loss : 0.6400007723483007\n",
      "epoch : 352, loss : 0.6111695970221167\n",
      "epoch : 353, loss : 0.6277491946090455\n",
      "epoch : 354, loss : 0.5993071726970195\n",
      "epoch : 355, loss : 0.6280728278294608\n",
      "epoch : 356, loss : 0.6410070108914174\n",
      "epoch : 357, loss : 0.6164085329161689\n",
      "epoch : 358, loss : 0.630859231415842\n",
      "epoch : 359, loss : 0.6249540854149606\n",
      "epoch : 360, loss : 0.6431965897398902\n",
      "epoch : 361, loss : 0.5874212800569185\n",
      "epoch : 362, loss : 0.6588230045618367\n",
      "epoch : 363, loss : 0.6726038291372965\n",
      "epoch : 364, loss : 0.6542835443090029\n",
      "epoch : 365, loss : 0.6214421408894472\n",
      "epoch : 366, loss : 0.6744087860789116\n",
      "epoch : 367, loss : 0.6613118300186795\n",
      "epoch : 368, loss : 0.6426596055857595\n",
      "epoch : 369, loss : 0.6229981283609342\n",
      "epoch : 370, loss : 0.6398425153444276\n",
      "epoch : 371, loss : 0.6203948800147504\n",
      "epoch : 372, loss : 0.6205566795868828\n",
      "epoch : 373, loss : 0.6212538649049556\n",
      "epoch : 374, loss : 0.6198346009023261\n",
      "epoch : 375, loss : 0.6685161587395826\n",
      "epoch : 376, loss : 0.6290719182211734\n",
      "epoch : 377, loss : 0.5965182313735258\n",
      "epoch : 378, loss : 0.6528554061407477\n",
      "epoch : 379, loss : 0.6055911584895203\n",
      "epoch : 380, loss : 0.6263100535281292\n",
      "epoch : 381, loss : 0.6175490391524815\n",
      "epoch : 382, loss : 0.6172453307978742\n",
      "epoch : 383, loss : 0.6464129295954205\n",
      "epoch : 384, loss : 0.6451523703643396\n",
      "epoch : 385, loss : 0.6177811818995272\n",
      "epoch : 386, loss : 0.6199990314112588\n",
      "epoch : 387, loss : 0.6231447834165317\n",
      "epoch : 388, loss : 0.6403180431457294\n",
      "epoch : 389, loss : 0.608389049406891\n",
      "epoch : 390, loss : 0.6094883686386608\n",
      "epoch : 391, loss : 0.6565599340939221\n",
      "epoch : 392, loss : 0.6262044086884873\n",
      "epoch : 393, loss : 0.6072096211021863\n",
      "epoch : 394, loss : 0.6300618195628935\n",
      "epoch : 395, loss : 0.6544241330431154\n",
      "epoch : 396, loss : 0.6105890132471761\n",
      "epoch : 397, loss : 0.6487354906314156\n",
      "epoch : 398, loss : 0.6441660813313386\n",
      "epoch : 399, loss : 0.5872817307966771\n",
      "epoch : 400, loss : 0.6277504325058443\n",
      "epoch : 401, loss : 0.6335151549965151\n",
      "epoch : 402, loss : 0.6210958136869413\n",
      "epoch : 403, loss : 0.6212395910044787\n",
      "epoch : 404, loss : 0.6232163621016233\n",
      "epoch : 405, loss : 0.657437753904904\n",
      "epoch : 406, loss : 0.6141796117732603\n",
      "epoch : 407, loss : 0.6028915030736733\n",
      "epoch : 408, loss : 0.6299552703840023\n",
      "epoch : 409, loss : 0.5892234736833799\n",
      "epoch : 410, loss : 0.6488463344444256\n",
      "epoch : 411, loss : 0.6357571454714537\n",
      "epoch : 412, loss : 0.6305255702364618\n",
      "epoch : 413, loss : 0.6236663559926799\n",
      "epoch : 414, loss : 0.5806442839016795\n",
      "epoch : 415, loss : 0.6271767271205292\n",
      "epoch : 416, loss : 0.5827137077874928\n",
      "epoch : 417, loss : 0.6371828978583968\n",
      "epoch : 418, loss : 0.6181914812234355\n",
      "epoch : 419, loss : 0.6183079697869727\n",
      "epoch : 420, loss : 0.577420097104349\n",
      "epoch : 421, loss : 0.6221050525196659\n",
      "epoch : 422, loss : 0.606212889538316\n",
      "epoch : 423, loss : 0.6078596233190012\n",
      "epoch : 424, loss : 0.6177372920087946\n",
      "epoch : 425, loss : 0.6095014876082855\n",
      "epoch : 426, loss : 0.6062624182789136\n",
      "epoch : 427, loss : 0.6080460764410305\n",
      "epoch : 428, loss : 0.626515495770639\n",
      "epoch : 429, loss : 0.6116492055338043\n",
      "epoch : 430, loss : 0.5805624974436384\n",
      "epoch : 431, loss : 0.6534635213270484\n",
      "epoch : 432, loss : 0.6337555786927154\n",
      "epoch : 433, loss : 0.644792782102323\n",
      "epoch : 434, loss : 0.6082422425282482\n",
      "epoch : 435, loss : 0.5905417468691895\n",
      "epoch : 436, loss : 0.6456673680924118\n",
      "epoch : 437, loss : 0.5921739217221753\n",
      "epoch : 438, loss : 0.630923171906941\n",
      "epoch : 439, loss : 0.5850658524529646\n",
      "epoch : 440, loss : 0.628874630301641\n",
      "epoch : 441, loss : 0.6434877534245504\n",
      "epoch : 442, loss : 0.5924597624019686\n",
      "epoch : 443, loss : 0.6203128810649358\n",
      "epoch : 444, loss : 0.6314244618423288\n",
      "epoch : 445, loss : 0.6298913396638085\n",
      "epoch : 446, loss : 0.6078635580640692\n",
      "epoch : 447, loss : 0.5941955978605238\n",
      "epoch : 448, loss : 0.6420059152668759\n",
      "epoch : 449, loss : 0.5652794014277961\n",
      "epoch : 450, loss : 0.6165112494385384\n",
      "epoch : 451, loss : 0.5996886638142248\n",
      "epoch : 452, loss : 0.6467030835399493\n",
      "epoch : 453, loss : 0.626071646940278\n",
      "epoch : 454, loss : 0.5749916788807363\n",
      "epoch : 455, loss : 0.610471446752426\n",
      "epoch : 456, loss : 0.618431819358049\n",
      "epoch : 457, loss : 0.6191231088964096\n",
      "epoch : 458, loss : 0.6103747239476569\n",
      "epoch : 459, loss : 0.5733657577713241\n",
      "epoch : 460, loss : 0.6228748679388318\n",
      "epoch : 461, loss : 0.6106044874603457\n",
      "epoch : 462, loss : 0.6102753586259382\n",
      "epoch : 463, loss : 0.6031325908925501\n",
      "epoch : 464, loss : 0.6142356375442696\n",
      "epoch : 465, loss : 0.5887426566200622\n",
      "epoch : 466, loss : 0.6024624644287666\n",
      "epoch : 467, loss : 0.5914756604455351\n",
      "epoch : 468, loss : 0.6280773032788709\n",
      "epoch : 469, loss : 0.6452666468848015\n",
      "epoch : 470, loss : 0.6079273989101188\n",
      "epoch : 471, loss : 0.6051191907251359\n",
      "epoch : 472, loss : 0.5848763780968355\n",
      "epoch : 473, loss : 0.6141786020066344\n",
      "epoch : 474, loss : 0.6340737209328529\n",
      "epoch : 475, loss : 0.6005979829996747\n",
      "epoch : 476, loss : 0.6260088884472137\n",
      "epoch : 477, loss : 0.6235681721489809\n",
      "epoch : 478, loss : 0.6208254600929198\n",
      "epoch : 479, loss : 0.6068301720124405\n",
      "epoch : 480, loss : 0.6154024332459291\n",
      "epoch : 481, loss : 0.5837616651526417\n",
      "epoch : 482, loss : 0.6332877084375067\n",
      "epoch : 483, loss : 0.62474070883158\n",
      "epoch : 484, loss : 0.597077241578262\n",
      "epoch : 485, loss : 0.5721591176716159\n",
      "epoch : 486, loss : 0.6271728931647763\n",
      "epoch : 487, loss : 0.5967303518506585\n",
      "epoch : 488, loss : 0.5940286954620614\n",
      "epoch : 489, loss : 0.5809464688841444\n",
      "epoch : 490, loss : 0.6057001920056554\n",
      "epoch : 491, loss : 0.6015811089673722\n",
      "epoch : 492, loss : 0.5925577044090111\n",
      "epoch : 493, loss : 0.5979398345904751\n",
      "epoch : 494, loss : 0.6315909051372282\n",
      "epoch : 495, loss : 0.5920269777414794\n",
      "epoch : 496, loss : 0.5878212609888004\n",
      "epoch : 497, loss : 0.6193233044217099\n",
      "epoch : 498, loss : 0.6246285785178343\n",
      "epoch : 499, loss : 0.5750612811762434\n",
      "epoch : 500, loss : 0.6141945182173231\n",
      "epoch : 501, loss : 0.5739786880794563\n",
      "epoch : 502, loss : 0.6021860223163021\n",
      "epoch : 503, loss : 0.6192722255553412\n",
      "epoch : 504, loss : 0.605026843543959\n",
      "epoch : 505, loss : 0.5997407581788206\n",
      "epoch : 506, loss : 0.5906829772579432\n",
      "epoch : 507, loss : 0.5883058139632035\n",
      "epoch : 508, loss : 0.5789500352433846\n",
      "epoch : 509, loss : 0.5833923617320229\n",
      "epoch : 510, loss : 0.6168052760128603\n",
      "epoch : 511, loss : 0.6482970103751736\n",
      "epoch : 512, loss : 0.6085185402313273\n",
      "epoch : 513, loss : 0.607480222111688\n",
      "epoch : 514, loss : 0.5926041475614862\n",
      "epoch : 515, loss : 0.5843631536628946\n",
      "epoch : 516, loss : 0.5942100701791361\n",
      "epoch : 517, loss : 0.5806664098587734\n",
      "epoch : 518, loss : 0.5787337147064218\n",
      "epoch : 519, loss : 0.5929064908423366\n",
      "epoch : 520, loss : 0.57341079534748\n",
      "epoch : 521, loss : 0.5932911624852824\n",
      "epoch : 522, loss : 0.6238465030050888\n",
      "epoch : 523, loss : 0.6027443325927039\n",
      "epoch : 524, loss : 0.5718236918025802\n",
      "epoch : 525, loss : 0.5899738353773786\n",
      "epoch : 526, loss : 0.5928462643388405\n",
      "epoch : 527, loss : 0.6195368710302829\n",
      "epoch : 528, loss : 0.6055372532565853\n",
      "epoch : 529, loss : 0.5795115651638205\n",
      "epoch : 530, loss : 0.5607691931914166\n",
      "epoch : 531, loss : 0.6330075815741235\n",
      "epoch : 532, loss : 0.5731214927223804\n",
      "epoch : 533, loss : 0.6188326195642412\n",
      "epoch : 534, loss : 0.6208466137926445\n",
      "epoch : 535, loss : 0.6135943276902716\n",
      "epoch : 536, loss : 0.5977424711134538\n",
      "epoch : 537, loss : 0.5798005041449988\n",
      "epoch : 538, loss : 0.5864120732123832\n",
      "epoch : 539, loss : 0.5715234950222617\n",
      "epoch : 540, loss : 0.5994076915582849\n",
      "epoch : 541, loss : 0.5784934507036569\n",
      "epoch : 542, loss : 0.5593359115987132\n",
      "epoch : 543, loss : 0.5653181591979433\n",
      "epoch : 544, loss : 0.5619887156361306\n",
      "epoch : 545, loss : 0.6134719566999879\n",
      "epoch : 546, loss : 0.6044410878675168\n",
      "epoch : 547, loss : 0.5617107051948701\n",
      "epoch : 548, loss : 0.5700795778449465\n",
      "epoch : 549, loss : 0.6227573188642013\n",
      "epoch : 550, loss : 0.600367614084615\n",
      "epoch : 551, loss : 0.5776234065351474\n",
      "epoch : 552, loss : 0.6087082082729323\n",
      "epoch : 553, loss : 0.5938919528674691\n",
      "epoch : 554, loss : 0.5734442097950603\n",
      "epoch : 555, loss : 0.5969410331889587\n",
      "epoch : 556, loss : 0.5990572926540045\n",
      "epoch : 557, loss : 0.5407784120637251\n",
      "epoch : 558, loss : 0.5787689448260424\n",
      "epoch : 559, loss : 0.5699053218145789\n",
      "epoch : 560, loss : 0.6056172384107985\n",
      "epoch : 561, loss : 0.5746892497597135\n",
      "epoch : 562, loss : 0.5768222375821838\n",
      "epoch : 563, loss : 0.5588348482951474\n",
      "epoch : 564, loss : 0.5602920047210583\n",
      "epoch : 565, loss : 0.6119588396219705\n",
      "epoch : 566, loss : 0.6090522127178566\n",
      "epoch : 567, loss : 0.5831874749319061\n",
      "epoch : 568, loss : 0.5542285679688449\n",
      "epoch : 569, loss : 0.5564190660455847\n",
      "epoch : 570, loss : 0.6036417086499773\n",
      "epoch : 571, loss : 0.554177976250431\n",
      "epoch : 572, loss : 0.5819842382166054\n",
      "epoch : 573, loss : 0.578811002674772\n",
      "epoch : 574, loss : 0.6054418790479396\n",
      "epoch : 575, loss : 0.5745384700272211\n",
      "epoch : 576, loss : 0.5696903530885046\n",
      "epoch : 577, loss : 0.5851266215405891\n",
      "epoch : 578, loss : 0.5636140184678977\n",
      "epoch : 579, loss : 0.5927008302457916\n",
      "epoch : 580, loss : 0.5938470433064497\n",
      "epoch : 581, loss : 0.547956931131012\n",
      "epoch : 582, loss : 0.6082700704297963\n",
      "epoch : 583, loss : 0.546480442914916\n",
      "epoch : 584, loss : 0.5678454434554179\n",
      "epoch : 585, loss : 0.5556657769631964\n",
      "epoch : 586, loss : 0.5760116188025306\n",
      "epoch : 587, loss : 0.5822475936896157\n",
      "epoch : 588, loss : 0.6067276777570393\n",
      "epoch : 589, loss : 0.562602047598716\n",
      "epoch : 590, loss : 0.5852208237727199\n",
      "epoch : 591, loss : 0.578091055754591\n",
      "epoch : 592, loss : 0.5615412850871875\n",
      "epoch : 593, loss : 0.580020976262791\n",
      "epoch : 594, loss : 0.576713416638399\n",
      "epoch : 595, loss : 0.6150319494720972\n",
      "epoch : 596, loss : 0.5670462398920113\n",
      "epoch : 597, loss : 0.6181911279368746\n",
      "epoch : 598, loss : 0.5712723139058041\n",
      "epoch : 599, loss : 0.5934216605847633\n",
      "epoch : 600, loss : 0.6010852273917545\n",
      "epoch : 601, loss : 0.594854725750326\n",
      "epoch : 602, loss : 0.539030833557336\n",
      "epoch : 603, loss : 0.5639081332291104\n",
      "epoch : 604, loss : 0.5438979172205752\n",
      "epoch : 605, loss : 0.5779452552167784\n",
      "epoch : 606, loss : 0.5655118272786638\n",
      "epoch : 607, loss : 0.5664690926020121\n",
      "epoch : 608, loss : 0.5570002419779717\n",
      "epoch : 609, loss : 0.5603762416837612\n",
      "epoch : 610, loss : 0.5977909595676094\n",
      "epoch : 611, loss : 0.5992195830180326\n",
      "epoch : 612, loss : 0.5476665169159356\n",
      "epoch : 613, loss : 0.5734438344934206\n",
      "epoch : 614, loss : 0.5881022337815338\n",
      "epoch : 615, loss : 0.5692370426723705\n",
      "epoch : 616, loss : 0.5706018011723952\n",
      "epoch : 617, loss : 0.5523543928166355\n",
      "epoch : 618, loss : 0.5751112812798378\n",
      "epoch : 619, loss : 0.5826006261681989\n",
      "epoch : 620, loss : 0.5568546793179766\n",
      "epoch : 621, loss : 0.6016659074877695\n",
      "epoch : 622, loss : 0.5399661126979168\n",
      "epoch : 623, loss : 0.5504713778278494\n",
      "epoch : 624, loss : 0.5720477799584983\n",
      "epoch : 625, loss : 0.568308497667607\n",
      "epoch : 626, loss : 0.5809507744095034\n",
      "epoch : 627, loss : 0.5713518831680832\n",
      "epoch : 628, loss : 0.5519472942763826\n",
      "epoch : 629, loss : 0.5759700319196258\n",
      "epoch : 630, loss : 0.6027550296068009\n",
      "epoch : 631, loss : 0.5409011814118344\n",
      "epoch : 632, loss : 0.5521758812651816\n",
      "epoch : 633, loss : 0.5602569404265411\n",
      "epoch : 634, loss : 0.5581249533156097\n",
      "epoch : 635, loss : 0.5504896596703767\n",
      "epoch : 636, loss : 0.5542598074443124\n",
      "epoch : 637, loss : 0.59024402890815\n",
      "epoch : 638, loss : 0.5806422858027598\n",
      "epoch : 639, loss : 0.5548277808849125\n",
      "epoch : 640, loss : 0.5948040680244296\n",
      "epoch : 641, loss : 0.5776339791013076\n",
      "epoch : 642, loss : 0.5551065475034478\n",
      "epoch : 643, loss : 0.5737566394969998\n",
      "epoch : 644, loss : 0.5476003997175017\n",
      "epoch : 645, loss : 0.6125139279608995\n",
      "epoch : 646, loss : 0.5759378156895042\n",
      "epoch : 647, loss : 0.5729409021275194\n",
      "epoch : 648, loss : 0.5464592030785543\n",
      "epoch : 649, loss : 0.5712911389694362\n",
      "epoch : 650, loss : 0.6081590929061362\n",
      "epoch : 651, loss : 0.5706096905811476\n",
      "epoch : 652, loss : 0.557867580044589\n",
      "epoch : 653, loss : 0.5731348192498429\n",
      "epoch : 654, loss : 0.5632066122422625\n",
      "epoch : 655, loss : 0.5478625205876734\n",
      "epoch : 656, loss : 0.5711744188467163\n",
      "epoch : 657, loss : 0.5690102342912764\n",
      "epoch : 658, loss : 0.5851461112850121\n",
      "epoch : 659, loss : 0.5904678988546451\n",
      "epoch : 660, loss : 0.5500932383510121\n",
      "epoch : 661, loss : 0.5694525883159958\n",
      "epoch : 662, loss : 0.5484145806298211\n",
      "epoch : 663, loss : 0.5705848602691473\n",
      "epoch : 664, loss : 0.5931994928931666\n",
      "epoch : 665, loss : 0.5528610763454236\n",
      "epoch : 666, loss : 0.5589504373372852\n",
      "epoch : 667, loss : 0.5491633401904302\n",
      "epoch : 668, loss : 0.6102173860898894\n",
      "epoch : 669, loss : 0.5676942134488651\n",
      "epoch : 670, loss : 0.5595262655428606\n",
      "epoch : 671, loss : 0.5691085728349483\n",
      "epoch : 672, loss : 0.5781078109267503\n",
      "epoch : 673, loss : 0.6026902738061721\n",
      "epoch : 674, loss : 0.5449333178683453\n",
      "epoch : 675, loss : 0.5783775229312466\n",
      "epoch : 676, loss : 0.5446350220860011\n",
      "epoch : 677, loss : 0.5458897468980213\n",
      "epoch : 678, loss : 0.5485268102137796\n",
      "epoch : 679, loss : 0.5739613490044115\n",
      "epoch : 680, loss : 0.5617853653333851\n",
      "epoch : 681, loss : 0.5320129124734576\n",
      "epoch : 682, loss : 0.5793893256040986\n",
      "epoch : 683, loss : 0.5526493164133292\n",
      "epoch : 684, loss : 0.5537606557670033\n",
      "epoch : 685, loss : 0.6027809967513821\n",
      "epoch : 686, loss : 0.5544536847405696\n",
      "epoch : 687, loss : 0.6161720378556761\n",
      "epoch : 688, loss : 0.5452601755059383\n",
      "epoch : 689, loss : 0.552400339273633\n",
      "epoch : 690, loss : 0.5429358804576591\n",
      "epoch : 691, loss : 0.564085544300241\n",
      "epoch : 692, loss : 0.5612487943500707\n",
      "epoch : 693, loss : 0.5458697190801788\n",
      "epoch : 694, loss : 0.5565182774267454\n",
      "epoch : 695, loss : 0.5885185045492463\n",
      "epoch : 696, loss : 0.5997306360721572\n",
      "epoch : 697, loss : 0.600268329258592\n",
      "epoch : 698, loss : 0.5258527756336338\n",
      "epoch : 699, loss : 0.5675935503545597\n",
      "epoch : 700, loss : 0.5712576658530374\n",
      "epoch : 701, loss : 0.5986627421056154\n",
      "epoch : 702, loss : 0.6104423010471722\n",
      "epoch : 703, loss : 0.5592194661844888\n",
      "epoch : 704, loss : 0.5594245125084876\n",
      "epoch : 705, loss : 0.5179519049602159\n",
      "epoch : 706, loss : 0.5871935490647457\n",
      "epoch : 707, loss : 0.5002659068756524\n",
      "epoch : 708, loss : 0.5097088933454461\n",
      "epoch : 709, loss : 0.5543998658622268\n",
      "epoch : 710, loss : 0.5467167341828107\n",
      "epoch : 711, loss : 0.5511839342801571\n",
      "epoch : 712, loss : 0.5415884732472461\n",
      "epoch : 713, loss : 0.5366606346238975\n",
      "epoch : 714, loss : 0.5344081510220059\n",
      "epoch : 715, loss : 0.5690093127353413\n",
      "epoch : 716, loss : 0.5826436554971728\n",
      "epoch : 717, loss : 0.5580005665772442\n",
      "epoch : 718, loss : 0.5406137277807131\n",
      "epoch : 719, loss : 0.5411005153193751\n",
      "epoch : 720, loss : 0.5040413796715553\n",
      "epoch : 721, loss : 0.5295261202785809\n",
      "epoch : 722, loss : 0.5576075842581515\n",
      "epoch : 723, loss : 0.5414632642750319\n",
      "epoch : 724, loss : 0.5475402212613427\n",
      "epoch : 725, loss : 0.5575003374810851\n",
      "epoch : 726, loss : 0.5598565499892524\n",
      "epoch : 727, loss : 0.48759475308226147\n",
      "epoch : 728, loss : 0.5873220661043315\n",
      "epoch : 729, loss : 0.530791277490769\n",
      "epoch : 730, loss : 0.5370961834764474\n",
      "epoch : 731, loss : 0.5620126471408903\n",
      "epoch : 732, loss : 0.5187003326924225\n",
      "epoch : 733, loss : 0.5185783199515189\n",
      "epoch : 734, loss : 0.538474514787935\n",
      "epoch : 735, loss : 0.5539546108404138\n",
      "epoch : 736, loss : 0.5571265324681235\n",
      "epoch : 737, loss : 0.5170308745039316\n",
      "epoch : 738, loss : 0.5639715153168371\n",
      "epoch : 739, loss : 0.5586230106713506\n",
      "epoch : 740, loss : 0.5807489497756438\n",
      "epoch : 741, loss : 0.5594351926099521\n",
      "epoch : 742, loss : 0.5466087236892296\n",
      "epoch : 743, loss : 0.5564742219790828\n",
      "epoch : 744, loss : 0.557024101664403\n",
      "epoch : 745, loss : 0.5340827595067584\n",
      "epoch : 746, loss : 0.5477820219757418\n",
      "epoch : 747, loss : 0.5483248079539979\n",
      "epoch : 748, loss : 0.5770962866102726\n",
      "epoch : 749, loss : 0.5129761300278503\n",
      "epoch : 750, loss : 0.5559013237380873\n",
      "epoch : 751, loss : 0.5755071048197455\n",
      "epoch : 752, loss : 0.5125624702243419\n",
      "epoch : 753, loss : 0.5631172069645851\n",
      "epoch : 754, loss : 0.5374922585275539\n",
      "epoch : 755, loss : 0.5701483008592579\n",
      "epoch : 756, loss : 0.5637054098214183\n",
      "epoch : 757, loss : 0.5290880084748809\n",
      "epoch : 758, loss : 0.5461308725480134\n",
      "epoch : 759, loss : 0.5471825931601242\n",
      "epoch : 760, loss : 0.554380491897136\n",
      "epoch : 761, loss : 0.5188269466428844\n",
      "epoch : 762, loss : 0.5486170619382846\n",
      "epoch : 763, loss : 0.5234431172876545\n",
      "epoch : 764, loss : 0.5263122066646923\n",
      "epoch : 765, loss : 0.4976535732752681\n",
      "epoch : 766, loss : 0.5784891088532176\n",
      "epoch : 767, loss : 0.5044793505315789\n",
      "epoch : 768, loss : 0.5587115997591321\n",
      "epoch : 769, loss : 0.5284806824808034\n",
      "epoch : 770, loss : 0.5377594467335733\n",
      "epoch : 771, loss : 0.5363133753962264\n",
      "epoch : 772, loss : 0.5385516680936122\n",
      "epoch : 773, loss : 0.5591755438936703\n",
      "epoch : 774, loss : 0.500403524308149\n",
      "epoch : 775, loss : 0.5322009513766105\n",
      "epoch : 776, loss : 0.5649064757593318\n",
      "epoch : 777, loss : 0.5376358194993824\n",
      "epoch : 778, loss : 0.49226176214984213\n",
      "epoch : 779, loss : 0.5539878888376184\n",
      "epoch : 780, loss : 0.5383867972170366\n",
      "epoch : 781, loss : 0.572099336988347\n",
      "epoch : 782, loss : 0.5672166196761557\n",
      "epoch : 783, loss : 0.5468766787890296\n",
      "epoch : 784, loss : 0.5349159146893021\n",
      "epoch : 785, loss : 0.5554832657512753\n",
      "epoch : 786, loss : 0.5340826896440076\n",
      "epoch : 787, loss : 0.548034678856827\n",
      "epoch : 788, loss : 0.5373474369175847\n",
      "epoch : 789, loss : 0.5208492339712496\n",
      "epoch : 790, loss : 0.5301948943566234\n",
      "epoch : 791, loss : 0.4821719057733864\n",
      "epoch : 792, loss : 0.5572336729751015\n",
      "epoch : 793, loss : 0.5592935404213187\n",
      "epoch : 794, loss : 0.5208881717343289\n",
      "epoch : 795, loss : 0.527859303713569\n",
      "epoch : 796, loss : 0.5016292432900499\n",
      "epoch : 797, loss : 0.5247386765847168\n",
      "epoch : 798, loss : 0.5600720981852659\n",
      "epoch : 799, loss : 0.5668307867739536\n",
      "epoch : 800, loss : 0.5011598522839444\n",
      "epoch : 801, loss : 0.562709811446201\n",
      "epoch : 802, loss : 0.531190474659522\n",
      "epoch : 803, loss : 0.5705311981763083\n",
      "epoch : 804, loss : 0.5207493884977374\n",
      "epoch : 805, loss : 0.5110449420724475\n",
      "epoch : 806, loss : 0.5095328906620595\n",
      "epoch : 807, loss : 0.5571250106025094\n",
      "epoch : 808, loss : 0.58084289791879\n",
      "epoch : 809, loss : 0.5801335724742527\n",
      "epoch : 810, loss : 0.5610967826859911\n",
      "epoch : 811, loss : 0.5288874205106555\n",
      "epoch : 812, loss : 0.5222216575241752\n",
      "epoch : 813, loss : 0.5254491683321039\n",
      "epoch : 814, loss : 0.5355556452021795\n",
      "epoch : 815, loss : 0.5449175086593192\n",
      "epoch : 816, loss : 0.5448158457444646\n",
      "epoch : 817, loss : 0.5523462930573343\n",
      "epoch : 818, loss : 0.5226496382628836\n",
      "epoch : 819, loss : 0.5694459020166368\n",
      "epoch : 820, loss : 0.5387330432682478\n",
      "epoch : 821, loss : 0.5458712430421829\n",
      "epoch : 822, loss : 0.5347849928913784\n",
      "epoch : 823, loss : 0.5109817959444543\n",
      "epoch : 824, loss : 0.5028563179879861\n",
      "epoch : 825, loss : 0.5003382048348904\n",
      "epoch : 826, loss : 0.5220670290134094\n",
      "epoch : 827, loss : 0.5124946346474438\n",
      "epoch : 828, loss : 0.5507455532813091\n",
      "epoch : 829, loss : 0.5673275879820991\n",
      "epoch : 830, loss : 0.5243329995142502\n",
      "epoch : 831, loss : 0.5669289083542358\n",
      "epoch : 832, loss : 0.5531666508365491\n",
      "epoch : 833, loss : 0.5347381417145552\n",
      "epoch : 834, loss : 0.553377394304602\n",
      "epoch : 835, loss : 0.5132329080056033\n",
      "epoch : 836, loss : 0.5324056060202451\n",
      "epoch : 837, loss : 0.5076410280133759\n",
      "epoch : 838, loss : 0.5272045209994102\n",
      "epoch : 839, loss : 0.5536055074906223\n",
      "epoch : 840, loss : 0.4944328361204955\n",
      "epoch : 841, loss : 0.5193101446074943\n",
      "epoch : 842, loss : 0.5101997759121417\n",
      "epoch : 843, loss : 0.5142765255931109\n",
      "epoch : 844, loss : 0.5342679799252147\n",
      "epoch : 845, loss : 0.5173049033184731\n",
      "epoch : 846, loss : 0.5400026641214496\n",
      "epoch : 847, loss : 0.52810344327616\n",
      "epoch : 848, loss : 0.5364940296590089\n",
      "epoch : 849, loss : 0.5433352928515129\n",
      "epoch : 850, loss : 0.5089196164609326\n",
      "epoch : 851, loss : 0.5461126490957492\n",
      "epoch : 852, loss : 0.507493381631154\n",
      "epoch : 853, loss : 0.5378679635930235\n",
      "epoch : 854, loss : 0.5646170882277005\n",
      "epoch : 855, loss : 0.5401655160846623\n",
      "epoch : 856, loss : 0.5158115470557749\n",
      "epoch : 857, loss : 0.5427937009032051\n",
      "epoch : 858, loss : 0.515173911351513\n",
      "epoch : 859, loss : 0.5733245811320657\n",
      "epoch : 860, loss : 0.5280291716016211\n",
      "epoch : 861, loss : 0.5389373788058991\n",
      "epoch : 862, loss : 0.5307444491976425\n",
      "epoch : 863, loss : 0.5198044349001008\n",
      "epoch : 864, loss : 0.5246373786105928\n",
      "epoch : 865, loss : 0.4824675013764291\n",
      "epoch : 866, loss : 0.5232673751306912\n",
      "epoch : 867, loss : 0.5010978820715749\n",
      "epoch : 868, loss : 0.5140634941336547\n",
      "epoch : 869, loss : 0.5460166103419362\n",
      "epoch : 870, loss : 0.5382168329686539\n",
      "epoch : 871, loss : 0.5479157708308189\n",
      "epoch : 872, loss : 0.530749954224758\n",
      "epoch : 873, loss : 0.5349787475446243\n",
      "epoch : 874, loss : 0.4828638268601924\n",
      "epoch : 875, loss : 0.4893155744182464\n",
      "epoch : 876, loss : 0.5513191989929714\n",
      "epoch : 877, loss : 0.517955363448972\n",
      "epoch : 878, loss : 0.523447714414631\n",
      "epoch : 879, loss : 0.5115858234749442\n",
      "epoch : 880, loss : 0.5190935200200566\n",
      "epoch : 881, loss : 0.5481312349106249\n",
      "epoch : 882, loss : 0.5139120487374896\n",
      "epoch : 883, loss : 0.5519144973575787\n",
      "epoch : 884, loss : 0.5045240360429696\n",
      "epoch : 885, loss : 0.488094790638491\n",
      "epoch : 886, loss : 0.5282442719003916\n",
      "epoch : 887, loss : 0.4996407647614465\n",
      "epoch : 888, loss : 0.516675897670575\n",
      "epoch : 889, loss : 0.5108162475132825\n",
      "epoch : 890, loss : 0.5144101300887369\n",
      "epoch : 891, loss : 0.5363898684554719\n",
      "epoch : 892, loss : 0.4935370868164259\n",
      "epoch : 893, loss : 0.4900930294724021\n",
      "epoch : 894, loss : 0.4748311620174523\n",
      "epoch : 895, loss : 0.46475937707558596\n",
      "epoch : 896, loss : 0.5414350603637563\n",
      "epoch : 897, loss : 0.5283113005660732\n",
      "epoch : 898, loss : 0.48094105355314887\n",
      "epoch : 899, loss : 0.482786953264248\n",
      "epoch : 900, loss : 0.4958969621305318\n",
      "epoch : 901, loss : 0.5247363667944103\n",
      "epoch : 902, loss : 0.47452871081559767\n",
      "epoch : 903, loss : 0.5522046406926526\n",
      "epoch : 904, loss : 0.526321496304457\n",
      "epoch : 905, loss : 0.5687868636638946\n",
      "epoch : 906, loss : 0.5038754344469537\n",
      "epoch : 907, loss : 0.5350737343911183\n",
      "epoch : 908, loss : 0.499279128374991\n",
      "epoch : 909, loss : 0.5051061911733339\n",
      "epoch : 910, loss : 0.4994097760251512\n",
      "epoch : 911, loss : 0.5135812805895231\n",
      "epoch : 912, loss : 0.5784058397584128\n",
      "epoch : 913, loss : 0.5088733363419676\n",
      "epoch : 914, loss : 0.5197549323852767\n",
      "epoch : 915, loss : 0.5165665996726846\n",
      "epoch : 916, loss : 0.49374710976095204\n",
      "epoch : 917, loss : 0.5227329273748343\n",
      "epoch : 918, loss : 0.4942155857819711\n",
      "epoch : 919, loss : 0.5558371215763254\n",
      "epoch : 920, loss : 0.4658492062256079\n",
      "epoch : 921, loss : 0.5277870710190539\n",
      "epoch : 922, loss : 0.5112467030363155\n",
      "epoch : 923, loss : 0.5186188308571869\n",
      "epoch : 924, loss : 0.5287306280071481\n",
      "epoch : 925, loss : 0.46362880012723645\n",
      "epoch : 926, loss : 0.5478642978091915\n",
      "epoch : 927, loss : 0.546485085410976\n",
      "epoch : 928, loss : 0.5023374966026813\n",
      "epoch : 929, loss : 0.5156152002449461\n",
      "epoch : 930, loss : 0.4875267952446258\n",
      "epoch : 931, loss : 0.5151290954681493\n",
      "epoch : 932, loss : 0.5080632510230323\n",
      "epoch : 933, loss : 0.5397291609135088\n",
      "epoch : 934, loss : 0.48593413510044164\n",
      "epoch : 935, loss : 0.5280066971102779\n",
      "epoch : 936, loss : 0.4870278926789444\n",
      "epoch : 937, loss : 0.5317588567049866\n",
      "epoch : 938, loss : 0.4809254793249433\n",
      "epoch : 939, loss : 0.505135584581083\n",
      "epoch : 940, loss : 0.5142929159389433\n",
      "epoch : 941, loss : 0.5164148908201291\n",
      "epoch : 942, loss : 0.5054110866655851\n",
      "epoch : 943, loss : 0.48666601764845435\n",
      "epoch : 944, loss : 0.48139793663327873\n",
      "epoch : 945, loss : 0.5089184546385571\n",
      "epoch : 946, loss : 0.521731681360728\n",
      "epoch : 947, loss : 0.44787071025612873\n",
      "epoch : 948, loss : 0.5204177861131445\n",
      "epoch : 949, loss : 0.4835111237625979\n",
      "epoch : 950, loss : 0.5412944381619031\n",
      "epoch : 951, loss : 0.48101651449340543\n",
      "epoch : 952, loss : 0.48659993805074786\n",
      "epoch : 953, loss : 0.536093839047921\n",
      "epoch : 954, loss : 0.49440865545146523\n",
      "epoch : 955, loss : 0.5014360012459679\n",
      "epoch : 956, loss : 0.5339550830115267\n",
      "epoch : 957, loss : 0.5208216831078569\n",
      "epoch : 958, loss : 0.49122572093647277\n",
      "epoch : 959, loss : 0.5055030733797222\n",
      "epoch : 960, loss : 0.5028550175631484\n",
      "epoch : 961, loss : 0.5037956300322898\n",
      "epoch : 962, loss : 0.45311973763649577\n",
      "epoch : 963, loss : 0.5408096473595075\n",
      "epoch : 964, loss : 0.5143331986980182\n",
      "epoch : 965, loss : 0.4841403544285299\n",
      "epoch : 966, loss : 0.5204993611412839\n",
      "epoch : 967, loss : 0.5223861866882378\n",
      "epoch : 968, loss : 0.5489100421975388\n",
      "epoch : 969, loss : 0.5218942708874603\n",
      "epoch : 970, loss : 0.4906705493728231\n",
      "epoch : 971, loss : 0.5118193124024907\n",
      "epoch : 972, loss : 0.5360358570152177\n",
      "epoch : 973, loss : 0.4846892039810204\n",
      "epoch : 974, loss : 0.5034583179337704\n",
      "epoch : 975, loss : 0.5022589709745412\n",
      "epoch : 976, loss : 0.5309775993543723\n",
      "epoch : 977, loss : 0.5363649358702338\n",
      "epoch : 978, loss : 0.5269409086335317\n",
      "epoch : 979, loss : 0.4776167521512581\n",
      "epoch : 980, loss : 0.5005018051019653\n",
      "epoch : 981, loss : 0.5080589466271371\n",
      "epoch : 982, loss : 0.49189235330333764\n",
      "epoch : 983, loss : 0.4817759614020515\n",
      "epoch : 984, loss : 0.5243823529061203\n",
      "epoch : 985, loss : 0.4892989690252328\n",
      "epoch : 986, loss : 0.48295624773931806\n",
      "epoch : 987, loss : 0.5098449515344473\n",
      "epoch : 988, loss : 0.478295897056563\n",
      "epoch : 989, loss : 0.5112001356295824\n",
      "epoch : 990, loss : 0.48502209185684086\n",
      "epoch : 991, loss : 0.4762686567385461\n",
      "epoch : 992, loss : 0.48571300752305246\n",
      "epoch : 993, loss : 0.4904503063423523\n",
      "epoch : 994, loss : 0.5030219879995278\n",
      "epoch : 995, loss : 0.5183297007453371\n",
      "epoch : 996, loss : 0.4783208564333753\n",
      "epoch : 997, loss : 0.5078866908298141\n",
      "epoch : 998, loss : 0.49408172820364893\n",
      "epoch : 999, loss : 0.49315945056437616\n",
      "epoch : 1000, loss : 0.5182867545067011\n"
     ]
    }
   ],
   "source": [
    "# X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "# Y = np.array([0,0,0,1])\n",
    "model = DeepNeuralNetwork(\n",
    "    input_size=4,\n",
    "    output_size=1,\n",
    "    hidden_size=16,\n",
    "    epoches=1000,\n",
    "    learning_rate=0.001,\n",
    "    dropout=0.8,\n",
    "    hidden_layers=4\n",
    ")\n",
    "\n",
    "model.fit(X_train,Y_train)\n",
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb3dda5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.86      0.83       183\n",
      "           1       0.82      0.76      0.79       160\n",
      "\n",
      "    accuracy                           0.81       343\n",
      "   macro avg       0.81      0.81      0.81       343\n",
      "weighted avg       0.81      0.81      0.81       343\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "cmf = confusion_matrix(Y_test,preds)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(cmf,annot=True,fmt='d')\n",
    "plt.xlabel(\"True\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.show()\n",
    "print(classification_report(Y_test,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87f3f44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
