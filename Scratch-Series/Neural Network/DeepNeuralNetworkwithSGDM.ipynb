{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70cd4fe3",
   "metadata": {},
   "source": [
    "# Full fledge neural net with hyperparameter and droput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6565dba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "442fe161",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNeuralNetwork:\n",
    "    def __init__(self,input_size,output_size,hidden_size,epoches=1000,learning_rate=0.001,dropout=0.5,hidden_layers=2):\n",
    "        self.input_size = input_size \n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size \n",
    "        self.learning_rate = learning_rate \n",
    "        self.dropout = dropout \n",
    "        self.hidden_layers = hidden_layers \n",
    "        self.epoches = epoches\n",
    "\n",
    "        self.hidden_weights = [np.random.randn(self.input_size,self.hidden_size)]\n",
    "        self.hidden_bias = [np.zeros((1,self.hidden_size))]\n",
    "\n",
    "        self.hidden_velocity = [np.zeros_like(self.hidden_weights[0])]\n",
    "        self.hidden_bias_velocity = [np.zeros((1,self.hidden_size))]\n",
    "        \n",
    "        for _ in range(self.hidden_layers): \n",
    "            self.hidden_weights.append(np.random.randn(self.hidden_size,self.hidden_size))\n",
    "            self.hidden_bias.append(np.zeros((1,self.hidden_size)))\n",
    "\n",
    "            self.hidden_velocity.append(np.zeros((self.hidden_size,self.hidden_size)))\n",
    "            self.hidden_bias_velocity.append(np.zeros((1,self.hidden_size)))\n",
    "        \n",
    "        self.output_weight = np.random.randn(self.hidden_size,self.output_size)\n",
    "        self.output_bias = np.zeros((1,self.output_size))\n",
    "\n",
    "\n",
    "        \n",
    "        self.output_velocity = np.zeros_like(self.output_weight)\n",
    "        self.output_bias_velocity = np.zeros_like(self.output_bias)\n",
    "\n",
    "\n",
    "        self.momentum = 0.05\n",
    "\n",
    "    def SGDM(self,weight_gradient,bias_gradient,layer):\n",
    "        if weight_gradient.shape[1] == self.hidden_size:\n",
    "            self.hidden_velocity[layer] = self.momentum * self.hidden_velocity[layer] + self.learning_rate * weight_gradient\n",
    "            self.hidden_bias_velocity[layer] = self.momentum * self.hidden_bias_velocity[layer] + self.learning_rate * bias_gradient\n",
    "            return self.hidden_velocity[layer] , self.hidden_bias_velocity[layer]\n",
    "        else : \n",
    "            self.output_velocity = self.momentum * self.output_velocity + self.learning_rate * weight_gradient \n",
    "            self.output_bias_velocity = self.momentum * self.output_bias_velocity + self.learning_rate * bias_gradient\n",
    "            return self.output_velocity, self.output_bias_velocity\n",
    "\n",
    "    def sigmoid(self,z):\n",
    "        return (1/(1+np.exp(-z)))\n",
    "    \n",
    "    def derivative_sigmoid(self,z):\n",
    "        s = self.sigmoid(z)\n",
    "        return s * (1-s)\n",
    "    \n",
    "    def Relu(self,z):\n",
    "        return np.maximum(0,z)\n",
    "\n",
    "    def derivative_relu(self,z):\n",
    "        return (z > 0 ).astype(float)\n",
    "    \n",
    "    def compute_loss(self,preds,Y):\n",
    "        preds = np.clip(preds,1e-8,1 - 1e-8) \n",
    "        return -np.mean(Y*np.log(preds)+(1-Y)*np.log(1-preds)) \n",
    "    \n",
    "    def Dropout(self, A):\n",
    "        if self.dropout < 1.0:\n",
    "            mask = (np.random.rand(*A.shape) < self.dropout).astype(float)\n",
    "            return (A * mask) / self.dropout\n",
    "        else:\n",
    "            return A  \n",
    "    \n",
    "    def ForwardPropagation(self,X):\n",
    "        self.Activations = [X]\n",
    "        self.hidden_Z = []\n",
    "\n",
    "        for l in range(self.hidden_layers):\n",
    "            Z = np.dot(self.Activations[l],self.hidden_weights[l]) + self.hidden_bias[l] \n",
    "            self.hidden_Z.append(Z)\n",
    "            A = self.Relu(Z) \n",
    "            self.Activations.append(A)\n",
    "\n",
    "        self.output_Z = np.dot(self.Activations[-1],self.output_weight) + self.output_bias \n",
    "        self.output_A = self.sigmoid(self.output_Z) \n",
    "        return self.output_A\n",
    "\n",
    "    def BackPropagation(self,X,Y):\n",
    "        m = X.shape[0]\n",
    "        error = self.output_A - Y \n",
    "        Dwo = 1/m * np.dot(self.Activations[-1].T,error)\n",
    "        Dbo = 1/m * np.sum(error,axis=0,keepdims=True)\n",
    "        self.output_velocity , self.output_bias_velocity = self.SGDM(Dwo,Dbo,0)\n",
    "        self.output_weight =  self.output_weight - self.output_velocity\n",
    "        self.output_bias = self.output_bias - self.output_bias_velocity\n",
    "\n",
    "        for l in reversed(range(self.hidden_layers)):\n",
    "            da = np.dot(error,self.output_weight.T if l == self.hidden_layers-1 else self.hidden_weights[l+1].T)\n",
    "            dz = da * self.derivative_relu(self.hidden_Z[l]) # final errors \n",
    "\n",
    "            dw = 1/m * np.dot(self.Activations[l].T,dz)\n",
    "            db = 1/m * np.sum(dz,axis=0,keepdims=True)\n",
    "            error = dz \n",
    "\n",
    "            self.hidden_velocity[l],self.hidden_bias_velocity[l] = self.SGDM(dw,db,l)\n",
    "            self.hidden_weights[l] = self.hidden_weights[l] - self.hidden_velocity[l]\n",
    "            self.hidden_bias[l] = self.hidden_bias[l] - self.hidden_bias_velocity[l]\n",
    "\n",
    "    def fit(self,X,Y):\n",
    "        Y = np.array(Y).reshape(-1,1)\n",
    "        for epoch in range(self.epoches):\n",
    "            preds_F = self.ForwardPropagation(X)\n",
    "            loss = self.compute_loss(preds_F, Y)\n",
    "            print(f\"epoch : {epoch+1}, loss : {loss}\")\n",
    "            self.BackPropagation(X,Y)\n",
    "    def predict(self,X):\n",
    "        return (self.ForwardPropagation(X) > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17900ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.62160</td>\n",
       "      <td>8.6661</td>\n",
       "      <td>-2.8073</td>\n",
       "      <td>-0.44699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.1674</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.6383</td>\n",
       "      <td>1.9242</td>\n",
       "      <td>0.10645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.5228</td>\n",
       "      <td>-4.0112</td>\n",
       "      <td>-3.59440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.4552</td>\n",
       "      <td>4.5718</td>\n",
       "      <td>-0.98880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variance  skewness  curtosis  entropy  class\n",
       "0   3.62160    8.6661   -2.8073 -0.44699      0\n",
       "1   4.54590    8.1674   -2.4586 -1.46210      0\n",
       "2   3.86600   -2.6383    1.9242  0.10645      0\n",
       "3   3.45660    9.5228   -4.0112 -3.59440      0\n",
       "4   0.32924   -4.4552    4.5718 -0.98880      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "dataset = pd.read_csv(\"../BankNote_Authentication.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05df4745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1029, 1)\n"
     ]
    }
   ],
   "source": [
    "Y = np.array(dataset['class']).reshape(-1,1)\n",
    "X = np.array(dataset.drop(columns=['class']))\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y)\n",
    "\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd1d14ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1, loss : 10.331558859662119\n",
      "epoch : 2, loss : 10.56296941594797\n",
      "epoch : 3, loss : 10.834827786849674\n",
      "epoch : 4, loss : 11.252534936216401\n",
      "epoch : 5, loss : 10.044435838331104\n",
      "epoch : 6, loss : 8.047946605953404\n",
      "epoch : 7, loss : 7.237385351329654\n",
      "epoch : 8, loss : 6.839071315781897\n",
      "epoch : 9, loss : 6.4158435332684585\n",
      "epoch : 10, loss : 6.015475284620861\n",
      "epoch : 11, loss : 5.6122861408978295\n",
      "epoch : 12, loss : 5.049000100128175\n",
      "epoch : 13, loss : 4.731689580131892\n",
      "epoch : 14, loss : 4.445126537729477\n",
      "epoch : 15, loss : 4.094184223092405\n",
      "epoch : 16, loss : 3.743103982402918\n",
      "epoch : 17, loss : 3.5263419253388713\n",
      "epoch : 18, loss : 3.4336782545536253\n",
      "epoch : 19, loss : 3.303516072462378\n",
      "epoch : 20, loss : 3.1994854572163947\n",
      "epoch : 21, loss : 3.0752370409000878\n",
      "epoch : 22, loss : 2.8581593175776265\n",
      "epoch : 23, loss : 2.608973295549468\n",
      "epoch : 24, loss : 2.4251087423500257\n",
      "epoch : 25, loss : 2.3167560729721797\n",
      "epoch : 26, loss : 2.236621698802695\n",
      "epoch : 27, loss : 2.1213328944728205\n",
      "epoch : 28, loss : 1.9535859598468743\n",
      "epoch : 29, loss : 1.79595989714645\n",
      "epoch : 30, loss : 1.6413258356665736\n",
      "epoch : 31, loss : 1.5043975579047097\n",
      "epoch : 32, loss : 1.3809806433294665\n",
      "epoch : 33, loss : 1.2652558227445134\n",
      "epoch : 34, loss : 1.162129582095828\n",
      "epoch : 35, loss : 1.0675683712441193\n",
      "epoch : 36, loss : 0.9784253594844745\n",
      "epoch : 37, loss : 0.8970455097339233\n",
      "epoch : 38, loss : 0.8389228232519667\n",
      "epoch : 39, loss : 0.7975091250929156\n",
      "epoch : 40, loss : 0.7663582756559408\n",
      "epoch : 41, loss : 0.7399168674948531\n",
      "epoch : 42, loss : 0.7202731793580565\n",
      "epoch : 43, loss : 0.7054099546709159\n",
      "epoch : 44, loss : 0.6931316940387384\n",
      "epoch : 45, loss : 0.6824737371050297\n",
      "epoch : 46, loss : 0.6720450774395245\n",
      "epoch : 47, loss : 0.6615037706271032\n",
      "epoch : 48, loss : 0.651214037659189\n",
      "epoch : 49, loss : 0.6404600530695219\n",
      "epoch : 50, loss : 0.6309804065933023\n",
      "epoch : 51, loss : 0.6220177106899677\n",
      "epoch : 52, loss : 0.6121682737219157\n",
      "epoch : 53, loss : 0.6019101596755472\n",
      "epoch : 54, loss : 0.5931561519063777\n",
      "epoch : 55, loss : 0.5853960085814292\n",
      "epoch : 56, loss : 0.57753670821194\n",
      "epoch : 57, loss : 0.5700256879668628\n",
      "epoch : 58, loss : 0.5619093744984884\n",
      "epoch : 59, loss : 0.553934751844289\n",
      "epoch : 60, loss : 0.5431889532644101\n",
      "epoch : 61, loss : 0.5315163339055858\n",
      "epoch : 62, loss : 0.519998010524135\n",
      "epoch : 63, loss : 0.5074015225475134\n",
      "epoch : 64, loss : 0.4947082739845423\n",
      "epoch : 65, loss : 0.48259613662880047\n",
      "epoch : 66, loss : 0.4704554210520331\n",
      "epoch : 67, loss : 0.4549409239004234\n",
      "epoch : 68, loss : 0.4382375037435905\n",
      "epoch : 69, loss : 0.4202293905940095\n",
      "epoch : 70, loss : 0.4006979747477145\n",
      "epoch : 71, loss : 0.3795209309939548\n",
      "epoch : 72, loss : 0.3592458114567925\n",
      "epoch : 73, loss : 0.34094451585841073\n",
      "epoch : 74, loss : 0.32417128417789653\n",
      "epoch : 75, loss : 0.3069284266928524\n",
      "epoch : 76, loss : 0.29124526950174645\n",
      "epoch : 77, loss : 0.276484728480351\n",
      "epoch : 78, loss : 0.2630296046845018\n",
      "epoch : 79, loss : 0.2509587245991092\n",
      "epoch : 80, loss : 0.24024156896267362\n",
      "epoch : 81, loss : 0.23040181175939722\n",
      "epoch : 82, loss : 0.2216563177889393\n",
      "epoch : 83, loss : 0.21412614022074603\n",
      "epoch : 84, loss : 0.20766179801761764\n",
      "epoch : 85, loss : 0.2020786194282302\n",
      "epoch : 86, loss : 0.1971317404018445\n",
      "epoch : 87, loss : 0.19255759766683644\n",
      "epoch : 88, loss : 0.1882917452964886\n",
      "epoch : 89, loss : 0.18437812908621992\n",
      "epoch : 90, loss : 0.18074360381980198\n",
      "epoch : 91, loss : 0.1773284346933665\n",
      "epoch : 92, loss : 0.1739695382842311\n",
      "epoch : 93, loss : 0.17076042973742622\n",
      "epoch : 94, loss : 0.1676709987099078\n",
      "epoch : 95, loss : 0.1646835836060872\n",
      "epoch : 96, loss : 0.1617847639595932\n",
      "epoch : 97, loss : 0.15896443207526986\n",
      "epoch : 98, loss : 0.15621452924767149\n",
      "epoch : 99, loss : 0.15353086191871385\n",
      "epoch : 100, loss : 0.15090788408153766\n",
      "epoch : 101, loss : 0.1481763415081295\n",
      "epoch : 102, loss : 0.1455039913038795\n",
      "epoch : 103, loss : 0.14291838357665376\n",
      "epoch : 104, loss : 0.14042672665746925\n",
      "epoch : 105, loss : 0.13803821803287786\n",
      "epoch : 106, loss : 0.13575776861657474\n",
      "epoch : 107, loss : 0.13358574145041077\n",
      "epoch : 108, loss : 0.13152214419374741\n",
      "epoch : 109, loss : 0.12956457667847235\n",
      "epoch : 110, loss : 0.12770878591310592\n",
      "epoch : 111, loss : 0.1259497960058906\n",
      "epoch : 112, loss : 0.12428436541357998\n",
      "epoch : 113, loss : 0.1227001331217227\n",
      "epoch : 114, loss : 0.12119011307342326\n",
      "epoch : 115, loss : 0.11974680873850364\n",
      "epoch : 116, loss : 0.11836270926167143\n",
      "epoch : 117, loss : 0.11703070728641102\n",
      "epoch : 118, loss : 0.1157442770324894\n",
      "epoch : 119, loss : 0.11449756451011789\n",
      "epoch : 120, loss : 0.11328542292323542\n",
      "epoch : 121, loss : 0.11210340988214133\n",
      "epoch : 122, loss : 0.11094775610257278\n",
      "epoch : 123, loss : 0.10981531821540205\n",
      "epoch : 124, loss : 0.10870352448947997\n",
      "epoch : 125, loss : 0.10761031921472905\n",
      "epoch : 126, loss : 0.10653410900385026\n",
      "epoch : 127, loss : 0.1054737124686051\n",
      "epoch : 128, loss : 0.1044283134210616\n",
      "epoch : 129, loss : 0.10339741691724734\n",
      "epoch : 130, loss : 0.10238080818254174\n",
      "epoch : 131, loss : 0.10137822141163473\n",
      "epoch : 132, loss : 0.10038812693521805\n",
      "epoch : 133, loss : 0.09935783920929761\n",
      "epoch : 134, loss : 0.09818579993393144\n",
      "epoch : 135, loss : 0.09703090855670651\n",
      "epoch : 136, loss : 0.09589750219274569\n",
      "epoch : 137, loss : 0.09478478311987269\n",
      "epoch : 138, loss : 0.09369250327404638\n",
      "epoch : 139, loss : 0.09262072141817367\n",
      "epoch : 140, loss : 0.09156959002691221\n",
      "epoch : 141, loss : 0.09053927772917844\n",
      "epoch : 142, loss : 0.089529946509159\n",
      "epoch : 143, loss : 0.08854175161569898\n",
      "epoch : 144, loss : 0.08757485360981508\n",
      "epoch : 145, loss : 0.0866294340493418\n",
      "epoch : 146, loss : 0.0857057097276004\n",
      "epoch : 147, loss : 0.084804157471372\n",
      "epoch : 148, loss : 0.08392258883323045\n",
      "epoch : 149, loss : 0.08306280487892295\n",
      "epoch : 150, loss : 0.08222610448181399\n",
      "epoch : 151, loss : 0.08142942057115425\n",
      "epoch : 152, loss : 0.0806566953593973\n",
      "epoch : 153, loss : 0.07990734431470937\n",
      "epoch : 154, loss : 0.07918162829406017\n",
      "epoch : 155, loss : 0.07847964472904617\n",
      "epoch : 156, loss : 0.07777433631661776\n",
      "epoch : 157, loss : 0.07698929894571196\n",
      "epoch : 158, loss : 0.07623915776876306\n",
      "epoch : 159, loss : 0.07552635280371452\n",
      "epoch : 160, loss : 0.07484921845469072\n",
      "epoch : 161, loss : 0.07420615841437468\n",
      "epoch : 162, loss : 0.07359565537738934\n",
      "epoch : 163, loss : 0.07301618246336596\n",
      "epoch : 164, loss : 0.07246617794314586\n",
      "epoch : 165, loss : 0.07194405302861868\n",
      "epoch : 166, loss : 0.07144821286591634\n",
      "epoch : 167, loss : 0.07097708087102844\n",
      "epoch : 168, loss : 0.07052912130883444\n",
      "epoch : 169, loss : 0.0701028579016095\n",
      "epoch : 170, loss : 0.06969688819069583\n",
      "epoch : 171, loss : 0.06930989278117096\n",
      "epoch : 172, loss : 0.06892943638096889\n",
      "epoch : 173, loss : 0.06853095999941042\n",
      "epoch : 174, loss : 0.06815079696573811\n",
      "epoch : 175, loss : 0.06778880802869613\n",
      "epoch : 176, loss : 0.06744285467108915\n",
      "epoch : 177, loss : 0.06710736217244201\n",
      "epoch : 178, loss : 0.0667867848808392\n",
      "epoch : 179, loss : 0.06648028922892218\n",
      "epoch : 180, loss : 0.06618694789257758\n",
      "epoch : 181, loss : 0.06590595117076069\n",
      "epoch : 182, loss : 0.06563658802802932\n",
      "epoch : 183, loss : 0.06537822472608901\n",
      "epoch : 184, loss : 0.0651302890729714\n",
      "epoch : 185, loss : 0.06489225857215829\n",
      "epoch : 186, loss : 0.06466365124572117\n",
      "epoch : 187, loss : 0.06444401842368612\n",
      "epoch : 188, loss : 0.06423215503470439\n",
      "epoch : 189, loss : 0.0640268702411816\n",
      "epoch : 190, loss : 0.06382991199719026\n",
      "epoch : 191, loss : 0.06364084087724547\n",
      "epoch : 192, loss : 0.06345919269948469\n",
      "epoch : 193, loss : 0.06328455327287906\n",
      "epoch : 194, loss : 0.06310556477602529\n",
      "epoch : 195, loss : 0.06294401302809188\n",
      "epoch : 196, loss : 0.06277846177910543\n",
      "epoch : 197, loss : 0.06257007275385706\n",
      "epoch : 198, loss : 0.06233822102906748\n",
      "epoch : 199, loss : 0.06211266560424146\n",
      "epoch : 200, loss : 0.0619017442801747\n",
      "epoch : 201, loss : 0.061687400701410565\n",
      "epoch : 202, loss : 0.061477942366964355\n",
      "epoch : 203, loss : 0.06127352386762571\n",
      "epoch : 204, loss : 0.06107388821829582\n",
      "epoch : 205, loss : 0.06087877684931313\n",
      "epoch : 206, loss : 0.06068794946272413\n",
      "epoch : 207, loss : 0.06050118367486326\n",
      "epoch : 208, loss : 0.06031827361031189\n",
      "epoch : 209, loss : 0.06013902850101441\n",
      "epoch : 210, loss : 0.05996327137744587\n",
      "epoch : 211, loss : 0.05979083761955429\n",
      "epoch : 212, loss : 0.0596224731065026\n",
      "epoch : 213, loss : 0.05945584716237256\n",
      "epoch : 214, loss : 0.05929224553386691\n",
      "epoch : 215, loss : 0.059132547817403944\n",
      "epoch : 216, loss : 0.05897426049570549\n",
      "epoch : 217, loss : 0.058818698070849856\n",
      "epoch : 218, loss : 0.058666639595990284\n",
      "epoch : 219, loss : 0.058515732295454634\n",
      "epoch : 220, loss : 0.058367294313373064\n",
      "epoch : 221, loss : 0.058222045155326606\n",
      "epoch : 222, loss : 0.058077729811743514\n",
      "epoch : 223, loss : 0.05793567025425402\n",
      "epoch : 224, loss : 0.057796555264360616\n",
      "epoch : 225, loss : 0.05761887629582065\n",
      "epoch : 226, loss : 0.057322989930800775\n",
      "epoch : 227, loss : 0.057028857823675186\n",
      "epoch : 228, loss : 0.05673620021606227\n",
      "epoch : 229, loss : 0.05644545382774455\n",
      "epoch : 230, loss : 0.05615632964231928\n",
      "epoch : 231, loss : 0.05586853055421747\n",
      "epoch : 232, loss : 0.05558248604470594\n",
      "epoch : 233, loss : 0.055297976540577345\n",
      "epoch : 234, loss : 0.055014651398613446\n",
      "epoch : 235, loss : 0.05473293193252439\n",
      "epoch : 236, loss : 0.0544526998217968\n",
      "epoch : 237, loss : 0.05417351631658517\n",
      "epoch : 238, loss : 0.053895793882569314\n",
      "epoch : 239, loss : 0.05361954647247532\n",
      "epoch : 240, loss : 0.053344214028871365\n",
      "epoch : 241, loss : 0.053070199676591864\n",
      "epoch : 242, loss : 0.05279767989458899\n",
      "epoch : 243, loss : 0.0525259413449703\n",
      "epoch : 244, loss : 0.052255377464044794\n",
      "epoch : 245, loss : 0.051986356852905105\n",
      "epoch : 246, loss : 0.05171798197097564\n",
      "epoch : 247, loss : 0.0514506566458383\n",
      "epoch : 248, loss : 0.051184917547196826\n",
      "epoch : 249, loss : 0.05091966512180999\n",
      "epoch : 250, loss : 0.05065545521599542\n",
      "epoch : 251, loss : 0.05039265746297923\n",
      "epoch : 252, loss : 0.05013045490377682\n",
      "epoch : 253, loss : 0.049869150577005174\n",
      "epoch : 254, loss : 0.0496091039406294\n",
      "epoch : 255, loss : 0.04934978610056837\n",
      "epoch : 256, loss : 0.04909121737208192\n",
      "epoch : 257, loss : 0.04883374709724754\n",
      "epoch : 258, loss : 0.04857716187293714\n",
      "epoch : 259, loss : 0.048321171474171545\n",
      "epoch : 260, loss : 0.048066115074225556\n",
      "epoch : 261, loss : 0.047812122005894644\n",
      "epoch : 262, loss : 0.04755856397075984\n",
      "epoch : 263, loss : 0.04730576993446082\n",
      "epoch : 264, loss : 0.047054239076938904\n",
      "epoch : 265, loss : 0.04680297770406694\n",
      "epoch : 266, loss : 0.0465524296682307\n",
      "epoch : 267, loss : 0.04630301780253019\n",
      "epoch : 268, loss : 0.046054000327674656\n",
      "epoch : 269, loss : 0.04580560714280259\n",
      "epoch : 270, loss : 0.045557930040138764\n",
      "epoch : 271, loss : 0.045306308961926466\n",
      "epoch : 272, loss : 0.04505571796577373\n",
      "epoch : 273, loss : 0.04480632959094383\n",
      "epoch : 274, loss : 0.04455848156032667\n",
      "epoch : 275, loss : 0.04431134238112543\n",
      "epoch : 276, loss : 0.04406517341045352\n",
      "epoch : 277, loss : 0.04381997385580346\n",
      "epoch : 278, loss : 0.043576007275346516\n",
      "epoch : 279, loss : 0.043332555767616715\n",
      "epoch : 280, loss : 0.04308987687300192\n",
      "epoch : 281, loss : 0.04284811831178497\n",
      "epoch : 282, loss : 0.042607195977192244\n",
      "epoch : 283, loss : 0.04236671080200952\n",
      "epoch : 284, loss : 0.04212687301754499\n",
      "epoch : 285, loss : 0.041887952418725526\n",
      "epoch : 286, loss : 0.04164955326872404\n",
      "epoch : 287, loss : 0.04141161228738063\n",
      "epoch : 288, loss : 0.04117422459410575\n",
      "epoch : 289, loss : 0.04093767514681953\n",
      "epoch : 290, loss : 0.040701421170737516\n",
      "epoch : 291, loss : 0.04046570634410436\n",
      "epoch : 292, loss : 0.040230541244341056\n",
      "epoch : 293, loss : 0.03999637131451723\n",
      "epoch : 294, loss : 0.03976233104874529\n",
      "epoch : 295, loss : 0.0395288116492467\n",
      "epoch : 296, loss : 0.03929592191039418\n",
      "epoch : 297, loss : 0.03906382819798897\n",
      "epoch : 298, loss : 0.038831949196131484\n",
      "epoch : 299, loss : 0.03860059808856537\n",
      "epoch : 300, loss : 0.0383699875479155\n",
      "epoch : 301, loss : 0.03814002895590402\n",
      "epoch : 302, loss : 0.03791042718056248\n",
      "epoch : 303, loss : 0.037681423963261546\n",
      "epoch : 304, loss : 0.03745334496364014\n",
      "epoch : 305, loss : 0.03722585168127267\n",
      "epoch : 306, loss : 0.03699894950888458\n",
      "epoch : 307, loss : 0.03677282100332531\n",
      "epoch : 308, loss : 0.03654792883317173\n",
      "epoch : 309, loss : 0.036323680404034546\n",
      "epoch : 310, loss : 0.03610042286037961\n",
      "epoch : 311, loss : 0.03587831645269694\n",
      "epoch : 312, loss : 0.035657907843798554\n",
      "epoch : 313, loss : 0.03543852640447634\n",
      "epoch : 314, loss : 0.035220733004798534\n",
      "epoch : 315, loss : 0.03500492108224036\n",
      "epoch : 316, loss : 0.03479126630257045\n",
      "epoch : 317, loss : 0.03457963101315373\n",
      "epoch : 318, loss : 0.03437053564255854\n",
      "epoch : 319, loss : 0.03416471100546885\n",
      "epoch : 320, loss : 0.03396175998069799\n",
      "epoch : 321, loss : 0.03376227304922588\n",
      "epoch : 322, loss : 0.03356687119504319\n",
      "epoch : 323, loss : 0.03337577793176871\n",
      "epoch : 324, loss : 0.03318899058264269\n",
      "epoch : 325, loss : 0.033007225477359174\n",
      "epoch : 326, loss : 0.03283087017474847\n",
      "epoch : 327, loss : 0.03265953755113677\n",
      "epoch : 328, loss : 0.032493989672811155\n",
      "epoch : 329, loss : 0.0323342222612109\n",
      "epoch : 330, loss : 0.0321798170431283\n",
      "epoch : 331, loss : 0.03203156331351143\n",
      "epoch : 332, loss : 0.03188847834298833\n",
      "epoch : 333, loss : 0.03175091526788513\n",
      "epoch : 334, loss : 0.0316187351319841\n",
      "epoch : 335, loss : 0.03149108121405594\n",
      "epoch : 336, loss : 0.03136873136219094\n",
      "epoch : 337, loss : 0.03125005509872504\n",
      "epoch : 338, loss : 0.031136201333068974\n",
      "epoch : 339, loss : 0.03102538417565747\n",
      "epoch : 340, loss : 0.030918524059992535\n",
      "epoch : 341, loss : 0.030813208993672816\n",
      "epoch : 342, loss : 0.030711794253842892\n",
      "epoch : 343, loss : 0.030612365860301447\n",
      "epoch : 344, loss : 0.030516422068278072\n",
      "epoch : 345, loss : 0.030422170154786307\n",
      "epoch : 346, loss : 0.030330815756346524\n",
      "epoch : 347, loss : 0.03024108652077678\n",
      "epoch : 348, loss : 0.030153703742063727\n",
      "epoch : 349, loss : 0.0300680033581707\n",
      "epoch : 350, loss : 0.02998399907142103\n",
      "epoch : 351, loss : 0.02990192486859008\n",
      "epoch : 352, loss : 0.029820930851180923\n",
      "epoch : 353, loss : 0.02974204764921722\n",
      "epoch : 354, loss : 0.02966394886292513\n",
      "epoch : 355, loss : 0.029587518955112493\n",
      "epoch : 356, loss : 0.029512355034833227\n",
      "epoch : 357, loss : 0.029438100871527203\n",
      "epoch : 358, loss : 0.02936566694398276\n",
      "epoch : 359, loss : 0.02929368490623964\n",
      "epoch : 360, loss : 0.02922311204666836\n",
      "epoch : 361, loss : 0.02915365447360842\n",
      "epoch : 362, loss : 0.02908487067929656\n",
      "epoch : 363, loss : 0.029017555822675536\n",
      "epoch : 364, loss : 0.028950742863657175\n",
      "epoch : 365, loss : 0.028884875919327885\n",
      "epoch : 366, loss : 0.028820320246733214\n",
      "epoch : 367, loss : 0.02875613057189146\n",
      "epoch : 368, loss : 0.028693049274549895\n",
      "epoch : 369, loss : 0.02863083835552991\n",
      "epoch : 370, loss : 0.028569126113971666\n",
      "epoch : 371, loss : 0.02850812301089681\n",
      "epoch : 372, loss : 0.028447601620625734\n",
      "epoch : 373, loss : 0.02838771075569594\n",
      "epoch : 374, loss : 0.02832907659730448\n",
      "epoch : 375, loss : 0.028270667996475696\n",
      "epoch : 376, loss : 0.02821295097733652\n",
      "epoch : 377, loss : 0.028156459354377564\n",
      "epoch : 378, loss : 0.02810008317106862\n",
      "epoch : 379, loss : 0.028044408378015293\n",
      "epoch : 380, loss : 0.027989795597215705\n",
      "epoch : 381, loss : 0.027935323426482347\n",
      "epoch : 382, loss : 0.02788156238004566\n",
      "epoch : 383, loss : 0.027828700510638765\n",
      "epoch : 384, loss : 0.027776005703437266\n",
      "epoch : 385, loss : 0.027724021786474838\n",
      "epoch : 386, loss : 0.02767280528857021\n",
      "epoch : 387, loss : 0.027621751821881715\n",
      "epoch : 388, loss : 0.027571399878354372\n",
      "epoch : 389, loss : 0.027521712973698334\n",
      "epoch : 390, loss : 0.027472185446481293\n",
      "epoch : 391, loss : 0.027423342614237088\n",
      "epoch : 392, loss : 0.0273750876457413\n",
      "epoch : 393, loss : 0.027326980192067512\n",
      "epoch : 394, loss : 0.027279533515584653\n",
      "epoch : 395, loss : 0.027232620561200488\n",
      "epoch : 396, loss : 0.027185835754825357\n",
      "epoch : 397, loss : 0.02713968125155206\n",
      "epoch : 398, loss : 0.027094027528663515\n",
      "epoch : 399, loss : 0.027048475672208264\n",
      "epoch : 400, loss : 0.027003517545496458\n",
      "epoch : 401, loss : 0.026959046868273998\n",
      "epoch : 402, loss : 0.026914645359670364\n",
      "epoch : 403, loss : 0.026870795304147096\n",
      "epoch : 404, loss : 0.026827437577756054\n",
      "epoch : 405, loss : 0.02678411033884737\n",
      "epoch : 406, loss : 0.026741286997694638\n",
      "epoch : 407, loss : 0.026698977819950745\n",
      "epoch : 408, loss : 0.026656654928271836\n",
      "epoch : 409, loss : 0.026614783763278915\n",
      "epoch : 410, loss : 0.02657346439044071\n",
      "epoch : 411, loss : 0.026532081573198887\n",
      "epoch : 412, loss : 0.02649109545653111\n",
      "epoch : 413, loss : 0.02645072767805304\n",
      "epoch : 414, loss : 0.026410242462743334\n",
      "epoch : 415, loss : 0.026370087028052217\n",
      "epoch : 416, loss : 0.026330604469099978\n",
      "epoch : 417, loss : 0.02629094317714375\n",
      "epoch : 418, loss : 0.026251543854690338\n",
      "epoch : 419, loss : 0.026212899925966805\n",
      "epoch : 420, loss : 0.02617401195435515\n",
      "epoch : 421, loss : 0.026135345602130402\n",
      "epoch : 422, loss : 0.026097463886857575\n",
      "epoch : 423, loss : 0.026059278289812162\n",
      "epoch : 424, loss : 0.02602134977209143\n",
      "epoch : 425, loss : 0.025984104257071792\n",
      "epoch : 426, loss : 0.025946663125784444\n",
      "epoch : 427, loss : 0.025909403994227132\n",
      "epoch : 428, loss : 0.025872747262685052\n",
      "epoch : 429, loss : 0.025836015440513876\n",
      "epoch : 430, loss : 0.02579938715001694\n",
      "epoch : 431, loss : 0.025763277289021766\n",
      "epoch : 432, loss : 0.025727222351441703\n",
      "epoch : 433, loss : 0.025691188842282543\n",
      "epoch : 434, loss : 0.025655586748990853\n",
      "epoch : 435, loss : 0.02562017861775848\n",
      "epoch : 436, loss : 0.025584706531367993\n",
      "epoch : 437, loss : 0.025549575860662952\n",
      "epoch : 438, loss : 0.025514786601780935\n",
      "epoch : 439, loss : 0.02547984494637713\n",
      "epoch : 440, loss : 0.02544515195444299\n",
      "epoch : 441, loss : 0.025410955730667643\n",
      "epoch : 442, loss : 0.025376515789566758\n",
      "epoch : 443, loss : 0.0253422291218156\n",
      "epoch : 444, loss : 0.025308601822526523\n",
      "epoch : 445, loss : 0.025274636779400124\n",
      "epoch : 446, loss : 0.025240799171634232\n",
      "epoch : 447, loss : 0.02520760276273266\n",
      "epoch : 448, loss : 0.025174105046784367\n",
      "epoch : 449, loss : 0.025140735049924322\n",
      "epoch : 450, loss : 0.02510788112513477\n",
      "epoch : 451, loss : 0.02507490268975038\n",
      "epoch : 452, loss : 0.025041941465551613\n",
      "epoch : 453, loss : 0.025009335596631127\n",
      "epoch : 454, loss : 0.024976786295952107\n",
      "epoch : 455, loss : 0.024944165681747416\n",
      "epoch : 456, loss : 0.02491187036310182\n",
      "epoch : 457, loss : 0.024879946727860713\n",
      "epoch : 458, loss : 0.02484797565430367\n",
      "epoch : 459, loss : 0.024816215638258726\n",
      "epoch : 460, loss : 0.024784878326283837\n",
      "epoch : 461, loss : 0.024753267939488687\n",
      "epoch : 462, loss : 0.02472174771903513\n",
      "epoch : 463, loss : 0.02469088181932853\n",
      "epoch : 464, loss : 0.02465956804289174\n",
      "epoch : 465, loss : 0.02462840654716112\n",
      "epoch : 466, loss : 0.024597744153200853\n",
      "epoch : 467, loss : 0.024566878904703205\n",
      "epoch : 468, loss : 0.024536035483484042\n",
      "epoch : 469, loss : 0.02450555724076789\n",
      "epoch : 470, loss : 0.024475128458086946\n",
      "epoch : 471, loss : 0.02444458850537932\n",
      "epoch : 472, loss : 0.024414277808134134\n",
      "epoch : 473, loss : 0.02438427391840762\n",
      "epoch : 474, loss : 0.024354023758964543\n",
      "epoch : 475, loss : 0.024323865173577045\n",
      "epoch : 476, loss : 0.024294275204124874\n",
      "epoch : 477, loss : 0.024264302017717395\n",
      "epoch : 478, loss : 0.024234393782474103\n",
      "epoch : 479, loss : 0.024205009504954095\n",
      "epoch : 480, loss : 0.02417536061856893\n",
      "epoch : 481, loss : 0.02414573828746539\n",
      "epoch : 482, loss : 0.02411647535373298\n",
      "epoch : 483, loss : 0.0240872169540723\n",
      "epoch : 484, loss : 0.024057844631463685\n",
      "epoch : 485, loss : 0.024028689450031383\n",
      "epoch : 486, loss : 0.02399981241905712\n",
      "epoch : 487, loss : 0.02397067969861151\n",
      "epoch : 488, loss : 0.02394162094522073\n",
      "epoch : 489, loss : 0.023913116471251515\n",
      "epoch : 490, loss : 0.023884213563244622\n",
      "epoch : 491, loss : 0.023855360548994058\n",
      "epoch : 492, loss : 0.02382700770123861\n",
      "epoch : 493, loss : 0.023798392185494267\n",
      "epoch : 494, loss : 0.02376978153867093\n",
      "epoch : 495, loss : 0.02374150277747381\n",
      "epoch : 496, loss : 0.02371324089812619\n",
      "epoch : 497, loss : 0.02368483935951072\n",
      "epoch : 498, loss : 0.023656624268920366\n",
      "epoch : 499, loss : 0.023628708638451526\n",
      "epoch : 500, loss : 0.023600508173772884\n",
      "epoch : 501, loss : 0.023572348395512067\n",
      "epoch : 502, loss : 0.023544798918752597\n",
      "epoch : 503, loss : 0.02351673894900955\n",
      "epoch : 504, loss : 0.023488794928147674\n",
      "epoch : 505, loss : 0.02346129167142115\n",
      "epoch : 506, loss : 0.023433561692861777\n",
      "epoch : 507, loss : 0.023405801702029803\n",
      "epoch : 508, loss : 0.023378336111523224\n",
      "epoch : 509, loss : 0.023350929758833582\n",
      "epoch : 510, loss : 0.023323347160522732\n",
      "epoch : 511, loss : 0.023295912601301785\n",
      "epoch : 512, loss : 0.023268823633516077\n",
      "epoch : 513, loss : 0.023241412152767198\n",
      "epoch : 514, loss : 0.023214031717245287\n",
      "epoch : 515, loss : 0.023187222632105344\n",
      "epoch : 516, loss : 0.0231599526187683\n",
      "epoch : 517, loss : 0.02313276014239598\n",
      "epoch : 518, loss : 0.023105969543830925\n",
      "epoch : 519, loss : 0.023079003339016135\n",
      "epoch : 520, loss : 0.02305196861666986\n",
      "epoch : 521, loss : 0.023025189832680423\n",
      "epoch : 522, loss : 0.02299852209837195\n",
      "epoch : 523, loss : 0.022971639902752364\n",
      "epoch : 524, loss : 0.02294486806726189\n",
      "epoch : 525, loss : 0.02291849351985776\n",
      "epoch : 526, loss : 0.022891758900288006\n",
      "epoch : 527, loss : 0.02286504772735002\n",
      "epoch : 528, loss : 0.022838871847095155\n",
      "epoch : 529, loss : 0.02281228530987055\n",
      "epoch : 530, loss : 0.022785740198955195\n",
      "epoch : 531, loss : 0.022759562457488402\n",
      "epoch : 532, loss : 0.022733257513281167\n",
      "epoch : 533, loss : 0.022706849386415488\n",
      "epoch : 534, loss : 0.022680664558819032\n",
      "epoch : 535, loss : 0.022654636622336237\n",
      "epoch : 536, loss : 0.02262836131961201\n",
      "epoch : 537, loss : 0.022602165906275975\n",
      "epoch : 538, loss : 0.02257641039101997\n",
      "epoch : 539, loss : 0.02255026396254918\n",
      "epoch : 540, loss : 0.02252413490287055\n",
      "epoch : 541, loss : 0.022498514076689957\n",
      "epoch : 542, loss : 0.02247251982161058\n",
      "epoch : 543, loss : 0.022446539397341647\n",
      "epoch : 544, loss : 0.022420902087450695\n",
      "epoch : 545, loss : 0.02239517009067538\n",
      "epoch : 546, loss : 0.022369310055083674\n",
      "epoch : 547, loss : 0.02234365216616898\n",
      "epoch : 548, loss : 0.022318178381504586\n",
      "epoch : 549, loss : 0.022292435384758592\n",
      "epoch : 550, loss : 0.022266754488085325\n",
      "epoch : 551, loss : 0.02224153482515854\n",
      "epoch : 552, loss : 0.022215905614535465\n",
      "epoch : 553, loss : 0.022190288723213044\n",
      "epoch : 554, loss : 0.02216516725070764\n",
      "epoch : 555, loss : 0.02213968488116741\n",
      "epoch : 556, loss : 0.022114202577900696\n",
      "epoch : 557, loss : 0.022089054511045557\n",
      "epoch : 558, loss : 0.022063817063782905\n",
      "epoch : 559, loss : 0.022038441710080493\n",
      "epoch : 560, loss : 0.022013263853535917\n",
      "epoch : 561, loss : 0.021988267672175023\n",
      "epoch : 562, loss : 0.021962996551541082\n",
      "epoch : 563, loss : 0.021937787326115496\n",
      "epoch : 564, loss : 0.021913028638373355\n",
      "epoch : 565, loss : 0.021887859112689166\n",
      "epoch : 566, loss : 0.021862697986099928\n",
      "epoch : 567, loss : 0.02183803845582847\n",
      "epoch : 568, loss : 0.021812995521126522\n",
      "epoch : 569, loss : 0.02178795751930118\n",
      "epoch : 570, loss : 0.021763264809564035\n",
      "epoch : 571, loss : 0.0217384508737354\n",
      "epoch : 572, loss : 0.021713508774145785\n",
      "epoch : 573, loss : 0.021688780377457754\n",
      "epoch : 574, loss : 0.021664192007799444\n",
      "epoch : 575, loss : 0.02163934352958223\n",
      "epoch : 576, loss : 0.021614578525795607\n",
      "epoch : 577, loss : 0.021590212186763293\n",
      "epoch : 578, loss : 0.021565455138879835\n",
      "epoch : 579, loss : 0.02154070309345526\n",
      "epoch : 580, loss : 0.02151648179924554\n",
      "epoch : 581, loss : 0.02149181084737867\n",
      "epoch : 582, loss : 0.02146717236810756\n",
      "epoch : 583, loss : 0.02144291408248485\n",
      "epoch : 584, loss : 0.02141845708647935\n",
      "epoch : 585, loss : 0.021393905229077276\n",
      "epoch : 586, loss : 0.021369607985349696\n",
      "epoch : 587, loss : 0.021345361660321215\n",
      "epoch : 588, loss : 0.021320894478823155\n",
      "epoch : 589, loss : 0.021296557870054827\n",
      "epoch : 590, loss : 0.021272518810878566\n",
      "epoch : 591, loss : 0.021248134425358518\n",
      "epoch : 592, loss : 0.021223758140598577\n",
      "epoch : 593, loss : 0.021199922967345533\n",
      "epoch : 594, loss : 0.02117561955628463\n",
      "epoch : 595, loss : 0.02115131760552692\n",
      "epoch : 596, loss : 0.02112748183940181\n",
      "epoch : 597, loss : 0.021103318018693033\n",
      "epoch : 598, loss : 0.02107911975017575\n",
      "epoch : 599, loss : 0.02105524521449109\n",
      "epoch : 600, loss : 0.021031277499613425\n",
      "epoch : 601, loss : 0.02100715616096283\n",
      "epoch : 602, loss : 0.020983241374288843\n",
      "epoch : 603, loss : 0.020959466584031947\n",
      "epoch : 604, loss : 0.02093542051727872\n",
      "epoch : 605, loss : 0.02091146553173242\n",
      "epoch : 606, loss : 0.020887880353846335\n",
      "epoch : 607, loss : 0.020863907946451234\n",
      "epoch : 608, loss : 0.020839672607185968\n",
      "epoch : 609, loss : 0.020814286684019925\n",
      "epoch : 610, loss : 0.02078830132093295\n",
      "epoch : 611, loss : 0.020762454995317387\n",
      "epoch : 612, loss : 0.0207371640986548\n",
      "epoch : 613, loss : 0.020711417945672158\n",
      "epoch : 614, loss : 0.02068579727298637\n",
      "epoch : 615, loss : 0.020660641252102045\n",
      "epoch : 616, loss : 0.020635078897969495\n",
      "epoch : 617, loss : 0.02060969807883228\n",
      "epoch : 618, loss : 0.02058465441146648\n",
      "epoch : 619, loss : 0.02055926113554195\n",
      "epoch : 620, loss : 0.02053410845620922\n",
      "epoch : 621, loss : 0.02050916372888478\n",
      "epoch : 622, loss : 0.02048392869099198\n",
      "epoch : 623, loss : 0.02045899496541931\n",
      "epoch : 624, loss : 0.020434138749599286\n",
      "epoch : 625, loss : 0.020409053037439687\n",
      "epoch : 626, loss : 0.02038433037349501\n",
      "epoch : 627, loss : 0.020359553928611152\n",
      "epoch : 628, loss : 0.020334609785227352\n",
      "epoch : 629, loss : 0.020310091131669223\n",
      "epoch : 630, loss : 0.02028538682073217\n",
      "epoch : 631, loss : 0.020260577303005203\n",
      "epoch : 632, loss : 0.020236256253555723\n",
      "epoch : 633, loss : 0.020211617237499178\n",
      "epoch : 634, loss : 0.020186898054946216\n",
      "epoch : 635, loss : 0.020162551273821487\n",
      "epoch : 636, loss : 0.020137746658908617\n",
      "epoch : 637, loss : 0.020112972324938173\n",
      "epoch : 638, loss : 0.02008879259903231\n",
      "epoch : 639, loss : 0.02006409762019754\n",
      "epoch : 640, loss : 0.020039498544384627\n",
      "epoch : 641, loss : 0.020015365452261718\n",
      "epoch : 642, loss : 0.019990793301007354\n",
      "epoch : 643, loss : 0.019966396236621775\n",
      "epoch : 644, loss : 0.019942301534552807\n",
      "epoch : 645, loss : 0.019917846983893747\n",
      "epoch : 646, loss : 0.01989364673888287\n",
      "epoch : 647, loss : 0.019869585404195617\n",
      "epoch : 648, loss : 0.0198452435206713\n",
      "epoch : 649, loss : 0.019821235261294035\n",
      "epoch : 650, loss : 0.019797202594130436\n",
      "epoch : 651, loss : 0.01977296876479088\n",
      "epoch : 652, loss : 0.01974914798619832\n",
      "epoch : 653, loss : 0.019725139564497587\n",
      "epoch : 654, loss : 0.019701009462375952\n",
      "epoch : 655, loss : 0.019677371959634482\n",
      "epoch : 656, loss : 0.01965338360462333\n",
      "epoch : 657, loss : 0.019629353160727298\n",
      "epoch : 658, loss : 0.01960589246739479\n",
      "epoch : 659, loss : 0.01958191299541869\n",
      "epoch : 660, loss : 0.019558002337934168\n",
      "epoch : 661, loss : 0.019534623625149735\n",
      "epoch : 662, loss : 0.019510743723957155\n",
      "epoch : 663, loss : 0.019486975707357124\n",
      "epoch : 664, loss : 0.019463609153664962\n",
      "epoch : 665, loss : 0.019439819604666043\n",
      "epoch : 666, loss : 0.019416223169044955\n",
      "epoch : 667, loss : 0.01939286251197734\n",
      "epoch : 668, loss : 0.019369160050014184\n",
      "epoch : 669, loss : 0.019345731994838907\n",
      "epoch : 670, loss : 0.019322373955447638\n",
      "epoch : 671, loss : 0.019298755421633958\n",
      "epoch : 672, loss : 0.01927549274194645\n",
      "epoch : 673, loss : 0.019252134181145086\n",
      "epoch : 674, loss : 0.019228596580995485\n",
      "epoch : 675, loss : 0.01920549645515581\n",
      "epoch : 676, loss : 0.019182134367088596\n",
      "epoch : 677, loss : 0.019158674859831654\n",
      "epoch : 678, loss : 0.019135734635154823\n",
      "epoch : 679, loss : 0.01911236614154679\n",
      "epoch : 680, loss : 0.019088982030611455\n",
      "epoch : 681, loss : 0.019066199210479452\n",
      "epoch : 682, loss : 0.019042821555584215\n",
      "epoch : 683, loss : 0.019019579290973303\n",
      "epoch : 684, loss : 0.01899678665209186\n",
      "epoch : 685, loss : 0.018973522613263568\n",
      "epoch : 686, loss : 0.018950401725570876\n",
      "epoch : 687, loss : 0.018927598238084074\n",
      "epoch : 688, loss : 0.018904402976493147\n",
      "epoch : 689, loss : 0.018881433679902762\n",
      "epoch : 690, loss : 0.01885861407331702\n",
      "epoch : 691, loss : 0.01883548542387314\n",
      "epoch : 692, loss : 0.018812665691765783\n",
      "epoch : 693, loss : 0.018789827637156545\n",
      "epoch : 694, loss : 0.018766763474306544\n",
      "epoch : 695, loss : 0.01874409138722232\n",
      "epoch : 696, loss : 0.01872123265896227\n",
      "epoch : 697, loss : 0.018698230961971325\n",
      "epoch : 698, loss : 0.018675704703394937\n",
      "epoch : 699, loss : 0.018652823172234934\n",
      "epoch : 700, loss : 0.018629882011441088\n",
      "epoch : 701, loss : 0.018607499851947696\n",
      "epoch : 702, loss : 0.018584593489208687\n",
      "epoch : 703, loss : 0.018561743492236785\n",
      "epoch : 704, loss : 0.01853941204285553\n",
      "epoch : 705, loss : 0.01851656781179738\n",
      "epoch : 706, loss : 0.018493827047506135\n",
      "epoch : 707, loss : 0.018471470244690025\n",
      "epoch : 708, loss : 0.018448681812010266\n",
      "epoch : 709, loss : 0.01842608114784312\n",
      "epoch : 710, loss : 0.01840369439169913\n",
      "epoch : 711, loss : 0.018380960230803742\n",
      "epoch : 712, loss : 0.018358498230938002\n",
      "epoch : 713, loss : 0.018336079843993457\n",
      "epoch : 714, loss : 0.01831339843698507\n",
      "epoch : 715, loss : 0.018291073730486027\n",
      "epoch : 716, loss : 0.018268622120579506\n",
      "epoch : 717, loss : 0.018245992025129356\n",
      "epoch : 718, loss : 0.018223803305316658\n",
      "epoch : 719, loss : 0.018201316963566163\n",
      "epoch : 720, loss : 0.018178736809678225\n",
      "epoch : 721, loss : 0.01815668282972344\n",
      "epoch : 722, loss : 0.018134160328613865\n",
      "epoch : 723, loss : 0.018111632609971826\n",
      "epoch : 724, loss : 0.01808967769706574\n",
      "epoch : 725, loss : 0.01806717795378671\n",
      "epoch : 726, loss : 0.018044750811338793\n",
      "epoch : 727, loss : 0.018022760176004817\n",
      "epoch : 728, loss : 0.018000306981031423\n",
      "epoch : 729, loss : 0.017978011689056717\n",
      "epoch : 730, loss : 0.01795598129158004\n",
      "epoch : 731, loss : 0.01793357358695241\n",
      "epoch : 732, loss : 0.01791140908110262\n",
      "epoch : 733, loss : 0.01788933778950233\n",
      "epoch : 734, loss : 0.017866974507773196\n",
      "epoch : 735, loss : 0.017844939767254684\n",
      "epoch : 736, loss : 0.017822826522200595\n",
      "epoch : 737, loss : 0.017800506655209006\n",
      "epoch : 738, loss : 0.017778600705111978\n",
      "epoch : 739, loss : 0.017756444518615754\n",
      "epoch : 740, loss : 0.017734167116332345\n",
      "epoch : 741, loss : 0.01771238902654909\n",
      "epoch : 742, loss : 0.017690188981200113\n",
      "epoch : 743, loss : 0.017667953150997638\n",
      "epoch : 744, loss : 0.017646302035809552\n",
      "epoch : 745, loss : 0.017624057284282953\n",
      "epoch : 746, loss : 0.01760192760911339\n",
      "epoch : 747, loss : 0.017580244887114873\n",
      "epoch : 748, loss : 0.0175580765393866\n",
      "epoch : 749, loss : 0.017536040260958844\n",
      "epoch : 750, loss : 0.017514314211840968\n",
      "epoch : 751, loss : 0.01749218526441147\n",
      "epoch : 752, loss : 0.017470273995508975\n",
      "epoch : 753, loss : 0.017448501138551873\n",
      "epoch : 754, loss : 0.017426410984021434\n",
      "epoch : 755, loss : 0.017404624005676765\n",
      "epoch : 756, loss : 0.017382803765159934\n",
      "epoch : 757, loss : 0.01736075178667369\n",
      "epoch : 758, loss : 0.017339088429740704\n",
      "epoch : 759, loss : 0.017317220302119872\n",
      "epoch : 760, loss : 0.017295205948549528\n",
      "epoch : 761, loss : 0.01727366560459834\n",
      "epoch : 762, loss : 0.017251749162016848\n",
      "epoch : 763, loss : 0.017229771952812344\n",
      "epoch : 764, loss : 0.017208354081267415\n",
      "epoch : 765, loss : 0.017186388975705888\n",
      "epoch : 766, loss : 0.017164489958610776\n",
      "epoch : 767, loss : 0.01714308429850139\n",
      "epoch : 768, loss : 0.017121168161151244\n",
      "epoch : 769, loss : 0.017099358712426804\n",
      "epoch : 770, loss : 0.017077906633454445\n",
      "epoch : 771, loss : 0.017056026688809744\n",
      "epoch : 772, loss : 0.01703433886368268\n",
      "epoch : 773, loss : 0.017012837441533875\n",
      "epoch : 774, loss : 0.01699099360793575\n",
      "epoch : 775, loss : 0.0169694272072545\n",
      "epoch : 776, loss : 0.016947876456695533\n",
      "epoch : 777, loss : 0.016926068701243597\n",
      "epoch : 778, loss : 0.016904652985815234\n",
      "epoch : 779, loss : 0.016883251891678636\n",
      "epoch : 780, loss : 0.01686160864775403\n",
      "epoch : 781, loss : 0.016840350504615683\n",
      "epoch : 782, loss : 0.016818997789124795\n",
      "epoch : 783, loss : 0.01679738588858177\n",
      "epoch : 784, loss : 0.016776191872676215\n",
      "epoch : 785, loss : 0.016754860141856145\n",
      "epoch : 786, loss : 0.016733308379162298\n",
      "epoch : 787, loss : 0.01671217432428678\n",
      "epoch : 788, loss : 0.01669083369692287\n",
      "epoch : 789, loss : 0.016669367906318316\n",
      "epoch : 790, loss : 0.016648291397566753\n",
      "epoch : 791, loss : 0.01662693540678269\n",
      "epoch : 792, loss : 0.01660554861354242\n",
      "epoch : 793, loss : 0.016584537980903195\n",
      "epoch : 794, loss : 0.01656322258684178\n",
      "epoch : 795, loss : 0.016541825579173067\n",
      "epoch : 796, loss : 0.016520845334617125\n",
      "epoch : 797, loss : 0.01649963953890089\n",
      "epoch : 798, loss : 0.016478290765118923\n",
      "epoch : 799, loss : 0.016457351746189727\n",
      "epoch : 800, loss : 0.016436142793269277\n",
      "epoch : 801, loss : 0.016414865852839237\n",
      "epoch : 802, loss : 0.016394008005051988\n",
      "epoch : 803, loss : 0.01637283840358306\n",
      "epoch : 804, loss : 0.01635158625936531\n",
      "epoch : 805, loss : 0.016330737000973667\n",
      "epoch : 806, loss : 0.016309639498852932\n",
      "epoch : 807, loss : 0.016288451868646955\n",
      "epoch : 808, loss : 0.016267688442761708\n",
      "epoch : 809, loss : 0.016246631958411174\n",
      "epoch : 810, loss : 0.016225486256641764\n",
      "epoch : 811, loss : 0.016204740123384832\n",
      "epoch : 812, loss : 0.016183758758823417\n",
      "epoch : 813, loss : 0.01616267359227921\n",
      "epoch : 814, loss : 0.016141972423082285\n",
      "epoch : 815, loss : 0.01612107253904255\n",
      "epoch : 816, loss : 0.0161000655714879\n",
      "epoch : 817, loss : 0.016079446563054964\n",
      "epoch : 818, loss : 0.016058591918820076\n",
      "epoch : 819, loss : 0.016037638017447176\n",
      "epoch : 820, loss : 0.01601707947764045\n",
      "epoch : 821, loss : 0.015996312473522012\n",
      "epoch : 822, loss : 0.015975436402599\n",
      "epoch : 823, loss : 0.015954949733138026\n",
      "epoch : 824, loss : 0.015934275486522362\n",
      "epoch : 825, loss : 0.015913485753605083\n",
      "epoch : 826, loss : 0.015893083733810518\n",
      "epoch : 827, loss : 0.015872508486684325\n",
      "epoch : 828, loss : 0.015851815477884022\n",
      "epoch : 829, loss : 0.0158315134934321\n",
      "epoch : 830, loss : 0.01581104485702725\n",
      "epoch : 831, loss : 0.015790461031317603\n",
      "epoch : 832, loss : 0.01577027737171598\n",
      "epoch : 833, loss : 0.01574992442874277\n",
      "epoch : 834, loss : 0.015729464514046294\n",
      "epoch : 835, loss : 0.015709420685028924\n",
      "epoch : 836, loss : 0.015689194057235473\n",
      "epoch : 837, loss : 0.015668875222656274\n",
      "epoch : 838, loss : 0.01564899624539915\n",
      "epoch : 839, loss : 0.015628908097395824\n",
      "epoch : 840, loss : 0.015608750070006337\n",
      "epoch : 841, loss : 0.015589064731335672\n",
      "epoch : 842, loss : 0.01556912867251568\n",
      "epoch : 843, loss : 0.015549153757093842\n",
      "epoch : 844, loss : 0.015529694764034761\n",
      "epoch : 845, loss : 0.015509925602751884\n",
      "epoch : 846, loss : 0.015490158554114175\n",
      "epoch : 847, loss : 0.015470962535804721\n",
      "epoch : 848, loss : 0.01545137583415168\n",
      "epoch : 849, loss : 0.015431843525365503\n",
      "epoch : 850, loss : 0.015412950817970318\n",
      "epoch : 851, loss : 0.01539356219511884\n",
      "epoch : 852, loss : 0.015374336519944383\n",
      "epoch : 853, loss : 0.015355678896980763\n",
      "epoch : 854, loss : 0.015336602558685611\n",
      "epoch : 855, loss : 0.015317722712130685\n",
      "epoch : 856, loss : 0.015299289098101203\n",
      "epoch : 857, loss : 0.015280524076208755\n",
      "epoch : 858, loss : 0.015262068845435933\n",
      "epoch : 859, loss : 0.015243866489893673\n",
      "epoch : 860, loss : 0.015225440032991438\n",
      "epoch : 861, loss : 0.01520745649402745\n",
      "epoch : 862, loss : 0.015189490475676278\n",
      "epoch : 863, loss : 0.015171426490705475\n",
      "epoch : 864, loss : 0.015153960233159465\n",
      "epoch : 865, loss : 0.015136228790419228\n",
      "epoch : 866, loss : 0.015118574869842045\n",
      "epoch : 867, loss : 0.015101585432963978\n",
      "epoch : 868, loss : 0.015084166195748771\n",
      "epoch : 869, loss : 0.015067064508756875\n",
      "epoch : 870, loss : 0.015050289315212389\n",
      "epoch : 871, loss : 0.015033267072919685\n",
      "epoch : 872, loss : 0.015016783883150966\n",
      "epoch : 873, loss : 0.01500019278495101\n",
      "epoch : 874, loss : 0.014983582382913821\n",
      "epoch : 875, loss : 0.014967684168025252\n",
      "epoch : 876, loss : 0.014951317395912017\n",
      "epoch : 877, loss : 0.01493533035918455\n",
      "epoch : 878, loss : 0.014919549192145805\n",
      "epoch : 879, loss : 0.014903554732051256\n",
      "epoch : 880, loss : 0.01488823875454977\n",
      "epoch : 881, loss : 0.014872532105111925\n",
      "epoch : 882, loss : 0.014857260302000371\n",
      "epoch : 883, loss : 0.014842386163813524\n",
      "epoch : 884, loss : 0.014827286248740556\n",
      "epoch : 885, loss : 0.014812751056629152\n",
      "epoch : 886, loss : 0.014797990422702472\n",
      "epoch : 887, loss : 0.014783308290504608\n",
      "epoch : 888, loss : 0.014769176765576403\n",
      "epoch : 889, loss : 0.014754593170118221\n",
      "epoch : 890, loss : 0.01474041733909026\n",
      "epoch : 891, loss : 0.014726316909643846\n",
      "epoch : 892, loss : 0.014711974112001761\n",
      "epoch : 893, loss : 0.014698351629946692\n",
      "epoch : 894, loss : 0.01468415259637292\n",
      "epoch : 895, loss : 0.014670352662180038\n",
      "epoch : 896, loss : 0.014656619964622091\n",
      "epoch : 897, loss : 0.014642659645538592\n",
      "epoch : 898, loss : 0.014629342037789352\n",
      "epoch : 899, loss : 0.01461544347776329\n",
      "epoch : 900, loss : 0.01460195737889277\n",
      "epoch : 901, loss : 0.014588495284473271\n",
      "epoch : 902, loss : 0.01457478351286458\n",
      "epoch : 903, loss : 0.014561717885826246\n",
      "epoch : 904, loss : 0.014548044919257\n",
      "epoch : 905, loss : 0.014534773086754342\n",
      "epoch : 906, loss : 0.014521494621388733\n",
      "epoch : 907, loss : 0.014507966419753575\n",
      "epoch : 908, loss : 0.014495079032540763\n",
      "epoch : 909, loss : 0.014481585075117067\n",
      "epoch : 910, loss : 0.014468487826377352\n",
      "epoch : 911, loss : 0.014455289122901584\n",
      "epoch : 912, loss : 0.014441980901782508\n",
      "epoch : 913, loss : 0.014429119760787196\n",
      "epoch : 914, loss : 0.01441577890226655\n",
      "epoch : 915, loss : 0.01440288092774406\n",
      "epoch : 916, loss : 0.014389691647095054\n",
      "epoch : 917, loss : 0.014376596103790244\n",
      "epoch : 918, loss : 0.01436371106037312\n",
      "epoch : 919, loss : 0.014350453165175466\n",
      "epoch : 920, loss : 0.014337788636199788\n",
      "epoch : 921, loss : 0.01432452389776843\n",
      "epoch : 922, loss : 0.014311673255559914\n",
      "epoch : 923, loss : 0.014298724440716027\n",
      "epoch : 924, loss : 0.014285593847897711\n",
      "epoch : 925, loss : 0.014272985673434461\n",
      "epoch : 926, loss : 0.014259775026451468\n",
      "epoch : 927, loss : 0.014247089801313494\n",
      "epoch : 928, loss : 0.014234074397401566\n",
      "epoch : 929, loss : 0.014221157208857542\n",
      "epoch : 930, loss : 0.014208412548587516\n",
      "epoch : 931, loss : 0.014195268145663908\n",
      "epoch : 932, loss : 0.014182837165160308\n",
      "epoch : 933, loss : 0.014169671962032674\n",
      "epoch : 934, loss : 0.014156997771666334\n",
      "epoch : 935, loss : 0.014144119440138056\n",
      "epoch : 936, loss : 0.014131232723703768\n",
      "epoch : 937, loss : 0.01411864334671334\n",
      "epoch : 938, loss : 0.014105564705688729\n",
      "epoch : 939, loss : 0.014093095737032721\n",
      "epoch : 940, loss : 0.014080078253955955\n",
      "epoch : 941, loss : 0.014067407887180803\n",
      "epoch : 942, loss : 0.01405468938474331\n",
      "epoch : 943, loss : 0.01404174190198395\n",
      "epoch : 944, loss : 0.01402930688913023\n",
      "epoch : 945, loss : 0.014016290778520535\n",
      "epoch : 946, loss : 0.014003834177498684\n",
      "epoch : 947, loss : 0.013990951863823787\n",
      "epoch : 948, loss : 0.01397824523685158\n",
      "epoch : 949, loss : 0.013965712965517246\n",
      "epoch : 950, loss : 0.01395284511165852\n",
      "epoch : 951, loss : 0.013940597954433339\n",
      "epoch : 952, loss : 0.013927702530028745\n",
      "epoch : 953, loss : 0.013915402227042134\n",
      "epoch : 954, loss : 0.013902621559524372\n",
      "epoch : 955, loss : 0.013890131738206675\n",
      "epoch : 956, loss : 0.013877585891276533\n",
      "epoch : 957, loss : 0.013864900988113245\n",
      "epoch : 958, loss : 0.013852579935203684\n",
      "epoch : 959, loss : 0.013839738085978874\n",
      "epoch : 960, loss : 0.013827601331649127\n",
      "epoch : 961, loss : 0.013814740760687326\n",
      "epoch : 962, loss : 0.013802450442993835\n",
      "epoch : 963, loss : 0.013789826218054491\n",
      "epoch : 964, loss : 0.01377731722950979\n",
      "epoch : 965, loss : 0.013764923004757556\n",
      "epoch : 966, loss : 0.013752216736978\n",
      "epoch : 967, loss : 0.013740047642804582\n",
      "epoch : 968, loss : 0.013727271193051675\n",
      "epoch : 969, loss : 0.013715108690034897\n",
      "epoch : 970, loss : 0.013702401939086447\n",
      "epoch : 971, loss : 0.013690080813422994\n",
      "epoch : 972, loss : 0.013677597156216567\n",
      "epoch : 973, loss : 0.01366507813769532\n",
      "epoch : 974, loss : 0.013652820001115305\n",
      "epoch : 975, loss : 0.013640102667077585\n",
      "epoch : 976, loss : 0.01362807009739304\n",
      "epoch : 977, loss : 0.013615356642130587\n",
      "epoch : 978, loss : 0.013603179091964446\n",
      "epoch : 979, loss : 0.01359061090918116\n",
      "epoch : 980, loss : 0.013578272696865915\n",
      "epoch : 981, loss : 0.013565928363386015\n",
      "epoch : 982, loss : 0.013553390325864495\n",
      "epoch : 983, loss : 0.01354127236069947\n",
      "epoch : 984, loss : 0.013528606276567582\n",
      "epoch : 985, loss : 0.013516611094124859\n",
      "epoch : 986, loss : 0.013503962481161733\n",
      "epoch : 987, loss : 0.013491779666317304\n",
      "epoch : 988, loss : 0.013479371804982423\n",
      "epoch : 989, loss : 0.013466986818351664\n",
      "epoch : 990, loss : 0.01345480669191902\n",
      "epoch : 991, loss : 0.013442219731744486\n",
      "epoch : 992, loss : 0.013430267428116089\n",
      "epoch : 993, loss : 0.013417659270769272\n",
      "epoch : 994, loss : 0.013405600568696873\n",
      "epoch : 995, loss : 0.013393113608150833\n",
      "epoch : 996, loss : 0.01338089179553269\n",
      "epoch : 997, loss : 0.01336863001563801\n",
      "epoch : 998, loss : 0.0133562063519642\n",
      "epoch : 999, loss : 0.013344172198381644\n",
      "epoch : 1000, loss : 0.013331606329928837\n"
     ]
    }
   ],
   "source": [
    "# X  = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "# Y = np.array([0,1,1,1])\n",
    "\n",
    "model = DeepNeuralNetwork(\n",
    "    input_size=4,\n",
    "    output_size=1,\n",
    "    hidden_size=16,\n",
    "    epoches=1000,\n",
    "    learning_rate=0.001,\n",
    "    dropout=0.8,\n",
    "    hidden_layers=4\n",
    ")\n",
    "\n",
    "model.fit(X_train,Y_train)\n",
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb3dda5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       183\n",
      "           1       1.00      0.96      0.98       160\n",
      "\n",
      "    accuracy                           0.98       343\n",
      "   macro avg       0.98      0.98      0.98       343\n",
      "weighted avg       0.98      0.98      0.98       343\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "cmf = confusion_matrix(Y_test,preds)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(cmf,annot=True,fmt='d')\n",
    "plt.xlabel(\"True\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.show()\n",
    "print(classification_report(Y_test,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d614fbb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
