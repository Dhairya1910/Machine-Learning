{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d17618f",
   "metadata": {},
   "source": [
    "Implementation of Neural network from scratch (forward feed method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1554e148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34952ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self,input_layer_size,hidden_layer_size,output_layer_size,epochs=10,learning_rate=0.01):\n",
    "        self.input_layer_size = input_layer_size \n",
    "        self.hidden_layer_size = hidden_layer_size \n",
    "        self.output_layer_size = output_layer_size\n",
    "        self.epochs = epochs \n",
    "        self.learning_rate = learning_rate \n",
    "\n",
    "        \"\"\"Initialization of weights and bias\"\"\"\n",
    "        self.W1 = np.random.randn(self.input_layer_size,self.hidden_layer_size)\n",
    "        self.B1 = np.zeros((1,hidden_layer_size))\n",
    "        self.W2 = np.random.randn(self.hidden_layer_size,self.output_layer_size)\n",
    "        self.B2 = np.zeros((1,output_layer_size))\n",
    "\n",
    "    def sigmoid(self,z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "\n",
    "    def log_loss(self,A2,Y):\n",
    "        return -np.mean(Y*np.log(A2)+(1-Y)*np.log(1-A2))\n",
    "\n",
    "    def derivative_sigmoid(self,z):\n",
    "        s = self.sigmoid(z)\n",
    "        return s * (1-s)\n",
    "    \n",
    "    def ForwardPropagation(self,X):\n",
    "        \"\"\"Calculation from Input layer to Hidden Layer\"\"\"\n",
    "        self.Z1 = np.dot(X,self.W1) + self.B1 # size of x be (m,n) (dot product) size of W1 be (n,x) + size of B1 be (1,x) \n",
    "        self.A1 = self.sigmoid(self.Z1)\n",
    "        \"\"\"Calculation from Hidden Layer to Output Layer\"\"\"\n",
    "        self.Z2 = np.dot(self.A1,self.W2) + self.B2\n",
    "        self.A2 = self.sigmoid(self.Z2)\n",
    "\n",
    "        return self.A2 \n",
    "\n",
    "    def BackPropagation(self,X,Y):\n",
    "        m = X.shape[0] #number of samples\n",
    "        error = self.A2 - Y \n",
    "        \n",
    "        \"\"\"derivatives for output layer\"\"\"\n",
    "        DW2 = 1/m * np.dot(self.A1.T,error)\n",
    "        DB2 = 1/m * np.sum(error,axis=0,keepdims=True)\n",
    "\n",
    "        \"\"\"derivatives for hidden layer\"\"\" \n",
    "        DA1 = np.dot(error,self.W2.T) # error in each neuron of hidden layer \n",
    "        DZ1 = DA1 * self.derivative_sigmoid(self.Z1) # error before applying sigmoid operation \n",
    "        DW1 = 1/m * np.dot(X.T,DZ1) \n",
    "        DB1 = 1/m * np.sum(DZ1,axis=0,keepdims=True)\n",
    "\n",
    "        \"\"\"find gradient values\"\"\"\n",
    "        self.W1 = self.W1 - self.learning_rate * DW1 \n",
    "        self.W2 = self.W2 - self.learning_rate * DW2\n",
    "\n",
    "        self.B1 = self.B1 - self.learning_rate * DB1 \n",
    "        self.B2 = self.B2 - self.learning_rate * DB2 \n",
    "\n",
    "    \n",
    "    def fit(self,X,Y):\n",
    "        Y = np.array(Y).reshape(-1,1)\n",
    "        for _ in range(self.epochs):\n",
    "            Output_fw = self.ForwardPropagation(X)\n",
    "            loss = self.log_loss(Output_fw,Y)\n",
    "            print(f\"epoch {_+1} , loss = {loss}\")\n",
    "            self.BackPropagation(X,Y)\n",
    "\n",
    "    def predict(self,X):\n",
    "        preds = self.ForwardPropagation(X)\n",
    "        return (preds > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4eb46ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 , loss = 0.687173520439315\n",
      "Predictions : [0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# X = np.random.rand(2,2) # this simply creates an 2d array of 5 parameters and 10 samples \n",
    "# Y = [1,0]\n",
    "\"\"\"implemented And gate using neural network from scratch\"\"\"\n",
    "X = np.array([[1,1],[1,1],[1,1],[1,1]]) \n",
    "Y = np.array([0,0,0,1])\n",
    "model = NeuralNetwork(\n",
    "    input_layer_size=2,\n",
    "    hidden_layer_size=2,\n",
    "    output_layer_size=1,\n",
    "    epochs=1\n",
    "    )\n",
    "model.fit(X,Y)\n",
    "pred = model.predict(X)\n",
    "print(f'Predictions : {pred.flatten()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c325b0e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((5,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150c647e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
