{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70cd4fe3",
   "metadata": {},
   "source": [
    "# Full fledge neural net with hyperparameter and droput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6565dba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "442fe161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DeepNeuralNetwork:\n",
    "    def __init__(self, input_size, output_size, hidden_size, epoches=1000, learning_rate=0.001,\n",
    "                 dropout=0.5, hidden_layers=2, b1=0.9, b2=0.999):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.dropout = dropout\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.epoches = epoches\n",
    "        self.b1 = b1\n",
    "        self.b2 = b2\n",
    "        self.epsilon = 1e-8\n",
    "        self.t = 1  # timestep for bias correction\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.hidden_weights = [np.random.randn(input_size, hidden_size) * np.sqrt(2 / input_size)]\n",
    "        self.hidden_bias = [np.zeros((1, hidden_size))]\n",
    "\n",
    "        for _ in range(hidden_layers - 1):\n",
    "            self.hidden_weights.append(np.random.randn(hidden_size, hidden_size) * np.sqrt(2 / hidden_size))\n",
    "            self.hidden_bias.append(np.zeros((1, hidden_size)))\n",
    "\n",
    "        self.output_weight = np.random.randn(hidden_size, output_size) * np.sqrt(2 / hidden_size)\n",
    "        self.output_bias = np.zeros((1, output_size))\n",
    "\n",
    "        self.mw = [np.zeros_like(w) for w in self.hidden_weights]\n",
    "        self.vw = [np.zeros_like(w) for w in self.hidden_weights]\n",
    "        self.mb = [np.zeros_like(b) for b in self.hidden_bias]\n",
    "        self.vb = [np.zeros_like(b) for b in self.hidden_bias]\n",
    "\n",
    "        self.mw_out = np.zeros_like(self.output_weight)\n",
    "        self.vw_out = np.zeros_like(self.output_weight)\n",
    "        self.mb_out = np.zeros_like(self.output_bias)\n",
    "        self.vb_out = np.zeros_like(self.output_bias)\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def derivative_sigmoid(self, z):\n",
    "        s = self.sigmoid(z)\n",
    "        return s * (1 - s)\n",
    "\n",
    "    def Relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    def derivative_relu(self, z):\n",
    "        return (z > 0).astype(float)\n",
    "\n",
    "    def compute_loss(self, preds, Y):\n",
    "        preds = np.clip(preds, 1e-8, 1 - 1e-8)\n",
    "        return -np.mean(Y * np.log(preds) + (1 - Y) * np.log(1 - preds))\n",
    "\n",
    "    def Dropout(self, A):\n",
    "        mask = np.random.rand(*A.shape) < self.dropout\n",
    "        return (mask * A) / self.dropout\n",
    "\n",
    "    def Adam(self, dw, db, l):\n",
    "\n",
    "        self.mw[l] = self.b1 * self.mw[l] + (1 - self.b1) * dw\n",
    "        self.vw[l] = self.b2 * self.vw[l] + (1 - self.b2) * (dw ** 2)\n",
    "\n",
    "        mw_corr = self.mw[l] / (1 - self.b1 ** self.t)\n",
    "        vw_corr = self.vw[l] / (1 - self.b2 ** self.t)\n",
    "\n",
    "  \n",
    "        self.mb[l] = self.b1 * self.mb[l] + (1 - self.b1) * db\n",
    "        self.vb[l] = self.b2 * self.vb[l] + (1 - self.b2) * (db ** 2)\n",
    "\n",
    "        mb_corr = self.mb[l] / (1 - self.b1 ** self.t)\n",
    "        vb_corr = self.vb[l] / (1 - self.b2 ** self.t)\n",
    "\n",
    "\n",
    "        W = mw_corr / (np.sqrt(vw_corr) + self.epsilon)\n",
    "        B = mb_corr / (np.sqrt(vb_corr) + self.epsilon)\n",
    "        return W, B\n",
    "\n",
    "    def Adam_output(self, dw, db):\n",
    "        self.mw_out = self.b1 * self.mw_out + (1 - self.b1) * dw\n",
    "        self.vw_out = self.b2 * self.vw_out + (1 - self.b2) * (dw ** 2)\n",
    "\n",
    "        mw_corr = self.mw_out / (1 - self.b1 ** self.t)\n",
    "        vw_corr = self.vw_out / (1 - self.b2 ** self.t)\n",
    "\n",
    "        self.mb_out = self.b1 * self.mb_out + (1 - self.b1) * db\n",
    "        self.vb_out = self.b2 * self.vb_out + (1 - self.b2) * (db ** 2)\n",
    "\n",
    "        mb_corr = self.mb_out / (1 - self.b1 ** self.t)\n",
    "        vb_corr = self.vb_out / (1 - self.b2 ** self.t)\n",
    "\n",
    "        W = mw_corr / (np.sqrt(vw_corr) + self.epsilon)\n",
    "        B = mb_corr / (np.sqrt(vb_corr) + self.epsilon)\n",
    "        return W, B\n",
    "\n",
    "    def ForwardPropagation(self, X):\n",
    "        self.Activations = [X]\n",
    "        self.hidden_Z = []\n",
    "\n",
    "        for l in range(self.hidden_layers):\n",
    "            Z = np.dot(self.Activations[l], self.hidden_weights[l]) + self.hidden_bias[l]\n",
    "            self.hidden_Z.append(Z)\n",
    "            A = self.Relu(Z)\n",
    "            A = self.Dropout(A)\n",
    "            self.Activations.append(A)\n",
    "\n",
    "        self.output_Z = np.dot(self.Activations[-1], self.output_weight) + self.output_bias\n",
    "        self.output_A = self.sigmoid(self.output_Z)\n",
    "        return self.output_A\n",
    "\n",
    "    def BackPropagation(self, X, Y):\n",
    "        m = X.shape[0]\n",
    "        self.t += 1  \n",
    "\n",
    "        error = self.output_A - Y\n",
    "        Dwo = 1 / m * np.dot(self.Activations[-1].T, error)\n",
    "        Dbo = 1 / m * np.sum(error, axis=0, keepdims=True)\n",
    "\n",
    "        W_update, B_update = self.Adam_output(Dwo, Dbo)\n",
    "        self.output_weight -= self.learning_rate * W_update\n",
    "        self.output_bias -= self.learning_rate * B_update\n",
    "\n",
    "        for l in reversed(range(self.hidden_layers)):\n",
    "            da = np.dot(error, self.output_weight.T if l == self.hidden_layers - 1 else self.hidden_weights[l + 1].T)\n",
    "            dz = da * self.derivative_relu(self.hidden_Z[l])\n",
    "            dw = 1 / m * np.dot(self.Activations[l].T, dz)\n",
    "            db = 1 / m * np.sum(dz, axis=0, keepdims=True)\n",
    "            error = dz\n",
    "\n",
    "            W, B = self.Adam(dw, db, l)\n",
    "            self.hidden_weights[l] -= self.learning_rate * W\n",
    "            self.hidden_bias[l] -= self.learning_rate * B\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        Y = np.array(Y).reshape(-1, 1)\n",
    "        for epoch in range(self.epoches):\n",
    "            preds_F = self.ForwardPropagation(X)\n",
    "            loss = self.compute_loss(preds_F, Y)\n",
    "            print(f\"epoch: {epoch + 1}, loss: {loss:.6f}\")\n",
    "            self.BackPropagation(X, Y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.ForwardPropagation(X) > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17900ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.62160</td>\n",
       "      <td>8.6661</td>\n",
       "      <td>-2.8073</td>\n",
       "      <td>-0.44699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.1674</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.6383</td>\n",
       "      <td>1.9242</td>\n",
       "      <td>0.10645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.5228</td>\n",
       "      <td>-4.0112</td>\n",
       "      <td>-3.59440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.4552</td>\n",
       "      <td>4.5718</td>\n",
       "      <td>-0.98880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variance  skewness  curtosis  entropy  class\n",
       "0   3.62160    8.6661   -2.8073 -0.44699      0\n",
       "1   4.54590    8.1674   -2.4586 -1.46210      0\n",
       "2   3.86600   -2.6383    1.9242  0.10645      0\n",
       "3   3.45660    9.5228   -4.0112 -3.59440      0\n",
       "4   0.32924   -4.4552    4.5718 -0.98880      0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "dataset = pd.read_csv(\"../BankNote_Authentication.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05df4745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1029, 1)\n"
     ]
    }
   ],
   "source": [
    "Y = np.array(dataset['class']).reshape(-1,1)\n",
    "X = np.array(dataset.drop(columns=['class']))\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y)\n",
    "\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd1d14ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 2.699763\n",
      "epoch: 2, loss: 2.970510\n",
      "epoch: 3, loss: 2.569685\n",
      "epoch: 4, loss: 2.819074\n",
      "epoch: 5, loss: 2.444407\n",
      "epoch: 6, loss: 2.376190\n",
      "epoch: 7, loss: 2.449560\n",
      "epoch: 8, loss: 2.430240\n",
      "epoch: 9, loss: 2.306749\n",
      "epoch: 10, loss: 2.340564\n",
      "epoch: 11, loss: 2.187684\n",
      "epoch: 12, loss: 1.998306\n",
      "epoch: 13, loss: 2.012799\n",
      "epoch: 14, loss: 2.202371\n",
      "epoch: 15, loss: 2.028654\n",
      "epoch: 16, loss: 2.069457\n",
      "epoch: 17, loss: 2.059302\n",
      "epoch: 18, loss: 1.958949\n",
      "epoch: 19, loss: 1.954255\n",
      "epoch: 20, loss: 1.960761\n",
      "epoch: 21, loss: 1.855412\n",
      "epoch: 22, loss: 1.852337\n",
      "epoch: 23, loss: 1.813594\n",
      "epoch: 24, loss: 1.653063\n",
      "epoch: 25, loss: 1.950143\n",
      "epoch: 26, loss: 1.820264\n",
      "epoch: 27, loss: 1.622089\n",
      "epoch: 28, loss: 1.665053\n",
      "epoch: 29, loss: 1.843990\n",
      "epoch: 30, loss: 1.620025\n",
      "epoch: 31, loss: 1.684038\n",
      "epoch: 32, loss: 1.509626\n",
      "epoch: 33, loss: 1.590536\n",
      "epoch: 34, loss: 1.662309\n",
      "epoch: 35, loss: 1.498426\n",
      "epoch: 36, loss: 1.693833\n",
      "epoch: 37, loss: 1.632916\n",
      "epoch: 38, loss: 1.335387\n",
      "epoch: 39, loss: 1.588655\n",
      "epoch: 40, loss: 1.553218\n",
      "epoch: 41, loss: 1.465303\n",
      "epoch: 42, loss: 1.365175\n",
      "epoch: 43, loss: 1.486703\n",
      "epoch: 44, loss: 1.396512\n",
      "epoch: 45, loss: 1.409116\n",
      "epoch: 46, loss: 1.414942\n",
      "epoch: 47, loss: 1.452091\n",
      "epoch: 48, loss: 1.401188\n",
      "epoch: 49, loss: 1.300391\n",
      "epoch: 50, loss: 1.371657\n",
      "epoch: 51, loss: 1.286715\n",
      "epoch: 52, loss: 1.168822\n",
      "epoch: 53, loss: 1.341051\n",
      "epoch: 54, loss: 1.155336\n",
      "epoch: 55, loss: 1.216101\n",
      "epoch: 56, loss: 1.286839\n",
      "epoch: 57, loss: 1.170917\n",
      "epoch: 58, loss: 1.151407\n",
      "epoch: 59, loss: 1.262453\n",
      "epoch: 60, loss: 1.280669\n",
      "epoch: 61, loss: 1.167149\n",
      "epoch: 62, loss: 1.236107\n",
      "epoch: 63, loss: 1.192640\n",
      "epoch: 64, loss: 1.117604\n",
      "epoch: 65, loss: 1.205592\n",
      "epoch: 66, loss: 1.199701\n",
      "epoch: 67, loss: 1.101834\n",
      "epoch: 68, loss: 1.183121\n",
      "epoch: 69, loss: 1.193932\n",
      "epoch: 70, loss: 1.171876\n",
      "epoch: 71, loss: 1.088975\n",
      "epoch: 72, loss: 1.269554\n",
      "epoch: 73, loss: 1.089950\n",
      "epoch: 74, loss: 1.002276\n",
      "epoch: 75, loss: 1.168983\n",
      "epoch: 76, loss: 1.269528\n",
      "epoch: 77, loss: 1.102389\n",
      "epoch: 78, loss: 1.105060\n",
      "epoch: 79, loss: 1.129554\n",
      "epoch: 80, loss: 0.998670\n",
      "epoch: 81, loss: 1.062874\n",
      "epoch: 82, loss: 0.920579\n",
      "epoch: 83, loss: 1.101380\n",
      "epoch: 84, loss: 0.998790\n",
      "epoch: 85, loss: 0.957073\n",
      "epoch: 86, loss: 1.015412\n",
      "epoch: 87, loss: 0.947302\n",
      "epoch: 88, loss: 1.007287\n",
      "epoch: 89, loss: 1.044695\n",
      "epoch: 90, loss: 1.002617\n",
      "epoch: 91, loss: 0.948418\n",
      "epoch: 92, loss: 1.063256\n",
      "epoch: 93, loss: 0.976365\n",
      "epoch: 94, loss: 1.020396\n",
      "epoch: 95, loss: 1.126854\n",
      "epoch: 96, loss: 0.980830\n",
      "epoch: 97, loss: 1.005676\n",
      "epoch: 98, loss: 0.972185\n",
      "epoch: 99, loss: 0.947142\n",
      "epoch: 100, loss: 0.952377\n",
      "epoch: 101, loss: 1.062255\n",
      "epoch: 102, loss: 0.963161\n",
      "epoch: 103, loss: 0.930653\n",
      "epoch: 104, loss: 1.050490\n",
      "epoch: 105, loss: 0.966828\n",
      "epoch: 106, loss: 0.942737\n",
      "epoch: 107, loss: 0.890822\n",
      "epoch: 108, loss: 0.947782\n",
      "epoch: 109, loss: 1.064207\n",
      "epoch: 110, loss: 0.976717\n",
      "epoch: 111, loss: 0.933904\n",
      "epoch: 112, loss: 0.905743\n",
      "epoch: 113, loss: 0.825537\n",
      "epoch: 114, loss: 0.972412\n",
      "epoch: 115, loss: 0.772913\n",
      "epoch: 116, loss: 0.945378\n",
      "epoch: 117, loss: 0.939766\n",
      "epoch: 118, loss: 0.748579\n",
      "epoch: 119, loss: 0.917521\n",
      "epoch: 120, loss: 0.892951\n",
      "epoch: 121, loss: 0.895469\n",
      "epoch: 122, loss: 0.953980\n",
      "epoch: 123, loss: 0.908614\n",
      "epoch: 124, loss: 0.892952\n",
      "epoch: 125, loss: 0.866966\n",
      "epoch: 126, loss: 0.860148\n",
      "epoch: 127, loss: 0.921796\n",
      "epoch: 128, loss: 0.872392\n",
      "epoch: 129, loss: 0.866187\n",
      "epoch: 130, loss: 0.880395\n",
      "epoch: 131, loss: 0.966101\n",
      "epoch: 132, loss: 0.774845\n",
      "epoch: 133, loss: 0.925436\n",
      "epoch: 134, loss: 0.767117\n",
      "epoch: 135, loss: 0.696752\n",
      "epoch: 136, loss: 0.705590\n",
      "epoch: 137, loss: 0.828015\n",
      "epoch: 138, loss: 0.743343\n",
      "epoch: 139, loss: 0.881751\n",
      "epoch: 140, loss: 0.883042\n",
      "epoch: 141, loss: 0.845512\n",
      "epoch: 142, loss: 0.794459\n",
      "epoch: 143, loss: 0.809528\n",
      "epoch: 144, loss: 0.797432\n",
      "epoch: 145, loss: 0.775430\n",
      "epoch: 146, loss: 0.685684\n",
      "epoch: 147, loss: 0.861166\n",
      "epoch: 148, loss: 0.772967\n",
      "epoch: 149, loss: 0.713134\n",
      "epoch: 150, loss: 0.722198\n",
      "epoch: 151, loss: 0.679543\n",
      "epoch: 152, loss: 0.768585\n",
      "epoch: 153, loss: 0.689302\n",
      "epoch: 154, loss: 0.718628\n",
      "epoch: 155, loss: 0.705420\n",
      "epoch: 156, loss: 0.704409\n",
      "epoch: 157, loss: 0.664114\n",
      "epoch: 158, loss: 0.702136\n",
      "epoch: 159, loss: 0.716077\n",
      "epoch: 160, loss: 0.814308\n",
      "epoch: 161, loss: 0.734279\n",
      "epoch: 162, loss: 0.877750\n",
      "epoch: 163, loss: 0.792632\n",
      "epoch: 164, loss: 0.666591\n",
      "epoch: 165, loss: 0.683389\n",
      "epoch: 166, loss: 0.758384\n",
      "epoch: 167, loss: 0.687232\n",
      "epoch: 168, loss: 0.771693\n",
      "epoch: 169, loss: 0.650095\n",
      "epoch: 170, loss: 0.705781\n",
      "epoch: 171, loss: 0.688833\n",
      "epoch: 172, loss: 0.660740\n",
      "epoch: 173, loss: 0.637862\n",
      "epoch: 174, loss: 0.761992\n",
      "epoch: 175, loss: 0.738144\n",
      "epoch: 176, loss: 0.675747\n",
      "epoch: 177, loss: 0.642053\n",
      "epoch: 178, loss: 0.558133\n",
      "epoch: 179, loss: 0.668188\n",
      "epoch: 180, loss: 0.609755\n",
      "epoch: 181, loss: 0.619140\n",
      "epoch: 182, loss: 0.692469\n",
      "epoch: 183, loss: 0.750219\n",
      "epoch: 184, loss: 0.680792\n",
      "epoch: 185, loss: 0.630710\n",
      "epoch: 186, loss: 0.619075\n",
      "epoch: 187, loss: 0.598722\n",
      "epoch: 188, loss: 0.592668\n",
      "epoch: 189, loss: 0.672318\n",
      "epoch: 190, loss: 0.775154\n",
      "epoch: 191, loss: 0.689519\n",
      "epoch: 192, loss: 0.625761\n",
      "epoch: 193, loss: 0.685344\n",
      "epoch: 194, loss: 0.704647\n",
      "epoch: 195, loss: 0.561450\n",
      "epoch: 196, loss: 0.688102\n",
      "epoch: 197, loss: 0.609599\n",
      "epoch: 198, loss: 0.637731\n",
      "epoch: 199, loss: 0.644106\n",
      "epoch: 200, loss: 0.588193\n",
      "epoch: 201, loss: 0.622353\n",
      "epoch: 202, loss: 0.666959\n",
      "epoch: 203, loss: 0.475210\n",
      "epoch: 204, loss: 0.550913\n",
      "epoch: 205, loss: 0.632602\n",
      "epoch: 206, loss: 0.541228\n",
      "epoch: 207, loss: 0.520582\n",
      "epoch: 208, loss: 0.561915\n",
      "epoch: 209, loss: 0.567084\n",
      "epoch: 210, loss: 0.572897\n",
      "epoch: 211, loss: 0.557579\n",
      "epoch: 212, loss: 0.512191\n",
      "epoch: 213, loss: 0.544322\n",
      "epoch: 214, loss: 0.519929\n",
      "epoch: 215, loss: 0.459956\n",
      "epoch: 216, loss: 0.552147\n",
      "epoch: 217, loss: 0.599712\n",
      "epoch: 218, loss: 0.585990\n",
      "epoch: 219, loss: 0.469766\n",
      "epoch: 220, loss: 0.562387\n",
      "epoch: 221, loss: 0.551334\n",
      "epoch: 222, loss: 0.555782\n",
      "epoch: 223, loss: 0.560956\n",
      "epoch: 224, loss: 0.572564\n",
      "epoch: 225, loss: 0.488629\n",
      "epoch: 226, loss: 0.469861\n",
      "epoch: 227, loss: 0.505205\n",
      "epoch: 228, loss: 0.443202\n",
      "epoch: 229, loss: 0.509426\n",
      "epoch: 230, loss: 0.430123\n",
      "epoch: 231, loss: 0.611245\n",
      "epoch: 232, loss: 0.565523\n",
      "epoch: 233, loss: 0.582239\n",
      "epoch: 234, loss: 0.470612\n",
      "epoch: 235, loss: 0.480539\n",
      "epoch: 236, loss: 0.499265\n",
      "epoch: 237, loss: 0.520346\n",
      "epoch: 238, loss: 0.458082\n",
      "epoch: 239, loss: 0.514265\n",
      "epoch: 240, loss: 0.547957\n",
      "epoch: 241, loss: 0.531108\n",
      "epoch: 242, loss: 0.609622\n",
      "epoch: 243, loss: 0.497465\n",
      "epoch: 244, loss: 0.558416\n",
      "epoch: 245, loss: 0.547647\n",
      "epoch: 246, loss: 0.609052\n",
      "epoch: 247, loss: 0.417462\n",
      "epoch: 248, loss: 0.450531\n",
      "epoch: 249, loss: 0.601578\n",
      "epoch: 250, loss: 0.459553\n",
      "epoch: 251, loss: 0.575488\n",
      "epoch: 252, loss: 0.470168\n",
      "epoch: 253, loss: 0.551600\n",
      "epoch: 254, loss: 0.468674\n",
      "epoch: 255, loss: 0.546952\n",
      "epoch: 256, loss: 0.414298\n",
      "epoch: 257, loss: 0.496239\n",
      "epoch: 258, loss: 0.551781\n",
      "epoch: 259, loss: 0.420291\n",
      "epoch: 260, loss: 0.527579\n",
      "epoch: 261, loss: 0.404605\n",
      "epoch: 262, loss: 0.502965\n",
      "epoch: 263, loss: 0.442380\n",
      "epoch: 264, loss: 0.497319\n",
      "epoch: 265, loss: 0.375117\n",
      "epoch: 266, loss: 0.374115\n",
      "epoch: 267, loss: 0.508769\n",
      "epoch: 268, loss: 0.458707\n",
      "epoch: 269, loss: 0.425177\n",
      "epoch: 270, loss: 0.457982\n",
      "epoch: 271, loss: 0.527259\n",
      "epoch: 272, loss: 0.380012\n",
      "epoch: 273, loss: 0.409503\n",
      "epoch: 274, loss: 0.409653\n",
      "epoch: 275, loss: 0.398064\n",
      "epoch: 276, loss: 0.442659\n",
      "epoch: 277, loss: 0.441619\n",
      "epoch: 278, loss: 0.451818\n",
      "epoch: 279, loss: 0.467080\n",
      "epoch: 280, loss: 0.456425\n",
      "epoch: 281, loss: 0.466820\n",
      "epoch: 282, loss: 0.496461\n",
      "epoch: 283, loss: 0.528166\n",
      "epoch: 284, loss: 0.453142\n",
      "epoch: 285, loss: 0.464405\n",
      "epoch: 286, loss: 0.450435\n",
      "epoch: 287, loss: 0.468214\n",
      "epoch: 288, loss: 0.493978\n",
      "epoch: 289, loss: 0.430541\n",
      "epoch: 290, loss: 0.437136\n",
      "epoch: 291, loss: 0.387949\n",
      "epoch: 292, loss: 0.450258\n",
      "epoch: 293, loss: 0.487566\n",
      "epoch: 294, loss: 0.485173\n",
      "epoch: 295, loss: 0.410025\n",
      "epoch: 296, loss: 0.481392\n",
      "epoch: 297, loss: 0.406371\n",
      "epoch: 298, loss: 0.552762\n",
      "epoch: 299, loss: 0.488649\n",
      "epoch: 300, loss: 0.470584\n",
      "epoch: 301, loss: 0.370618\n",
      "epoch: 302, loss: 0.405626\n",
      "epoch: 303, loss: 0.417555\n",
      "epoch: 304, loss: 0.368262\n",
      "epoch: 305, loss: 0.458152\n",
      "epoch: 306, loss: 0.401842\n",
      "epoch: 307, loss: 0.461507\n",
      "epoch: 308, loss: 0.413677\n",
      "epoch: 309, loss: 0.344610\n",
      "epoch: 310, loss: 0.396960\n",
      "epoch: 311, loss: 0.387336\n",
      "epoch: 312, loss: 0.363072\n",
      "epoch: 313, loss: 0.414236\n",
      "epoch: 314, loss: 0.402429\n",
      "epoch: 315, loss: 0.429450\n",
      "epoch: 316, loss: 0.445852\n",
      "epoch: 317, loss: 0.415391\n",
      "epoch: 318, loss: 0.493603\n",
      "epoch: 319, loss: 0.408031\n",
      "epoch: 320, loss: 0.455937\n",
      "epoch: 321, loss: 0.324982\n",
      "epoch: 322, loss: 0.445049\n",
      "epoch: 323, loss: 0.427734\n",
      "epoch: 324, loss: 0.333635\n",
      "epoch: 325, loss: 0.352396\n",
      "epoch: 326, loss: 0.282944\n",
      "epoch: 327, loss: 0.448714\n",
      "epoch: 328, loss: 0.354488\n",
      "epoch: 329, loss: 0.404503\n",
      "epoch: 330, loss: 0.395139\n",
      "epoch: 331, loss: 0.403190\n",
      "epoch: 332, loss: 0.501657\n",
      "epoch: 333, loss: 0.474554\n",
      "epoch: 334, loss: 0.416608\n",
      "epoch: 335, loss: 0.332862\n",
      "epoch: 336, loss: 0.434867\n",
      "epoch: 337, loss: 0.410125\n",
      "epoch: 338, loss: 0.528285\n",
      "epoch: 339, loss: 0.422318\n",
      "epoch: 340, loss: 0.401384\n",
      "epoch: 341, loss: 0.354492\n",
      "epoch: 342, loss: 0.370607\n",
      "epoch: 343, loss: 0.405645\n",
      "epoch: 344, loss: 0.356724\n",
      "epoch: 345, loss: 0.370564\n",
      "epoch: 346, loss: 0.381283\n",
      "epoch: 347, loss: 0.486533\n",
      "epoch: 348, loss: 0.351881\n",
      "epoch: 349, loss: 0.343975\n",
      "epoch: 350, loss: 0.331958\n",
      "epoch: 351, loss: 0.373741\n",
      "epoch: 352, loss: 0.461062\n",
      "epoch: 353, loss: 0.378253\n",
      "epoch: 354, loss: 0.404702\n",
      "epoch: 355, loss: 0.406053\n",
      "epoch: 356, loss: 0.417689\n",
      "epoch: 357, loss: 0.308264\n",
      "epoch: 358, loss: 0.363813\n",
      "epoch: 359, loss: 0.423267\n",
      "epoch: 360, loss: 0.320543\n",
      "epoch: 361, loss: 0.400465\n",
      "epoch: 362, loss: 0.365125\n",
      "epoch: 363, loss: 0.302550\n",
      "epoch: 364, loss: 0.358978\n",
      "epoch: 365, loss: 0.296355\n",
      "epoch: 366, loss: 0.326720\n",
      "epoch: 367, loss: 0.304379\n",
      "epoch: 368, loss: 0.301367\n",
      "epoch: 369, loss: 0.399515\n",
      "epoch: 370, loss: 0.378553\n",
      "epoch: 371, loss: 0.392127\n",
      "epoch: 372, loss: 0.296328\n",
      "epoch: 373, loss: 0.355307\n",
      "epoch: 374, loss: 0.365991\n",
      "epoch: 375, loss: 0.300488\n",
      "epoch: 376, loss: 0.388497\n",
      "epoch: 377, loss: 0.332294\n",
      "epoch: 378, loss: 0.366221\n",
      "epoch: 379, loss: 0.313751\n",
      "epoch: 380, loss: 0.421976\n",
      "epoch: 381, loss: 0.328386\n",
      "epoch: 382, loss: 0.334828\n",
      "epoch: 383, loss: 0.318624\n",
      "epoch: 384, loss: 0.396864\n",
      "epoch: 385, loss: 0.292069\n",
      "epoch: 386, loss: 0.335124\n",
      "epoch: 387, loss: 0.360187\n",
      "epoch: 388, loss: 0.248721\n",
      "epoch: 389, loss: 0.353660\n",
      "epoch: 390, loss: 0.359068\n",
      "epoch: 391, loss: 0.311879\n",
      "epoch: 392, loss: 0.299546\n",
      "epoch: 393, loss: 0.305015\n",
      "epoch: 394, loss: 0.413229\n",
      "epoch: 395, loss: 0.299051\n",
      "epoch: 396, loss: 0.320171\n",
      "epoch: 397, loss: 0.310739\n",
      "epoch: 398, loss: 0.372580\n",
      "epoch: 399, loss: 0.376287\n",
      "epoch: 400, loss: 0.356455\n",
      "epoch: 401, loss: 0.372597\n",
      "epoch: 402, loss: 0.302793\n",
      "epoch: 403, loss: 0.274735\n",
      "epoch: 404, loss: 0.255600\n",
      "epoch: 405, loss: 0.375139\n",
      "epoch: 406, loss: 0.305500\n",
      "epoch: 407, loss: 0.238376\n",
      "epoch: 408, loss: 0.251282\n",
      "epoch: 409, loss: 0.268423\n",
      "epoch: 410, loss: 0.279793\n",
      "epoch: 411, loss: 0.331315\n",
      "epoch: 412, loss: 0.265532\n",
      "epoch: 413, loss: 0.369573\n",
      "epoch: 414, loss: 0.344450\n",
      "epoch: 415, loss: 0.233219\n",
      "epoch: 416, loss: 0.301120\n",
      "epoch: 417, loss: 0.320768\n",
      "epoch: 418, loss: 0.327766\n",
      "epoch: 419, loss: 0.404746\n",
      "epoch: 420, loss: 0.290283\n",
      "epoch: 421, loss: 0.253965\n",
      "epoch: 422, loss: 0.379491\n",
      "epoch: 423, loss: 0.313813\n",
      "epoch: 424, loss: 0.393313\n",
      "epoch: 425, loss: 0.330680\n",
      "epoch: 426, loss: 0.235590\n",
      "epoch: 427, loss: 0.394813\n",
      "epoch: 428, loss: 0.310788\n",
      "epoch: 429, loss: 0.341978\n",
      "epoch: 430, loss: 0.287352\n",
      "epoch: 431, loss: 0.397148\n",
      "epoch: 432, loss: 0.323346\n",
      "epoch: 433, loss: 0.315232\n",
      "epoch: 434, loss: 0.305782\n",
      "epoch: 435, loss: 0.344645\n",
      "epoch: 436, loss: 0.261519\n",
      "epoch: 437, loss: 0.241842\n",
      "epoch: 438, loss: 0.345080\n",
      "epoch: 439, loss: 0.280228\n",
      "epoch: 440, loss: 0.336569\n",
      "epoch: 441, loss: 0.346197\n",
      "epoch: 442, loss: 0.280116\n",
      "epoch: 443, loss: 0.301027\n",
      "epoch: 444, loss: 0.264592\n",
      "epoch: 445, loss: 0.254357\n",
      "epoch: 446, loss: 0.260855\n",
      "epoch: 447, loss: 0.319410\n",
      "epoch: 448, loss: 0.335214\n",
      "epoch: 449, loss: 0.342264\n",
      "epoch: 450, loss: 0.315096\n",
      "epoch: 451, loss: 0.205891\n",
      "epoch: 452, loss: 0.275228\n",
      "epoch: 453, loss: 0.346022\n",
      "epoch: 454, loss: 0.254362\n",
      "epoch: 455, loss: 0.293141\n",
      "epoch: 456, loss: 0.279536\n",
      "epoch: 457, loss: 0.269369\n",
      "epoch: 458, loss: 0.277038\n",
      "epoch: 459, loss: 0.261514\n",
      "epoch: 460, loss: 0.307763\n",
      "epoch: 461, loss: 0.232371\n",
      "epoch: 462, loss: 0.278727\n",
      "epoch: 463, loss: 0.295303\n",
      "epoch: 464, loss: 0.222133\n",
      "epoch: 465, loss: 0.167859\n",
      "epoch: 466, loss: 0.249428\n",
      "epoch: 467, loss: 0.284104\n",
      "epoch: 468, loss: 0.217484\n",
      "epoch: 469, loss: 0.221671\n",
      "epoch: 470, loss: 0.250550\n",
      "epoch: 471, loss: 0.195100\n",
      "epoch: 472, loss: 0.252630\n",
      "epoch: 473, loss: 0.269703\n",
      "epoch: 474, loss: 0.277726\n",
      "epoch: 475, loss: 0.233087\n",
      "epoch: 476, loss: 0.201239\n",
      "epoch: 477, loss: 0.192521\n",
      "epoch: 478, loss: 0.262549\n",
      "epoch: 479, loss: 0.230775\n",
      "epoch: 480, loss: 0.262042\n",
      "epoch: 481, loss: 0.302685\n",
      "epoch: 482, loss: 0.309186\n",
      "epoch: 483, loss: 0.194530\n",
      "epoch: 484, loss: 0.307753\n",
      "epoch: 485, loss: 0.297692\n",
      "epoch: 486, loss: 0.285521\n",
      "epoch: 487, loss: 0.241106\n",
      "epoch: 488, loss: 0.287023\n",
      "epoch: 489, loss: 0.249001\n",
      "epoch: 490, loss: 0.232777\n",
      "epoch: 491, loss: 0.232999\n",
      "epoch: 492, loss: 0.218075\n",
      "epoch: 493, loss: 0.217418\n",
      "epoch: 494, loss: 0.263916\n",
      "epoch: 495, loss: 0.154212\n",
      "epoch: 496, loss: 0.291180\n",
      "epoch: 497, loss: 0.176182\n",
      "epoch: 498, loss: 0.221333\n",
      "epoch: 499, loss: 0.280362\n",
      "epoch: 500, loss: 0.206314\n",
      "epoch: 501, loss: 0.218095\n",
      "epoch: 502, loss: 0.188865\n",
      "epoch: 503, loss: 0.217677\n",
      "epoch: 504, loss: 0.130166\n",
      "epoch: 505, loss: 0.214239\n",
      "epoch: 506, loss: 0.216502\n",
      "epoch: 507, loss: 0.236781\n",
      "epoch: 508, loss: 0.292436\n",
      "epoch: 509, loss: 0.211120\n",
      "epoch: 510, loss: 0.233880\n",
      "epoch: 511, loss: 0.256341\n",
      "epoch: 512, loss: 0.187337\n",
      "epoch: 513, loss: 0.215158\n",
      "epoch: 514, loss: 0.265799\n",
      "epoch: 515, loss: 0.207416\n",
      "epoch: 516, loss: 0.202726\n",
      "epoch: 517, loss: 0.257003\n",
      "epoch: 518, loss: 0.218358\n",
      "epoch: 519, loss: 0.205116\n",
      "epoch: 520, loss: 0.198994\n",
      "epoch: 521, loss: 0.167386\n",
      "epoch: 522, loss: 0.239131\n",
      "epoch: 523, loss: 0.220430\n",
      "epoch: 524, loss: 0.207274\n",
      "epoch: 525, loss: 0.157584\n",
      "epoch: 526, loss: 0.153755\n",
      "epoch: 527, loss: 0.174974\n",
      "epoch: 528, loss: 0.260402\n",
      "epoch: 529, loss: 0.204471\n",
      "epoch: 530, loss: 0.170625\n",
      "epoch: 531, loss: 0.283068\n",
      "epoch: 532, loss: 0.274441\n",
      "epoch: 533, loss: 0.159926\n",
      "epoch: 534, loss: 0.139552\n",
      "epoch: 535, loss: 0.202273\n",
      "epoch: 536, loss: 0.183734\n",
      "epoch: 537, loss: 0.194593\n",
      "epoch: 538, loss: 0.163348\n",
      "epoch: 539, loss: 0.180485\n",
      "epoch: 540, loss: 0.263527\n",
      "epoch: 541, loss: 0.181473\n",
      "epoch: 542, loss: 0.202954\n",
      "epoch: 543, loss: 0.196266\n",
      "epoch: 544, loss: 0.279187\n",
      "epoch: 545, loss: 0.175782\n",
      "epoch: 546, loss: 0.224464\n",
      "epoch: 547, loss: 0.164076\n",
      "epoch: 548, loss: 0.137641\n",
      "epoch: 549, loss: 0.233316\n",
      "epoch: 550, loss: 0.249244\n",
      "epoch: 551, loss: 0.202070\n",
      "epoch: 552, loss: 0.212390\n",
      "epoch: 553, loss: 0.158684\n",
      "epoch: 554, loss: 0.265710\n",
      "epoch: 555, loss: 0.210466\n",
      "epoch: 556, loss: 0.222927\n",
      "epoch: 557, loss: 0.171878\n",
      "epoch: 558, loss: 0.194373\n",
      "epoch: 559, loss: 0.234163\n",
      "epoch: 560, loss: 0.174855\n",
      "epoch: 561, loss: 0.217913\n",
      "epoch: 562, loss: 0.194325\n",
      "epoch: 563, loss: 0.098376\n",
      "epoch: 564, loss: 0.186405\n",
      "epoch: 565, loss: 0.256962\n",
      "epoch: 566, loss: 0.174221\n",
      "epoch: 567, loss: 0.245324\n",
      "epoch: 568, loss: 0.239220\n",
      "epoch: 569, loss: 0.183492\n",
      "epoch: 570, loss: 0.162833\n",
      "epoch: 571, loss: 0.182386\n",
      "epoch: 572, loss: 0.151043\n",
      "epoch: 573, loss: 0.262526\n",
      "epoch: 574, loss: 0.212328\n",
      "epoch: 575, loss: 0.182558\n",
      "epoch: 576, loss: 0.130694\n",
      "epoch: 577, loss: 0.221542\n",
      "epoch: 578, loss: 0.238503\n",
      "epoch: 579, loss: 0.159223\n",
      "epoch: 580, loss: 0.120244\n",
      "epoch: 581, loss: 0.194726\n",
      "epoch: 582, loss: 0.227667\n",
      "epoch: 583, loss: 0.173162\n",
      "epoch: 584, loss: 0.149046\n",
      "epoch: 585, loss: 0.167780\n",
      "epoch: 586, loss: 0.137409\n",
      "epoch: 587, loss: 0.185265\n",
      "epoch: 588, loss: 0.150308\n",
      "epoch: 589, loss: 0.135738\n",
      "epoch: 590, loss: 0.152672\n",
      "epoch: 591, loss: 0.231463\n",
      "epoch: 592, loss: 0.214162\n",
      "epoch: 593, loss: 0.216081\n",
      "epoch: 594, loss: 0.136690\n",
      "epoch: 595, loss: 0.182062\n",
      "epoch: 596, loss: 0.174345\n",
      "epoch: 597, loss: 0.148409\n",
      "epoch: 598, loss: 0.148904\n",
      "epoch: 599, loss: 0.150824\n",
      "epoch: 600, loss: 0.215620\n",
      "epoch: 601, loss: 0.133913\n",
      "epoch: 602, loss: 0.095252\n",
      "epoch: 603, loss: 0.121792\n",
      "epoch: 604, loss: 0.130319\n",
      "epoch: 605, loss: 0.224839\n",
      "epoch: 606, loss: 0.155217\n",
      "epoch: 607, loss: 0.163677\n",
      "epoch: 608, loss: 0.173190\n",
      "epoch: 609, loss: 0.158377\n",
      "epoch: 610, loss: 0.138892\n",
      "epoch: 611, loss: 0.177977\n",
      "epoch: 612, loss: 0.134717\n",
      "epoch: 613, loss: 0.162839\n",
      "epoch: 614, loss: 0.113176\n",
      "epoch: 615, loss: 0.237326\n",
      "epoch: 616, loss: 0.185142\n",
      "epoch: 617, loss: 0.124565\n",
      "epoch: 618, loss: 0.137542\n",
      "epoch: 619, loss: 0.149503\n",
      "epoch: 620, loss: 0.122073\n",
      "epoch: 621, loss: 0.179267\n",
      "epoch: 622, loss: 0.225601\n",
      "epoch: 623, loss: 0.124614\n",
      "epoch: 624, loss: 0.147297\n",
      "epoch: 625, loss: 0.113302\n",
      "epoch: 626, loss: 0.166985\n",
      "epoch: 627, loss: 0.108925\n",
      "epoch: 628, loss: 0.158800\n",
      "epoch: 629, loss: 0.203895\n",
      "epoch: 630, loss: 0.132402\n",
      "epoch: 631, loss: 0.131523\n",
      "epoch: 632, loss: 0.095554\n",
      "epoch: 633, loss: 0.168879\n",
      "epoch: 634, loss: 0.178356\n",
      "epoch: 635, loss: 0.140040\n",
      "epoch: 636, loss: 0.091021\n",
      "epoch: 637, loss: 0.209715\n",
      "epoch: 638, loss: 0.160756\n",
      "epoch: 639, loss: 0.162685\n",
      "epoch: 640, loss: 0.186911\n",
      "epoch: 641, loss: 0.235898\n",
      "epoch: 642, loss: 0.140285\n",
      "epoch: 643, loss: 0.152799\n",
      "epoch: 644, loss: 0.142344\n",
      "epoch: 645, loss: 0.141176\n",
      "epoch: 646, loss: 0.157926\n",
      "epoch: 647, loss: 0.129192\n",
      "epoch: 648, loss: 0.139775\n",
      "epoch: 649, loss: 0.119063\n",
      "epoch: 650, loss: 0.105373\n",
      "epoch: 651, loss: 0.164547\n",
      "epoch: 652, loss: 0.147524\n",
      "epoch: 653, loss: 0.153042\n",
      "epoch: 654, loss: 0.142493\n",
      "epoch: 655, loss: 0.235323\n",
      "epoch: 656, loss: 0.162539\n",
      "epoch: 657, loss: 0.085236\n",
      "epoch: 658, loss: 0.244217\n",
      "epoch: 659, loss: 0.097901\n",
      "epoch: 660, loss: 0.145049\n",
      "epoch: 661, loss: 0.124017\n",
      "epoch: 662, loss: 0.147572\n",
      "epoch: 663, loss: 0.173663\n",
      "epoch: 664, loss: 0.217865\n",
      "epoch: 665, loss: 0.122725\n",
      "epoch: 666, loss: 0.136938\n",
      "epoch: 667, loss: 0.202094\n",
      "epoch: 668, loss: 0.143496\n",
      "epoch: 669, loss: 0.081266\n",
      "epoch: 670, loss: 0.085779\n",
      "epoch: 671, loss: 0.116184\n",
      "epoch: 672, loss: 0.160529\n",
      "epoch: 673, loss: 0.142535\n",
      "epoch: 674, loss: 0.084702\n",
      "epoch: 675, loss: 0.154030\n",
      "epoch: 676, loss: 0.141287\n",
      "epoch: 677, loss: 0.112814\n",
      "epoch: 678, loss: 0.150513\n",
      "epoch: 679, loss: 0.139477\n",
      "epoch: 680, loss: 0.155680\n",
      "epoch: 681, loss: 0.093905\n",
      "epoch: 682, loss: 0.147762\n",
      "epoch: 683, loss: 0.143026\n",
      "epoch: 684, loss: 0.157770\n",
      "epoch: 685, loss: 0.195031\n",
      "epoch: 686, loss: 0.190813\n",
      "epoch: 687, loss: 0.143755\n",
      "epoch: 688, loss: 0.179318\n",
      "epoch: 689, loss: 0.107689\n",
      "epoch: 690, loss: 0.172055\n",
      "epoch: 691, loss: 0.091763\n",
      "epoch: 692, loss: 0.200768\n",
      "epoch: 693, loss: 0.109741\n",
      "epoch: 694, loss: 0.125898\n",
      "epoch: 695, loss: 0.147676\n",
      "epoch: 696, loss: 0.093660\n",
      "epoch: 697, loss: 0.097739\n",
      "epoch: 698, loss: 0.154184\n",
      "epoch: 699, loss: 0.153497\n",
      "epoch: 700, loss: 0.126550\n",
      "epoch: 701, loss: 0.148234\n",
      "epoch: 702, loss: 0.139010\n",
      "epoch: 703, loss: 0.103086\n",
      "epoch: 704, loss: 0.114839\n",
      "epoch: 705, loss: 0.110898\n",
      "epoch: 706, loss: 0.134365\n",
      "epoch: 707, loss: 0.086754\n",
      "epoch: 708, loss: 0.102751\n",
      "epoch: 709, loss: 0.161555\n",
      "epoch: 710, loss: 0.153786\n",
      "epoch: 711, loss: 0.112573\n",
      "epoch: 712, loss: 0.169457\n",
      "epoch: 713, loss: 0.141044\n",
      "epoch: 714, loss: 0.095427\n",
      "epoch: 715, loss: 0.111941\n",
      "epoch: 716, loss: 0.201195\n",
      "epoch: 717, loss: 0.101659\n",
      "epoch: 718, loss: 0.154577\n",
      "epoch: 719, loss: 0.129708\n",
      "epoch: 720, loss: 0.163256\n",
      "epoch: 721, loss: 0.122784\n",
      "epoch: 722, loss: 0.119313\n",
      "epoch: 723, loss: 0.091064\n",
      "epoch: 724, loss: 0.144144\n",
      "epoch: 725, loss: 0.098319\n",
      "epoch: 726, loss: 0.174278\n",
      "epoch: 727, loss: 0.179751\n",
      "epoch: 728, loss: 0.110601\n",
      "epoch: 729, loss: 0.160486\n",
      "epoch: 730, loss: 0.175187\n",
      "epoch: 731, loss: 0.135053\n",
      "epoch: 732, loss: 0.121399\n",
      "epoch: 733, loss: 0.096756\n",
      "epoch: 734, loss: 0.131844\n",
      "epoch: 735, loss: 0.114691\n",
      "epoch: 736, loss: 0.101344\n",
      "epoch: 737, loss: 0.159927\n",
      "epoch: 738, loss: 0.125023\n",
      "epoch: 739, loss: 0.142298\n",
      "epoch: 740, loss: 0.090039\n",
      "epoch: 741, loss: 0.161462\n",
      "epoch: 742, loss: 0.113046\n",
      "epoch: 743, loss: 0.148028\n",
      "epoch: 744, loss: 0.105562\n",
      "epoch: 745, loss: 0.122554\n",
      "epoch: 746, loss: 0.180148\n",
      "epoch: 747, loss: 0.152387\n",
      "epoch: 748, loss: 0.144859\n",
      "epoch: 749, loss: 0.113979\n",
      "epoch: 750, loss: 0.107814\n",
      "epoch: 751, loss: 0.113578\n",
      "epoch: 752, loss: 0.091617\n",
      "epoch: 753, loss: 0.146467\n",
      "epoch: 754, loss: 0.113792\n",
      "epoch: 755, loss: 0.127515\n",
      "epoch: 756, loss: 0.129853\n",
      "epoch: 757, loss: 0.149643\n",
      "epoch: 758, loss: 0.089583\n",
      "epoch: 759, loss: 0.117754\n",
      "epoch: 760, loss: 0.095086\n",
      "epoch: 761, loss: 0.112497\n",
      "epoch: 762, loss: 0.149502\n",
      "epoch: 763, loss: 0.092655\n",
      "epoch: 764, loss: 0.130659\n",
      "epoch: 765, loss: 0.131913\n",
      "epoch: 766, loss: 0.103766\n",
      "epoch: 767, loss: 0.087862\n",
      "epoch: 768, loss: 0.119207\n",
      "epoch: 769, loss: 0.108455\n",
      "epoch: 770, loss: 0.175016\n",
      "epoch: 771, loss: 0.105530\n",
      "epoch: 772, loss: 0.092570\n",
      "epoch: 773, loss: 0.084053\n",
      "epoch: 774, loss: 0.132411\n",
      "epoch: 775, loss: 0.167304\n",
      "epoch: 776, loss: 0.147479\n",
      "epoch: 777, loss: 0.114946\n",
      "epoch: 778, loss: 0.125284\n",
      "epoch: 779, loss: 0.105197\n",
      "epoch: 780, loss: 0.085472\n",
      "epoch: 781, loss: 0.102830\n",
      "epoch: 782, loss: 0.197180\n",
      "epoch: 783, loss: 0.174740\n",
      "epoch: 784, loss: 0.151208\n",
      "epoch: 785, loss: 0.062151\n",
      "epoch: 786, loss: 0.109722\n",
      "epoch: 787, loss: 0.088407\n",
      "epoch: 788, loss: 0.114392\n",
      "epoch: 789, loss: 0.083917\n",
      "epoch: 790, loss: 0.096954\n",
      "epoch: 791, loss: 0.115249\n",
      "epoch: 792, loss: 0.153621\n",
      "epoch: 793, loss: 0.102994\n",
      "epoch: 794, loss: 0.082185\n",
      "epoch: 795, loss: 0.088924\n",
      "epoch: 796, loss: 0.157784\n",
      "epoch: 797, loss: 0.094653\n",
      "epoch: 798, loss: 0.129681\n",
      "epoch: 799, loss: 0.089383\n",
      "epoch: 800, loss: 0.138567\n",
      "epoch: 801, loss: 0.133285\n",
      "epoch: 802, loss: 0.107415\n",
      "epoch: 803, loss: 0.112526\n",
      "epoch: 804, loss: 0.124573\n",
      "epoch: 805, loss: 0.136763\n",
      "epoch: 806, loss: 0.120274\n",
      "epoch: 807, loss: 0.106214\n",
      "epoch: 808, loss: 0.108995\n",
      "epoch: 809, loss: 0.102477\n",
      "epoch: 810, loss: 0.062953\n",
      "epoch: 811, loss: 0.171792\n",
      "epoch: 812, loss: 0.143759\n",
      "epoch: 813, loss: 0.068991\n",
      "epoch: 814, loss: 0.142100\n",
      "epoch: 815, loss: 0.111004\n",
      "epoch: 816, loss: 0.131896\n",
      "epoch: 817, loss: 0.087109\n",
      "epoch: 818, loss: 0.126981\n",
      "epoch: 819, loss: 0.121842\n",
      "epoch: 820, loss: 0.069034\n",
      "epoch: 821, loss: 0.118141\n",
      "epoch: 822, loss: 0.123661\n",
      "epoch: 823, loss: 0.096618\n",
      "epoch: 824, loss: 0.086980\n",
      "epoch: 825, loss: 0.107287\n",
      "epoch: 826, loss: 0.179563\n",
      "epoch: 827, loss: 0.071956\n",
      "epoch: 828, loss: 0.123268\n",
      "epoch: 829, loss: 0.134232\n",
      "epoch: 830, loss: 0.101783\n",
      "epoch: 831, loss: 0.086802\n",
      "epoch: 832, loss: 0.109699\n",
      "epoch: 833, loss: 0.101183\n",
      "epoch: 834, loss: 0.133767\n",
      "epoch: 835, loss: 0.169989\n",
      "epoch: 836, loss: 0.111309\n",
      "epoch: 837, loss: 0.110961\n",
      "epoch: 838, loss: 0.188728\n",
      "epoch: 839, loss: 0.101428\n",
      "epoch: 840, loss: 0.137096\n",
      "epoch: 841, loss: 0.122097\n",
      "epoch: 842, loss: 0.071256\n",
      "epoch: 843, loss: 0.097337\n",
      "epoch: 844, loss: 0.108578\n",
      "epoch: 845, loss: 0.114219\n",
      "epoch: 846, loss: 0.111447\n",
      "epoch: 847, loss: 0.170622\n",
      "epoch: 848, loss: 0.117181\n",
      "epoch: 849, loss: 0.095245\n",
      "epoch: 850, loss: 0.136383\n",
      "epoch: 851, loss: 0.081620\n",
      "epoch: 852, loss: 0.099026\n",
      "epoch: 853, loss: 0.134475\n",
      "epoch: 854, loss: 0.067240\n",
      "epoch: 855, loss: 0.095697\n",
      "epoch: 856, loss: 0.134498\n",
      "epoch: 857, loss: 0.087383\n",
      "epoch: 858, loss: 0.116676\n",
      "epoch: 859, loss: 0.099121\n",
      "epoch: 860, loss: 0.092980\n",
      "epoch: 861, loss: 0.082263\n",
      "epoch: 862, loss: 0.100855\n",
      "epoch: 863, loss: 0.112373\n",
      "epoch: 864, loss: 0.199220\n",
      "epoch: 865, loss: 0.103644\n",
      "epoch: 866, loss: 0.125399\n",
      "epoch: 867, loss: 0.118417\n",
      "epoch: 868, loss: 0.143425\n",
      "epoch: 869, loss: 0.154679\n",
      "epoch: 870, loss: 0.093403\n",
      "epoch: 871, loss: 0.129168\n",
      "epoch: 872, loss: 0.132251\n",
      "epoch: 873, loss: 0.115740\n",
      "epoch: 874, loss: 0.143434\n",
      "epoch: 875, loss: 0.106988\n",
      "epoch: 876, loss: 0.107393\n",
      "epoch: 877, loss: 0.065890\n",
      "epoch: 878, loss: 0.090112\n",
      "epoch: 879, loss: 0.094458\n",
      "epoch: 880, loss: 0.074165\n",
      "epoch: 881, loss: 0.053907\n",
      "epoch: 882, loss: 0.097876\n",
      "epoch: 883, loss: 0.151028\n",
      "epoch: 884, loss: 0.107281\n",
      "epoch: 885, loss: 0.169065\n",
      "epoch: 886, loss: 0.085986\n",
      "epoch: 887, loss: 0.134881\n",
      "epoch: 888, loss: 0.084504\n",
      "epoch: 889, loss: 0.094503\n",
      "epoch: 890, loss: 0.129430\n",
      "epoch: 891, loss: 0.108511\n",
      "epoch: 892, loss: 0.081229\n",
      "epoch: 893, loss: 0.094963\n",
      "epoch: 894, loss: 0.114493\n",
      "epoch: 895, loss: 0.161148\n",
      "epoch: 896, loss: 0.087865\n",
      "epoch: 897, loss: 0.108357\n",
      "epoch: 898, loss: 0.087700\n",
      "epoch: 899, loss: 0.138016\n",
      "epoch: 900, loss: 0.141345\n",
      "epoch: 901, loss: 0.091549\n",
      "epoch: 902, loss: 0.097611\n",
      "epoch: 903, loss: 0.119320\n",
      "epoch: 904, loss: 0.089320\n",
      "epoch: 905, loss: 0.129821\n",
      "epoch: 906, loss: 0.087640\n",
      "epoch: 907, loss: 0.107851\n",
      "epoch: 908, loss: 0.150950\n",
      "epoch: 909, loss: 0.087407\n",
      "epoch: 910, loss: 0.091933\n",
      "epoch: 911, loss: 0.146448\n",
      "epoch: 912, loss: 0.089737\n",
      "epoch: 913, loss: 0.099985\n",
      "epoch: 914, loss: 0.146755\n",
      "epoch: 915, loss: 0.080524\n",
      "epoch: 916, loss: 0.060169\n",
      "epoch: 917, loss: 0.119330\n",
      "epoch: 918, loss: 0.067618\n",
      "epoch: 919, loss: 0.110340\n",
      "epoch: 920, loss: 0.091761\n",
      "epoch: 921, loss: 0.104341\n",
      "epoch: 922, loss: 0.087860\n",
      "epoch: 923, loss: 0.071734\n",
      "epoch: 924, loss: 0.118375\n",
      "epoch: 925, loss: 0.112809\n",
      "epoch: 926, loss: 0.133698\n",
      "epoch: 927, loss: 0.103004\n",
      "epoch: 928, loss: 0.159674\n",
      "epoch: 929, loss: 0.132490\n",
      "epoch: 930, loss: 0.083844\n",
      "epoch: 931, loss: 0.082802\n",
      "epoch: 932, loss: 0.106334\n",
      "epoch: 933, loss: 0.109863\n",
      "epoch: 934, loss: 0.070360\n",
      "epoch: 935, loss: 0.093943\n",
      "epoch: 936, loss: 0.125101\n",
      "epoch: 937, loss: 0.141095\n",
      "epoch: 938, loss: 0.071554\n",
      "epoch: 939, loss: 0.086457\n",
      "epoch: 940, loss: 0.077521\n",
      "epoch: 941, loss: 0.112076\n",
      "epoch: 942, loss: 0.063355\n",
      "epoch: 943, loss: 0.145034\n",
      "epoch: 944, loss: 0.074148\n",
      "epoch: 945, loss: 0.102424\n",
      "epoch: 946, loss: 0.126187\n",
      "epoch: 947, loss: 0.071948\n",
      "epoch: 948, loss: 0.085627\n",
      "epoch: 949, loss: 0.148433\n",
      "epoch: 950, loss: 0.081243\n",
      "epoch: 951, loss: 0.115616\n",
      "epoch: 952, loss: 0.091331\n",
      "epoch: 953, loss: 0.078449\n",
      "epoch: 954, loss: 0.090502\n",
      "epoch: 955, loss: 0.091145\n",
      "epoch: 956, loss: 0.101773\n",
      "epoch: 957, loss: 0.086814\n",
      "epoch: 958, loss: 0.100415\n",
      "epoch: 959, loss: 0.104191\n",
      "epoch: 960, loss: 0.097961\n",
      "epoch: 961, loss: 0.171135\n",
      "epoch: 962, loss: 0.098484\n",
      "epoch: 963, loss: 0.066456\n",
      "epoch: 964, loss: 0.048613\n",
      "epoch: 965, loss: 0.053822\n",
      "epoch: 966, loss: 0.117570\n",
      "epoch: 967, loss: 0.087611\n",
      "epoch: 968, loss: 0.125537\n",
      "epoch: 969, loss: 0.092988\n",
      "epoch: 970, loss: 0.147062\n",
      "epoch: 971, loss: 0.188135\n",
      "epoch: 972, loss: 0.088036\n",
      "epoch: 973, loss: 0.114348\n",
      "epoch: 974, loss: 0.094800\n",
      "epoch: 975, loss: 0.046776\n",
      "epoch: 976, loss: 0.134524\n",
      "epoch: 977, loss: 0.079414\n",
      "epoch: 978, loss: 0.092447\n",
      "epoch: 979, loss: 0.083775\n",
      "epoch: 980, loss: 0.125210\n",
      "epoch: 981, loss: 0.096408\n",
      "epoch: 982, loss: 0.085408\n",
      "epoch: 983, loss: 0.105242\n",
      "epoch: 984, loss: 0.121188\n",
      "epoch: 985, loss: 0.087477\n",
      "epoch: 986, loss: 0.083446\n",
      "epoch: 987, loss: 0.118846\n",
      "epoch: 988, loss: 0.093235\n",
      "epoch: 989, loss: 0.138599\n",
      "epoch: 990, loss: 0.086525\n",
      "epoch: 991, loss: 0.114856\n",
      "epoch: 992, loss: 0.092557\n",
      "epoch: 993, loss: 0.094959\n",
      "epoch: 994, loss: 0.092180\n",
      "epoch: 995, loss: 0.146634\n",
      "epoch: 996, loss: 0.135472\n",
      "epoch: 997, loss: 0.057948\n",
      "epoch: 998, loss: 0.090851\n",
      "epoch: 999, loss: 0.086601\n",
      "epoch: 1000, loss: 0.081633\n"
     ]
    }
   ],
   "source": [
    "# X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "# Y = np.array([0,0,0,1])\n",
    "model = DeepNeuralNetwork(\n",
    "    input_size=4,\n",
    "    output_size=1,\n",
    "    hidden_size=16,\n",
    "    epoches=1000,\n",
    "    learning_rate=0.001,\n",
    "    dropout=0.8,\n",
    "    hidden_layers=4\n",
    ")\n",
    "\n",
    "model.fit(X_train,Y_train)\n",
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb3dda5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       195\n",
      "           1       0.97      0.96      0.96       148\n",
      "\n",
      "    accuracy                           0.97       343\n",
      "   macro avg       0.97      0.97      0.97       343\n",
      "weighted avg       0.97      0.97      0.97       343\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "cmf = confusion_matrix(Y_test,preds)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(cmf,annot=True,fmt='d')\n",
    "plt.xlabel(\"True\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.show()\n",
    "print(classification_report(Y_test,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cff45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 2 1 3]\n"
     ]
    }
   ],
   "source": [
    "print(np.random.randint())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
