{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63bf5364",
   "metadata": {},
   "source": [
    "# implemention of a deep neural network with its Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24fd8d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "127a66b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNeuralNet:\n",
    "\n",
    "    def __init__(self,input_size,hidden_size,output_size,Lambda=0.05,learning_rate=0.001,epoches=1000,hidden_layer=1):\n",
    "\n",
    "        \"\"\"initialization of parameters \"\"\"\n",
    "        self.input_size = input_size # number of parameters \n",
    "        self.hidden_size = hidden_size # number of neurons in hidden layers \n",
    "        self.output_size = output_size # size of output \n",
    "        self.Lambda = Lambda # for regularization \n",
    "        self.learning_rate = learning_rate # learning rate \n",
    "        self.epoches = epoches # number of iterations \n",
    "        self.hidden_layer = hidden_layer # number of hidden layer \n",
    "        \n",
    "        \"\"\"initialization of hidden layers\"\"\"\n",
    "\n",
    "        self.HW = [np.random.randn(self.input_size,self.hidden_size)]\n",
    "        self.HB = [np.zeros((1,self.hidden_size))] \n",
    "\n",
    "        for layers in range(self.hidden_layer): # iterativly add new hidden layer parameters\n",
    "            self.HW.append(np.random.randn(self.hidden_size,self.hidden_size))\n",
    "            self.HB.append(np.zeros((1,self.hidden_size)))\n",
    "        \n",
    "        \"\"\"initialization of Output layer\"\"\"\n",
    "\n",
    "        self.OW = np.random.randn(self.hidden_size,self.output_size)\n",
    "        self.OB = np.zeros((1,self.output_size))\n",
    "\n",
    "\n",
    "    def sigmoid(self,z):\n",
    "        return 1 / (1+np.exp(-z)) \n",
    "    \n",
    "    def derivative_sigmoid(self,z):\n",
    "        s = self.sigmoid(z) \n",
    "        return s * (1-s)\n",
    "    \n",
    "    def Relu(self,z):\n",
    "        return np.maximum(0,z)\n",
    "    \n",
    "    def derivative_relu(self,z):\n",
    "        return (z>0).astype(float)\n",
    "    \n",
    "    def compute_loss(self,preds,Y):\n",
    "        preds = np.clip(preds, 1e-8, 1 - 1e-8)\n",
    "        return -np.mean(Y*np.log(preds)+(1-Y)*np.log(1-preds))\n",
    "    \n",
    "    def ForwardPropagation(self,X):\n",
    "        A = X \n",
    "        self.HZ = []\n",
    "        self.HA = [A]\n",
    "\n",
    "        \"\"\"moving deep inside neural networks,calculation from 1 to L-1 layers \"\"\"\n",
    "        for l in range(self.hidden_layer):\n",
    "            Z = np.dot(A,self.HW[l]) + self.HB[l] \n",
    "            self.HZ.append(Z)\n",
    "            A = self.Relu(Z)\n",
    "            self.HA.append(A)\n",
    "\n",
    "        \"\"\"Calculation for Output layer \"\"\" \n",
    "        self.OZ = np.dot(self.HA[-1],self.OW) + self.OB \n",
    "        self.OA = self.sigmoid(self.OZ)\n",
    "\n",
    "        return self.OA \n",
    "    \n",
    "    def BackwardPropagation(self,X,Y):\n",
    "        m = X.shape[0]\n",
    "        error = self.OA - Y \n",
    "\n",
    "        \"\"\"Moving Backward from output layer\"\"\"\n",
    "        DOW = 1/m * np.dot(self.HA[-1].T,error) + self.Lambda * self.OW\n",
    "        DOB = 1/m * np.sum(error,axis=0,keepdims=True)\n",
    "\n",
    "        self.OW = self.OW - self.learning_rate * DOW \n",
    "        self.OB = self.OB - self.learning_rate * DOB \n",
    "\n",
    "        \"\"\"moving backward in hidden layers\"\"\"\n",
    "        for l in reversed(range(self.hidden_layer)):\n",
    "            DA = np.dot(error,self.OW.T if l == self.hidden_layer-1 else self.HW[l+1].T)\n",
    "            DZ = DA * self.derivative_relu(self.HZ[l]) \n",
    "            DW = 1/m * np.dot(self.HA[l].T,DZ) + self.Lambda * self.HW[l]\n",
    "            DB = 1/m * np.sum(DZ,axis=0,keepdims=True)\n",
    "\n",
    "            error = DZ \n",
    "            self.HW[l] = self.HW[l] - self.learning_rate * DW \n",
    "            self.HB[l] = self.HB[l] - self.learning_rate * DB \n",
    "\n",
    "\n",
    "    def fit(self,X,Y):\n",
    "        Y = np.array(Y).reshape(-1,1)\n",
    "\n",
    "        for epoch in range(self.epoches):\n",
    "            pred = self.ForwardPropagation(X)\n",
    "            loss = self.compute_loss(pred,Y)\n",
    "            print(f\"epoch : {epoch}, loss = {loss}\") \n",
    "            self.BackwardPropagation(X,Y) \n",
    "            \n",
    "    def predict(self,X):\n",
    "        return (self.ForwardPropagation(X) > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "216d4719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.62160</td>\n",
       "      <td>8.6661</td>\n",
       "      <td>-2.8073</td>\n",
       "      <td>-0.44699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.1674</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.6383</td>\n",
       "      <td>1.9242</td>\n",
       "      <td>0.10645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.5228</td>\n",
       "      <td>-4.0112</td>\n",
       "      <td>-3.59440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.4552</td>\n",
       "      <td>4.5718</td>\n",
       "      <td>-0.98880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variance  skewness  curtosis  entropy  class\n",
       "0   3.62160    8.6661   -2.8073 -0.44699      0\n",
       "1   4.54590    8.1674   -2.4586 -1.46210      0\n",
       "2   3.86600   -2.6383    1.9242  0.10645      0\n",
       "3   3.45660    9.5228   -4.0112 -3.59440      0\n",
       "4   0.32924   -4.4552    4.5718 -0.98880      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "dataset = pd.read_csv(\"../BankNote_Authentication.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa5185ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = dataset['class']\n",
    "X = dataset.drop(columns=['class'])\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80882e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Temp\\ipykernel_11112\\21932106.py:30: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1+np.exp(-z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0, loss = 8.977503919228614\n",
      "epoch : 1, loss = 9.62922795844302\n",
      "epoch : 2, loss = 8.68936079389877\n",
      "epoch : 3, loss = 8.113153945863093\n",
      "epoch : 4, loss = 6.7578193524932475\n",
      "epoch : 5, loss = 6.0637761540218476\n",
      "epoch : 6, loss = 5.460168171848019\n",
      "epoch : 7, loss = 5.346983767716589\n",
      "epoch : 8, loss = 5.135448055201728\n",
      "epoch : 9, loss = 5.148146776903617\n",
      "epoch : 10, loss = 4.934975044133387\n",
      "epoch : 11, loss = 4.7836710688083794\n",
      "epoch : 12, loss = 4.496064583569796\n",
      "epoch : 13, loss = 4.1753226747529135\n",
      "epoch : 14, loss = 3.8109560429074105\n",
      "epoch : 15, loss = 3.3947377913464414\n",
      "epoch : 16, loss = 3.100379307132725\n",
      "epoch : 17, loss = 2.8167121665845\n",
      "epoch : 18, loss = 2.5565963408969594\n",
      "epoch : 19, loss = 2.3842425802508167\n",
      "epoch : 20, loss = 2.197385721738968\n",
      "epoch : 21, loss = 2.058139921943681\n",
      "epoch : 22, loss = 1.916981611579445\n",
      "epoch : 23, loss = 1.8513540120025478\n",
      "epoch : 24, loss = 1.7567952174206787\n",
      "epoch : 25, loss = 1.6678724711628194\n",
      "epoch : 26, loss = 1.5687027479677667\n",
      "epoch : 27, loss = 1.4614086875143748\n",
      "epoch : 28, loss = 1.3669892197492277\n",
      "epoch : 29, loss = 1.2851778562083584\n",
      "epoch : 30, loss = 1.2105073994547517\n",
      "epoch : 31, loss = 1.1465856106734669\n",
      "epoch : 32, loss = 1.0857154075285191\n",
      "epoch : 33, loss = 1.0300179266895606\n",
      "epoch : 34, loss = 0.9757181012218167\n",
      "epoch : 35, loss = 0.9240009480179225\n",
      "epoch : 36, loss = 0.8759233669804208\n",
      "epoch : 37, loss = 0.8345442865952196\n",
      "epoch : 38, loss = 0.7987239941798816\n",
      "epoch : 39, loss = 0.7652716704565307\n",
      "epoch : 40, loss = 0.7308121688353528\n",
      "epoch : 41, loss = 0.696335043117005\n",
      "epoch : 42, loss = 0.6638396774217855\n",
      "epoch : 43, loss = 0.6312113642499266\n",
      "epoch : 44, loss = 0.6001813222846438\n",
      "epoch : 45, loss = 0.5723506020829723\n",
      "epoch : 46, loss = 0.5441913547234944\n",
      "epoch : 47, loss = 0.5171284686853599\n",
      "epoch : 48, loss = 0.4921636158322736\n",
      "epoch : 49, loss = 0.47043006321267317\n",
      "epoch : 50, loss = 0.4516264942420211\n",
      "epoch : 51, loss = 0.4355449173705547\n",
      "epoch : 52, loss = 0.42124814900081214\n",
      "epoch : 53, loss = 0.4083552141755945\n",
      "epoch : 54, loss = 0.3968688735422162\n",
      "epoch : 55, loss = 0.3866194066030501\n",
      "epoch : 56, loss = 0.3770415868015381\n",
      "epoch : 57, loss = 0.36842575971655334\n",
      "epoch : 58, loss = 0.36030922283514644\n",
      "epoch : 59, loss = 0.3527818674937328\n",
      "epoch : 60, loss = 0.3453636278467519\n",
      "epoch : 61, loss = 0.3384993276767083\n",
      "epoch : 62, loss = 0.33153458830280214\n",
      "epoch : 63, loss = 0.3252307229484223\n",
      "epoch : 64, loss = 0.3186070581761554\n",
      "epoch : 65, loss = 0.312949676001422\n",
      "epoch : 66, loss = 0.30661359547245776\n",
      "epoch : 67, loss = 0.3019798079061394\n",
      "epoch : 68, loss = 0.29590671693094567\n",
      "epoch : 69, loss = 0.29242729401015716\n",
      "epoch : 70, loss = 0.2848804363457048\n",
      "epoch : 71, loss = 0.28401542975580263\n",
      "epoch : 72, loss = 0.2743992980588506\n",
      "epoch : 73, loss = 0.27469978529987016\n",
      "epoch : 74, loss = 0.26416496596330413\n",
      "epoch : 75, loss = 0.2641637513544801\n",
      "epoch : 76, loss = 0.2544878528893576\n",
      "epoch : 77, loss = 0.25376958994413384\n",
      "epoch : 78, loss = 0.24532655597346\n",
      "epoch : 79, loss = 0.24397625253918734\n",
      "epoch : 80, loss = 0.23673850424298618\n",
      "epoch : 81, loss = 0.2349396711483887\n",
      "epoch : 82, loss = 0.2287957878841224\n",
      "epoch : 83, loss = 0.22665633129866564\n",
      "epoch : 84, loss = 0.22149260590029743\n",
      "epoch : 85, loss = 0.2191486232241562\n",
      "epoch : 86, loss = 0.21483950087564843\n",
      "epoch : 87, loss = 0.21239390942334985\n",
      "epoch : 88, loss = 0.20872986262499568\n",
      "epoch : 89, loss = 0.20625799622190216\n",
      "epoch : 90, loss = 0.20312690181453188\n",
      "epoch : 91, loss = 0.20072962912344108\n",
      "epoch : 92, loss = 0.19802006159452398\n",
      "epoch : 93, loss = 0.19574201389355347\n",
      "epoch : 94, loss = 0.19335936731502482\n",
      "epoch : 95, loss = 0.19128686707224446\n",
      "epoch : 96, loss = 0.18920423601256978\n",
      "epoch : 97, loss = 0.18728733760182625\n",
      "epoch : 98, loss = 0.18540572783532305\n",
      "epoch : 99, loss = 0.18363169547983024\n",
      "epoch : 100, loss = 0.18191013484838314\n",
      "epoch : 101, loss = 0.1802474752926901\n",
      "epoch : 102, loss = 0.1786794845319426\n",
      "epoch : 103, loss = 0.17715355169417604\n",
      "epoch : 104, loss = 0.17567896761766222\n",
      "epoch : 105, loss = 0.1742580269509379\n",
      "epoch : 106, loss = 0.17277812858499234\n",
      "epoch : 107, loss = 0.1713857359014735\n",
      "epoch : 108, loss = 0.16989206979831709\n",
      "epoch : 109, loss = 0.16838526019906708\n",
      "epoch : 110, loss = 0.1669219264243829\n",
      "epoch : 111, loss = 0.16549618212378286\n",
      "epoch : 112, loss = 0.16409501009126232\n",
      "epoch : 113, loss = 0.16273007899168931\n",
      "epoch : 114, loss = 0.16139377498996801\n",
      "epoch : 115, loss = 0.1601075207714046\n",
      "epoch : 116, loss = 0.15882720660728272\n",
      "epoch : 117, loss = 0.15756843142297505\n",
      "epoch : 118, loss = 0.15633916025198152\n",
      "epoch : 119, loss = 0.15514178890688876\n",
      "epoch : 120, loss = 0.15395852581160385\n",
      "epoch : 121, loss = 0.15280557103374467\n",
      "epoch : 122, loss = 0.15167652691511108\n",
      "epoch : 123, loss = 0.15056850481767098\n",
      "epoch : 124, loss = 0.14948559633689315\n",
      "epoch : 125, loss = 0.1484207684729553\n",
      "epoch : 126, loss = 0.14737714152982834\n",
      "epoch : 127, loss = 0.14636021020074105\n",
      "epoch : 128, loss = 0.1453536246759342\n",
      "epoch : 129, loss = 0.14436891165647217\n",
      "epoch : 130, loss = 0.14339191332479032\n",
      "epoch : 131, loss = 0.14243466374451738\n",
      "epoch : 132, loss = 0.1415000172291411\n",
      "epoch : 133, loss = 0.14057788708208113\n",
      "epoch : 134, loss = 0.13967622839493088\n",
      "epoch : 135, loss = 0.13873294084253815\n",
      "epoch : 136, loss = 0.13780488432605223\n",
      "epoch : 137, loss = 0.13692374211473718\n",
      "epoch : 138, loss = 0.1360564251017189\n",
      "epoch : 139, loss = 0.13521052414515433\n",
      "epoch : 140, loss = 0.13437822665310353\n",
      "epoch : 141, loss = 0.13356444000231452\n",
      "epoch : 142, loss = 0.1327662627544785\n",
      "epoch : 143, loss = 0.1319836528189717\n",
      "epoch : 144, loss = 0.1312182573460393\n",
      "epoch : 145, loss = 0.1304657503253994\n",
      "epoch : 146, loss = 0.12973300180813832\n",
      "epoch : 147, loss = 0.12901482410913373\n",
      "epoch : 148, loss = 0.12831709136604472\n",
      "epoch : 149, loss = 0.12762714974731154\n",
      "epoch : 150, loss = 0.1269566130666695\n",
      "epoch : 151, loss = 0.12629199281199804\n",
      "epoch : 152, loss = 0.12564702211650514\n",
      "epoch : 153, loss = 0.12500652938083734\n",
      "epoch : 154, loss = 0.12438702329893804\n",
      "epoch : 155, loss = 0.12376559013097804\n",
      "epoch : 156, loss = 0.12316866520820491\n",
      "epoch : 157, loss = 0.12256836155779345\n",
      "epoch : 158, loss = 0.12199119920094607\n",
      "epoch : 159, loss = 0.12141049178454258\n",
      "epoch : 160, loss = 0.1208516235316698\n",
      "epoch : 161, loss = 0.12028882977731926\n",
      "epoch : 162, loss = 0.11973713375873513\n",
      "epoch : 163, loss = 0.11915398082299375\n",
      "epoch : 164, loss = 0.1185679155515924\n",
      "epoch : 165, loss = 0.11796588567161798\n",
      "epoch : 166, loss = 0.11738499967897373\n",
      "epoch : 167, loss = 0.11682012659904337\n",
      "epoch : 168, loss = 0.11625861845052295\n",
      "epoch : 169, loss = 0.11570621337103965\n",
      "epoch : 170, loss = 0.11516999173545017\n",
      "epoch : 171, loss = 0.11463753687728255\n",
      "epoch : 172, loss = 0.11411059397866688\n",
      "epoch : 173, loss = 0.11360052184718247\n",
      "epoch : 174, loss = 0.11308690340036304\n",
      "epoch : 175, loss = 0.11258405844813164\n",
      "epoch : 176, loss = 0.11209079506095693\n",
      "epoch : 177, loss = 0.11159558861682446\n",
      "epoch : 178, loss = 0.11111690509231974\n",
      "epoch : 179, loss = 0.11063722905679335\n",
      "epoch : 180, loss = 0.11016453632168294\n",
      "epoch : 181, loss = 0.10968641342180108\n",
      "epoch : 182, loss = 0.10920364455764868\n",
      "epoch : 183, loss = 0.10873268848633547\n",
      "epoch : 184, loss = 0.10825654344187775\n",
      "epoch : 185, loss = 0.107797815494234\n",
      "epoch : 186, loss = 0.10733059308485646\n",
      "epoch : 187, loss = 0.10687733974367715\n",
      "epoch : 188, loss = 0.10642390327121526\n",
      "epoch : 189, loss = 0.10597692546691258\n",
      "epoch : 190, loss = 0.10553356115828409\n",
      "epoch : 191, loss = 0.1050933787449043\n",
      "epoch : 192, loss = 0.1046596793592997\n",
      "epoch : 193, loss = 0.10422539892280981\n",
      "epoch : 194, loss = 0.1038000982480582\n",
      "epoch : 195, loss = 0.1033729896537198\n",
      "epoch : 196, loss = 0.10295700384416136\n",
      "epoch : 197, loss = 0.10253753392905882\n",
      "epoch : 198, loss = 0.10212751792578117\n",
      "epoch : 199, loss = 0.10169822983709546\n",
      "epoch : 200, loss = 0.10127853001193678\n",
      "epoch : 201, loss = 0.10085361995383893\n",
      "epoch : 202, loss = 0.10044467814567261\n",
      "epoch : 203, loss = 0.10002972761994369\n",
      "epoch : 204, loss = 0.09963190539675815\n",
      "epoch : 205, loss = 0.09922749998172802\n",
      "epoch : 206, loss = 0.09884132951947414\n",
      "epoch : 207, loss = 0.09844795682636297\n",
      "epoch : 208, loss = 0.0980738819311691\n",
      "epoch : 209, loss = 0.09769186542121643\n",
      "epoch : 210, loss = 0.09732781849400905\n",
      "epoch : 211, loss = 0.09696629005006269\n",
      "epoch : 212, loss = 0.09663141851893185\n",
      "epoch : 213, loss = 0.09630446369758099\n",
      "epoch : 214, loss = 0.09599051387658404\n",
      "epoch : 215, loss = 0.09567489862561915\n",
      "epoch : 216, loss = 0.09537473978982258\n",
      "epoch : 217, loss = 0.09506714258015743\n",
      "epoch : 218, loss = 0.09477853842442698\n",
      "epoch : 219, loss = 0.09448147500471828\n",
      "epoch : 220, loss = 0.09420278825840048\n",
      "epoch : 221, loss = 0.09391535672514459\n",
      "epoch : 222, loss = 0.09364596164676206\n",
      "epoch : 223, loss = 0.09336724408543035\n",
      "epoch : 224, loss = 0.09310656208837385\n",
      "epoch : 225, loss = 0.0928358515647067\n",
      "epoch : 226, loss = 0.0925807136655112\n",
      "epoch : 227, loss = 0.09232107403772122\n",
      "epoch : 228, loss = 0.09207166378721676\n",
      "epoch : 229, loss = 0.091819664767697\n",
      "epoch : 230, loss = 0.09157622597537214\n",
      "epoch : 231, loss = 0.09133182692997852\n",
      "epoch : 232, loss = 0.09109422054839994\n",
      "epoch : 233, loss = 0.09085731004700899\n",
      "epoch : 234, loss = 0.09062480171442644\n",
      "epoch : 235, loss = 0.09039459792602653\n",
      "epoch : 236, loss = 0.09016631629507622\n",
      "epoch : 237, loss = 0.08994259451957975\n",
      "epoch : 238, loss = 0.0897180659872971\n",
      "epoch : 239, loss = 0.08950064075213443\n",
      "epoch : 240, loss = 0.08927943297955065\n",
      "epoch : 241, loss = 0.08906814364767635\n",
      "epoch : 242, loss = 0.0888498606118972\n",
      "epoch : 243, loss = 0.0886445657207058\n",
      "epoch : 244, loss = 0.08842941249525307\n",
      "epoch : 245, loss = 0.08822665330900914\n",
      "epoch : 246, loss = 0.08801919031396492\n",
      "epoch : 247, loss = 0.0878177631758285\n",
      "epoch : 248, loss = 0.08761596456866089\n",
      "epoch : 249, loss = 0.08741627352672494\n",
      "epoch : 250, loss = 0.0872201028259133\n",
      "epoch : 251, loss = 0.08702188324577594\n",
      "epoch : 252, loss = 0.08683125068694661\n",
      "epoch : 253, loss = 0.08663425449122476\n",
      "epoch : 254, loss = 0.08644907658300698\n",
      "epoch : 255, loss = 0.08625406859377413\n",
      "epoch : 256, loss = 0.0860701018989931\n",
      "epoch : 257, loss = 0.08588209103116635\n",
      "epoch : 258, loss = 0.0856982094718788\n",
      "epoch : 259, loss = 0.08551540910770158\n",
      "epoch : 260, loss = 0.08533200394362095\n",
      "epoch : 261, loss = 0.08515444462768977\n",
      "epoch : 262, loss = 0.08497149293552676\n",
      "epoch : 263, loss = 0.0847991672078738\n",
      "epoch : 264, loss = 0.08461730728707356\n",
      "epoch : 265, loss = 0.0844460104294508\n",
      "epoch : 266, loss = 0.08427047469552357\n",
      "epoch : 267, loss = 0.0840986568936836\n",
      "epoch : 268, loss = 0.08392806761015405\n",
      "epoch : 269, loss = 0.08375610259503855\n",
      "epoch : 270, loss = 0.08359048274025964\n",
      "epoch : 271, loss = 0.08341832759072092\n",
      "epoch : 272, loss = 0.08325786956471468\n",
      "epoch : 273, loss = 0.0830875970430845\n",
      "epoch : 274, loss = 0.08294296695306805\n",
      "epoch : 275, loss = 0.08277638360611227\n",
      "epoch : 276, loss = 0.08261928899569726\n",
      "epoch : 277, loss = 0.08246139757646792\n",
      "epoch : 278, loss = 0.08230957271181456\n",
      "epoch : 279, loss = 0.08215437032407144\n",
      "epoch : 280, loss = 0.0820065124207738\n",
      "epoch : 281, loss = 0.08185354100271011\n",
      "epoch : 282, loss = 0.08170943033174195\n",
      "epoch : 283, loss = 0.0815590846099802\n",
      "epoch : 284, loss = 0.08141984176688453\n",
      "epoch : 285, loss = 0.08127020733792029\n",
      "epoch : 286, loss = 0.08113328770693519\n",
      "epoch : 287, loss = 0.08098724882274955\n",
      "epoch : 288, loss = 0.08085290604311486\n",
      "epoch : 289, loss = 0.08070110618167177\n",
      "epoch : 290, loss = 0.08056284265320476\n",
      "epoch : 291, loss = 0.08042034487551192\n",
      "epoch : 292, loss = 0.08028549886725343\n",
      "epoch : 293, loss = 0.08014731035082723\n",
      "epoch : 294, loss = 0.08001461171973484\n",
      "epoch : 295, loss = 0.0798803893084065\n",
      "epoch : 296, loss = 0.07974960401991177\n",
      "epoch : 297, loss = 0.07961928003896901\n",
      "epoch : 298, loss = 0.07949025114288127\n",
      "epoch : 299, loss = 0.07936377833746862\n",
      "epoch : 300, loss = 0.07923634400236831\n",
      "epoch : 301, loss = 0.07911369249295719\n",
      "epoch : 302, loss = 0.07898770334154603\n",
      "epoch : 303, loss = 0.07886885646707009\n",
      "epoch : 304, loss = 0.07874448854837414\n",
      "epoch : 305, loss = 0.07863105650782418\n",
      "epoch : 306, loss = 0.07850775986125283\n",
      "epoch : 307, loss = 0.07839608754638128\n",
      "epoch : 308, loss = 0.07827667290940345\n",
      "epoch : 309, loss = 0.07816593014638809\n",
      "epoch : 310, loss = 0.07805033268915679\n",
      "epoch : 311, loss = 0.07794035583804776\n",
      "epoch : 312, loss = 0.07782858848268029\n",
      "epoch : 313, loss = 0.07771923387471144\n",
      "epoch : 314, loss = 0.07761141339006644\n",
      "epoch : 315, loss = 0.07750282268137625\n",
      "epoch : 316, loss = 0.07739902753251367\n",
      "epoch : 317, loss = 0.07729082338277249\n",
      "epoch : 318, loss = 0.07719084399011879\n",
      "epoch : 319, loss = 0.077084553936378\n",
      "epoch : 320, loss = 0.07698723179925962\n",
      "epoch : 321, loss = 0.07688240647795112\n",
      "epoch : 322, loss = 0.07678522944203817\n",
      "epoch : 323, loss = 0.0766843688499081\n",
      "epoch : 324, loss = 0.07658706406572915\n",
      "epoch : 325, loss = 0.07649016737124192\n",
      "epoch : 326, loss = 0.07639259051508586\n",
      "epoch : 327, loss = 0.07629972869160603\n",
      "epoch : 328, loss = 0.07620225369066834\n",
      "epoch : 329, loss = 0.0761145343136166\n",
      "epoch : 330, loss = 0.07601722545856306\n",
      "epoch : 331, loss = 0.07592893265201014\n",
      "epoch : 332, loss = 0.07583581504902114\n",
      "epoch : 333, loss = 0.07574672891647177\n",
      "epoch : 334, loss = 0.07565782752344748\n",
      "epoch : 335, loss = 0.07556779243161041\n",
      "epoch : 336, loss = 0.07548315340619596\n",
      "epoch : 337, loss = 0.07539350955385787\n",
      "epoch : 338, loss = 0.07531248768646079\n",
      "epoch : 339, loss = 0.07522391327591985\n",
      "epoch : 340, loss = 0.0751418047177569\n",
      "epoch : 341, loss = 0.07505801394207895\n",
      "epoch : 342, loss = 0.07497454190903685\n",
      "epoch : 343, loss = 0.07489522719704726\n",
      "epoch : 344, loss = 0.07481148962049712\n",
      "epoch : 345, loss = 0.07473601450861093\n",
      "epoch : 346, loss = 0.07465295568917817\n",
      "epoch : 347, loss = 0.07457564620614351\n",
      "epoch : 348, loss = 0.0744971666246102\n",
      "epoch : 349, loss = 0.07441784727724249\n",
      "epoch : 350, loss = 0.07434398407049629\n",
      "epoch : 351, loss = 0.07426501088141794\n",
      "epoch : 352, loss = 0.0741928835249349\n",
      "epoch : 353, loss = 0.07411558553180485\n",
      "epoch : 354, loss = 0.0740297346439331\n",
      "epoch : 355, loss = 0.07394654773823396\n",
      "epoch : 356, loss = 0.0738596785368116\n",
      "epoch : 357, loss = 0.07377952826947\n",
      "epoch : 358, loss = 0.07369423142807624\n",
      "epoch : 359, loss = 0.07361073691024916\n",
      "epoch : 360, loss = 0.07353104853478447\n",
      "epoch : 361, loss = 0.07344707371551543\n",
      "epoch : 362, loss = 0.07336872711048074\n",
      "epoch : 363, loss = 0.07328712839863152\n",
      "epoch : 364, loss = 0.07320510980628639\n",
      "epoch : 365, loss = 0.07312923933646812\n",
      "epoch : 366, loss = 0.07304777984006916\n",
      "epoch : 367, loss = 0.07297017254400373\n",
      "epoch : 368, loss = 0.07289254478477422\n",
      "epoch : 369, loss = 0.07281272630161505\n",
      "epoch : 370, loss = 0.07273987895610573\n",
      "epoch : 371, loss = 0.07266063579176323\n",
      "epoch : 372, loss = 0.07258300956978682\n",
      "epoch : 373, loss = 0.07250953135521737\n",
      "epoch : 374, loss = 0.07243186745195931\n",
      "epoch : 375, loss = 0.07235871240233872\n",
      "epoch : 376, loss = 0.0722840999260635\n",
      "epoch : 377, loss = 0.07220787806722465\n",
      "epoch : 378, loss = 0.07213854543283392\n",
      "epoch : 379, loss = 0.07206277055751656\n",
      "epoch : 380, loss = 0.07198893824532124\n",
      "epoch : 381, loss = 0.0719193214345321\n",
      "epoch : 382, loss = 0.0718451999458826\n",
      "epoch : 383, loss = 0.07177472408120265\n",
      "epoch : 384, loss = 0.07170407604448623\n",
      "epoch : 385, loss = 0.07163116207218145\n",
      "epoch : 386, loss = 0.07156384057468855\n",
      "epoch : 387, loss = 0.07149230857270958\n",
      "epoch : 388, loss = 0.07142052756156168\n",
      "epoch : 389, loss = 0.07135612863447205\n",
      "epoch : 390, loss = 0.07128381758313207\n",
      "epoch : 391, loss = 0.07121439354047551\n",
      "epoch : 392, loss = 0.07114920165976385\n",
      "epoch : 393, loss = 0.07108282615653287\n",
      "epoch : 394, loss = 0.07101798085536692\n",
      "epoch : 395, loss = 0.07095390984790474\n",
      "epoch : 396, loss = 0.07088697900480362\n",
      "epoch : 397, loss = 0.0708232332392897\n",
      "epoch : 398, loss = 0.07075948446242808\n",
      "epoch : 399, loss = 0.07069363717386187\n",
      "epoch : 400, loss = 0.07063142764550953\n",
      "epoch : 401, loss = 0.07056833402519196\n",
      "epoch : 402, loss = 0.07050359884110798\n",
      "epoch : 403, loss = 0.07044275282703606\n",
      "epoch : 404, loss = 0.07038040754963444\n",
      "epoch : 405, loss = 0.07031672578194458\n",
      "epoch : 406, loss = 0.0702570454610832\n",
      "epoch : 407, loss = 0.07019552686564266\n",
      "epoch : 408, loss = 0.07013283908679566\n",
      "epoch : 409, loss = 0.07007413458670655\n",
      "epoch : 410, loss = 0.07001351910686328\n",
      "epoch : 411, loss = 0.06995176993174837\n",
      "epoch : 412, loss = 0.0698938608112505\n",
      "epoch : 413, loss = 0.06980588861343823\n",
      "epoch : 414, loss = 0.06974047220615423\n",
      "epoch : 415, loss = 0.069682212241515\n",
      "epoch : 416, loss = 0.06961395236769811\n",
      "epoch : 417, loss = 0.0695532504546955\n",
      "epoch : 418, loss = 0.06948646839154562\n",
      "epoch : 419, loss = 0.06941069261255872\n",
      "epoch : 420, loss = 0.0693326189653322\n",
      "epoch : 421, loss = 0.06926268461768996\n",
      "epoch : 422, loss = 0.06919464519217333\n",
      "epoch : 423, loss = 0.06912775412823345\n",
      "epoch : 424, loss = 0.06906245144934368\n",
      "epoch : 425, loss = 0.06899605412023986\n",
      "epoch : 426, loss = 0.06893247479006223\n",
      "epoch : 427, loss = 0.06886585589593863\n",
      "epoch : 428, loss = 0.0688037332132448\n",
      "epoch : 429, loss = 0.0687364994981603\n",
      "epoch : 430, loss = 0.0686757689606172\n",
      "epoch : 431, loss = 0.06860806001557342\n",
      "epoch : 432, loss = 0.06854959423648228\n",
      "epoch : 433, loss = 0.06848077336775865\n",
      "epoch : 434, loss = 0.06842082597614246\n",
      "epoch : 435, loss = 0.06835312795811667\n",
      "epoch : 436, loss = 0.06829148821219343\n",
      "epoch : 437, loss = 0.068224878633459\n",
      "epoch : 438, loss = 0.06816130769909391\n",
      "epoch : 439, loss = 0.06809571429689255\n",
      "epoch : 440, loss = 0.06802992336726002\n",
      "epoch : 441, loss = 0.06796536733520184\n",
      "epoch : 442, loss = 0.06789733058160675\n",
      "epoch : 443, loss = 0.06783378701401167\n",
      "epoch : 444, loss = 0.06776367740624686\n",
      "epoch : 445, loss = 0.06770186923205387\n",
      "epoch : 446, loss = 0.06762983585473528\n",
      "epoch : 447, loss = 0.06756515283394826\n",
      "epoch : 448, loss = 0.06749405697204328\n",
      "epoch : 449, loss = 0.06742636969035062\n",
      "epoch : 450, loss = 0.06735623871749638\n",
      "epoch : 451, loss = 0.06728540088281476\n",
      "epoch : 452, loss = 0.06721624132946477\n",
      "epoch : 453, loss = 0.06714213099127839\n",
      "epoch : 454, loss = 0.0670743115735186\n",
      "epoch : 455, loss = 0.06699949704181546\n",
      "epoch : 456, loss = 0.06693077345378046\n",
      "epoch : 457, loss = 0.06685567215517307\n",
      "epoch : 458, loss = 0.06678308517388958\n",
      "epoch : 459, loss = 0.06670935854381599\n",
      "epoch : 460, loss = 0.06663273967759034\n",
      "epoch : 461, loss = 0.06656041401929276\n",
      "epoch : 462, loss = 0.06648148846595583\n",
      "epoch : 463, loss = 0.0664085757049311\n",
      "epoch : 464, loss = 0.06632860416474061\n",
      "epoch : 465, loss = 0.06625135634489607\n",
      "epoch : 466, loss = 0.0661729351574059\n",
      "epoch : 467, loss = 0.06609130814002268\n",
      "epoch : 468, loss = 0.06601453087099167\n",
      "epoch : 469, loss = 0.0659309600341457\n",
      "epoch : 470, loss = 0.06585255186100675\n",
      "epoch : 471, loss = 0.06576857887342913\n",
      "epoch : 472, loss = 0.06568560845255951\n",
      "epoch : 473, loss = 0.06560350765428001\n",
      "epoch : 474, loss = 0.0655166936608912\n",
      "epoch : 475, loss = 0.06543666460311988\n",
      "epoch : 476, loss = 0.06534744380175432\n",
      "epoch : 477, loss = 0.0652614169841061\n",
      "epoch : 478, loss = 0.06517327285352346\n",
      "epoch : 479, loss = 0.06508259083085291\n",
      "epoch : 480, loss = 0.06500256221366166\n",
      "epoch : 481, loss = 0.06491956972621507\n",
      "epoch : 482, loss = 0.06483915860877972\n",
      "epoch : 483, loss = 0.06475575197290215\n",
      "epoch : 484, loss = 0.06466978094437217\n",
      "epoch : 485, loss = 0.06458982466110827\n",
      "epoch : 486, loss = 0.06450192539072812\n",
      "epoch : 487, loss = 0.0644161542649702\n",
      "epoch : 488, loss = 0.06433214168164657\n",
      "epoch : 489, loss = 0.0642437598056454\n",
      "epoch : 490, loss = 0.06415909612408598\n",
      "epoch : 491, loss = 0.06407141905731305\n",
      "epoch : 492, loss = 0.06398197823425798\n",
      "epoch : 493, loss = 0.06389863277890453\n",
      "epoch : 494, loss = 0.06380779868195038\n",
      "epoch : 495, loss = 0.06371872998531813\n",
      "epoch : 496, loss = 0.06363230462088848\n",
      "epoch : 497, loss = 0.06354134511495285\n",
      "epoch : 498, loss = 0.06345336511046003\n",
      "epoch : 499, loss = 0.0633640204024836\n",
      "epoch : 500, loss = 0.06327225576019677\n",
      "epoch : 501, loss = 0.06318537457375945\n",
      "epoch : 502, loss = 0.06309339585225734\n",
      "epoch : 503, loss = 0.06300096213311583\n",
      "epoch : 504, loss = 0.06291517671439367\n",
      "epoch : 505, loss = 0.0628208283210148\n",
      "epoch : 506, loss = 0.06272936230671088\n",
      "epoch : 507, loss = 0.06264004441776215\n",
      "epoch : 508, loss = 0.0625525321104336\n",
      "epoch : 509, loss = 0.06244722993424314\n",
      "epoch : 510, loss = 0.06234807687596541\n",
      "epoch : 511, loss = 0.06227103103788613\n",
      "epoch : 512, loss = 0.06221240722460111\n",
      "epoch : 513, loss = 0.0621570704095782\n",
      "epoch : 514, loss = 0.062098029415587334\n",
      "epoch : 515, loss = 0.062045427649211934\n",
      "epoch : 516, loss = 0.06198677662642488\n",
      "epoch : 517, loss = 0.06193091709601522\n",
      "epoch : 518, loss = 0.0618761433224242\n",
      "epoch : 519, loss = 0.061817933441854066\n",
      "epoch : 520, loss = 0.06176635273836973\n",
      "epoch : 521, loss = 0.061707702048986145\n",
      "epoch : 522, loss = 0.06165258681655664\n",
      "epoch : 523, loss = 0.061597856598225156\n",
      "epoch : 524, loss = 0.06154006295064137\n",
      "epoch : 525, loss = 0.06148899863271862\n",
      "epoch : 526, loss = 0.061430582537266756\n",
      "epoch : 527, loss = 0.061375886217038864\n",
      "epoch : 528, loss = 0.06132151348787269\n",
      "epoch : 529, loss = 0.06126415881309293\n",
      "epoch : 530, loss = 0.061213350809055725\n",
      "epoch : 531, loss = 0.06115549920093394\n",
      "epoch : 532, loss = 0.06110097526529265\n",
      "epoch : 533, loss = 0.06104725294617902\n",
      "epoch : 534, loss = 0.0609901768417338\n",
      "epoch : 535, loss = 0.06093920214992477\n",
      "epoch : 536, loss = 0.06088205643617229\n",
      "epoch : 537, loss = 0.06082729598193858\n",
      "epoch : 538, loss = 0.06077443438599302\n",
      "epoch : 539, loss = 0.06071790454733478\n",
      "epoch : 540, loss = 0.060666738526763554\n",
      "epoch : 541, loss = 0.0606107907150709\n",
      "epoch : 542, loss = 0.0605557550878946\n",
      "epoch : 543, loss = 0.06050418872380195\n",
      "epoch : 544, loss = 0.06044821174512584\n",
      "epoch : 545, loss = 0.06039663187422117\n",
      "epoch : 546, loss = 0.0603421521845132\n",
      "epoch : 547, loss = 0.06028663327788398\n",
      "epoch : 548, loss = 0.06023663080079949\n",
      "epoch : 549, loss = 0.06018123638700471\n",
      "epoch : 550, loss = 0.060129043757463564\n",
      "epoch : 551, loss = 0.06007629278647352\n",
      "epoch : 552, loss = 0.060021241022854034\n",
      "epoch : 553, loss = 0.05997234455971725\n",
      "epoch : 554, loss = 0.05991697194540544\n",
      "epoch : 555, loss = 0.05986403626395904\n",
      "epoch : 556, loss = 0.05981328853254767\n",
      "epoch : 557, loss = 0.059758878469589445\n",
      "epoch : 558, loss = 0.05970906991273838\n",
      "epoch : 559, loss = 0.05965582596762574\n",
      "epoch : 560, loss = 0.05960191123133554\n",
      "epoch : 561, loss = 0.0595533689690741\n",
      "epoch : 562, loss = 0.05949960744434434\n",
      "epoch : 563, loss = 0.0594487004972831\n",
      "epoch : 564, loss = 0.05939780369539355\n",
      "epoch : 565, loss = 0.0593444194428405\n",
      "epoch : 566, loss = 0.05929649663568831\n",
      "epoch : 567, loss = 0.0592433597890558\n",
      "epoch : 568, loss = 0.059191241615606204\n",
      "epoch : 569, loss = 0.05914293122203472\n",
      "epoch : 570, loss = 0.05909024104614021\n",
      "epoch : 571, loss = 0.05904094830626511\n",
      "epoch : 572, loss = 0.05899050418176532\n",
      "epoch : 573, loss = 0.05893820786575084\n",
      "epoch : 574, loss = 0.05889171824850335\n",
      "epoch : 575, loss = 0.058839247583808936\n",
      "epoch : 576, loss = 0.05878847306337807\n",
      "epoch : 577, loss = 0.058740938810275804\n",
      "epoch : 578, loss = 0.05868935889137878\n",
      "epoch : 579, loss = 0.058641232695296225\n",
      "epoch : 580, loss = 0.05859176750974231\n",
      "epoch : 581, loss = 0.0585405943869097\n",
      "epoch : 582, loss = 0.05849509806049568\n",
      "epoch : 583, loss = 0.05844380478847674\n",
      "epoch : 584, loss = 0.05839393885092994\n",
      "epoch : 585, loss = 0.058347681933573675\n",
      "epoch : 586, loss = 0.05829724453698675\n",
      "epoch : 587, loss = 0.05824985969340706\n",
      "epoch : 588, loss = 0.05820186241901881\n",
      "epoch : 589, loss = 0.058151843247024666\n",
      "epoch : 590, loss = 0.05810690652135532\n",
      "epoch : 591, loss = 0.05805724477926349\n",
      "epoch : 592, loss = 0.05800784370129386\n",
      "epoch : 593, loss = 0.05796330409703257\n",
      "epoch : 594, loss = 0.05791400090292955\n",
      "epoch : 595, loss = 0.05786691427905638\n",
      "epoch : 596, loss = 0.05782081394821141\n",
      "epoch : 597, loss = 0.05777193338215488\n",
      "epoch : 598, loss = 0.057727148641001175\n",
      "epoch : 599, loss = 0.057679581539222737\n",
      "epoch : 600, loss = 0.05763114008313407\n",
      "epoch : 601, loss = 0.05758860335652026\n",
      "epoch : 602, loss = 0.05754847128077453\n",
      "epoch : 603, loss = 0.057512349661557306\n",
      "epoch : 604, loss = 0.05747968815349566\n",
      "epoch : 605, loss = 0.057443162656466026\n",
      "epoch : 606, loss = 0.05740981551983037\n",
      "epoch : 607, loss = 0.05737566600529805\n",
      "epoch : 608, loss = 0.05733964262996548\n",
      "epoch : 609, loss = 0.05730874367994031\n",
      "epoch : 610, loss = 0.05727299707057879\n",
      "epoch : 611, loss = 0.05723767205428491\n",
      "epoch : 612, loss = 0.0572069712113362\n",
      "epoch : 613, loss = 0.057171629115994334\n",
      "epoch : 614, loss = 0.05713854567990661\n",
      "epoch : 615, loss = 0.057106232724840715\n",
      "epoch : 616, loss = 0.057071232075382276\n",
      "epoch : 617, loss = 0.05704036094116382\n",
      "epoch : 618, loss = 0.05700648643829325\n",
      "epoch : 619, loss = 0.05697181980355008\n",
      "epoch : 620, loss = 0.05694310047885032\n",
      "epoch : 621, loss = 0.05690769858371298\n",
      "epoch : 622, loss = 0.05687442771696841\n",
      "epoch : 623, loss = 0.056844093024168235\n",
      "epoch : 624, loss = 0.05680997339384669\n",
      "epoch : 625, loss = 0.056778708019321444\n",
      "epoch : 626, loss = 0.05674686479701909\n",
      "epoch : 627, loss = 0.05671302454982546\n",
      "epoch : 628, loss = 0.0566730569770454\n",
      "epoch : 629, loss = 0.056538571102700635\n",
      "epoch : 630, loss = 0.05640220675180922\n",
      "epoch : 631, loss = 0.05627213393706762\n",
      "epoch : 632, loss = 0.0561362502230251\n",
      "epoch : 633, loss = 0.056001467718969376\n",
      "epoch : 634, loss = 0.0558706476546265\n",
      "epoch : 635, loss = 0.05573475360541691\n",
      "epoch : 636, loss = 0.055601844078005176\n",
      "epoch : 637, loss = 0.05546968941510868\n",
      "epoch : 638, loss = 0.0553340764976641\n",
      "epoch : 639, loss = 0.05520299162136501\n",
      "epoch : 640, loss = 0.055069545150487786\n",
      "epoch : 641, loss = 0.05493421058806753\n",
      "epoch : 642, loss = 0.05480490263010613\n",
      "epoch : 643, loss = 0.05467020673271425\n",
      "epoch : 644, loss = 0.05453612188897861\n",
      "epoch : 645, loss = 0.054406543771847615\n",
      "epoch : 646, loss = 0.054271662735191226\n",
      "epoch : 647, loss = 0.05413928551577229\n",
      "epoch : 648, loss = 0.05400851879602479\n",
      "epoch : 649, loss = 0.05387390920295008\n",
      "epoch : 650, loss = 0.053743193328450393\n",
      "epoch : 651, loss = 0.05361127935895905\n",
      "epoch : 652, loss = 0.05347693874704399\n",
      "epoch : 653, loss = 0.053347838497550586\n",
      "epoch : 654, loss = 0.05321481823194336\n",
      "epoch : 655, loss = 0.053080879768389565\n",
      "epoch : 656, loss = 0.05295302699881389\n",
      "epoch : 657, loss = 0.05281912499316472\n",
      "epoch : 658, loss = 0.0526867362751938\n",
      "epoch : 659, loss = 0.05255783549958205\n",
      "epoch : 660, loss = 0.05242419628439735\n",
      "epoch : 661, loss = 0.05229331296969405\n",
      "epoch : 662, loss = 0.052163404100233096\n",
      "epoch : 663, loss = 0.052030025455698\n",
      "epoch : 664, loss = 0.05190060373283915\n",
      "epoch : 665, loss = 0.05176972627057967\n",
      "epoch : 666, loss = 0.051636606033401686\n",
      "epoch : 667, loss = 0.05150860259950049\n",
      "epoch : 668, loss = 0.051376795663039036\n",
      "epoch : 669, loss = 0.05124393174182902\n",
      "epoch : 670, loss = 0.05111730380014303\n",
      "epoch : 671, loss = 0.050984606116125486\n",
      "epoch : 672, loss = 0.050853052893456444\n",
      "epoch : 673, loss = 0.05072559403047662\n",
      "epoch : 674, loss = 0.050593148508959934\n",
      "epoch : 675, loss = 0.05046290991430933\n",
      "epoch : 676, loss = 0.05033461468079882\n",
      "epoch : 677, loss = 0.05020242019483117\n",
      "epoch : 678, loss = 0.05007345470780351\n",
      "epoch : 679, loss = 0.049944360831582754\n",
      "epoch : 680, loss = 0.04981241548825188\n",
      "epoch : 681, loss = 0.04968468208568891\n",
      "epoch : 682, loss = 0.04955482691399382\n",
      "epoch : 683, loss = 0.04942312888008403\n",
      "epoch : 684, loss = 0.049296587017672124\n",
      "epoch : 685, loss = 0.049165962966967854\n",
      "epoch : 686, loss = 0.04903432117044492\n",
      "epoch : 687, loss = 0.04890870651806432\n",
      "epoch : 688, loss = 0.04877726993663791\n",
      "epoch : 689, loss = 0.048646622667539434\n",
      "epoch : 690, loss = 0.048520522837730354\n",
      "epoch : 691, loss = 0.048389287003743106\n",
      "epoch : 692, loss = 0.048259685339746075\n",
      "epoch : 693, loss = 0.0481330040322787\n",
      "epoch : 694, loss = 0.04800201209748474\n",
      "epoch : 695, loss = 0.047873417936223836\n",
      "epoch : 696, loss = 0.04774619009839444\n",
      "epoch : 697, loss = 0.04761544052336551\n",
      "epoch : 698, loss = 0.0474878162439267\n",
      "epoch : 699, loss = 0.04736007654212654\n",
      "epoch : 700, loss = 0.04722956789516605\n",
      "epoch : 701, loss = 0.0471028763597713\n",
      "epoch : 702, loss = 0.046974659207171074\n",
      "epoch : 703, loss = 0.04684439018534079\n",
      "epoch : 704, loss = 0.04671859476043207\n",
      "epoch : 705, loss = 0.04658993434879852\n",
      "epoch : 706, loss = 0.04645990380897146\n",
      "epoch : 707, loss = 0.046334968396469854\n",
      "epoch : 708, loss = 0.04620589873868268\n",
      "epoch : 709, loss = 0.04607610574142512\n",
      "epoch : 710, loss = 0.045951994823458304\n",
      "epoch : 711, loss = 0.04582254981108163\n",
      "epoch : 712, loss = 0.04569332957710378\n",
      "epoch : 713, loss = 0.045569285851897144\n",
      "epoch : 714, loss = 0.04543988336466447\n",
      "epoch : 715, loss = 0.045311440106730515\n",
      "epoch : 716, loss = 0.04518706729608742\n",
      "epoch : 717, loss = 0.045057901387794565\n",
      "epoch : 718, loss = 0.04493020151896988\n",
      "epoch : 719, loss = 0.044805533626537664\n",
      "epoch : 720, loss = 0.04467660480345617\n",
      "epoch : 721, loss = 0.04454961569915716\n",
      "epoch : 722, loss = 0.04442468704258705\n",
      "epoch : 723, loss = 0.044295996550133346\n",
      "epoch : 724, loss = 0.04416968672961613\n",
      "epoch : 725, loss = 0.0440445321870507\n",
      "epoch : 726, loss = 0.043917712228790234\n",
      "epoch : 727, loss = 0.04379409219951167\n",
      "epoch : 728, loss = 0.04367122236267642\n",
      "epoch : 729, loss = 0.04354550213111531\n",
      "epoch : 730, loss = 0.04342132399983043\n",
      "epoch : 731, loss = 0.043299554270829016\n",
      "epoch : 732, loss = 0.04317410635080868\n",
      "epoch : 733, loss = 0.04304932900366071\n",
      "epoch : 734, loss = 0.04292867745059377\n",
      "epoch : 735, loss = 0.04280350187967455\n",
      "epoch : 736, loss = 0.04267842226742273\n",
      "epoch : 737, loss = 0.042558326128612926\n",
      "epoch : 738, loss = 0.04243370170747997\n",
      "epoch : 739, loss = 0.04230890210042752\n",
      "epoch : 740, loss = 0.04218814338222243\n",
      "epoch : 741, loss = 0.042064725971388134\n",
      "epoch : 742, loss = 0.041940216968540445\n",
      "epoch : 743, loss = 0.04181877984501799\n",
      "epoch : 744, loss = 0.041696611003322476\n",
      "epoch : 745, loss = 0.04157240822021215\n",
      "epoch : 746, loss = 0.04145028170141656\n",
      "epoch : 747, loss = 0.041329409005757975\n",
      "epoch : 748, loss = 0.041205534146647654\n",
      "epoch : 749, loss = 0.041082713686294826\n",
      "epoch : 750, loss = 0.04096319231118711\n",
      "epoch : 751, loss = 0.040839674946088846\n",
      "epoch : 752, loss = 0.04071628951245755\n",
      "epoch : 753, loss = 0.040597983801253186\n",
      "epoch : 754, loss = 0.040474941311873\n",
      "epoch : 755, loss = 0.04035196867078551\n",
      "epoch : 756, loss = 0.04023298195553261\n",
      "epoch : 757, loss = 0.04011147293478093\n",
      "epoch : 758, loss = 0.0399889700448707\n",
      "epoch : 759, loss = 0.0398693350465069\n",
      "epoch : 760, loss = 0.03974945635750882\n",
      "epoch : 761, loss = 0.039627496977999874\n",
      "epoch : 762, loss = 0.03955498380876506\n",
      "epoch : 763, loss = 0.039472855843566294\n",
      "epoch : 764, loss = 0.03935038465900806\n",
      "epoch : 765, loss = 0.03922842193814436\n",
      "epoch : 766, loss = 0.03911062827643751\n",
      "epoch : 767, loss = 0.039012175522186894\n",
      "epoch : 768, loss = 0.03896109872883299\n",
      "epoch : 769, loss = 0.03883977523419901\n",
      "epoch : 770, loss = 0.03872157214214228\n",
      "epoch : 771, loss = 0.038599978443657805\n",
      "epoch : 772, loss = 0.038517442551042363\n",
      "epoch : 773, loss = 0.03846105027002392\n",
      "epoch : 774, loss = 0.03833826915024433\n",
      "epoch : 775, loss = 0.0382161649735399\n",
      "epoch : 776, loss = 0.038097887331826445\n",
      "epoch : 777, loss = 0.03806363505602959\n",
      "epoch : 778, loss = 0.03796080689164048\n",
      "epoch : 779, loss = 0.03783897018112901\n",
      "epoch : 780, loss = 0.037720858602583424\n",
      "epoch : 781, loss = 0.0376553727795901\n",
      "epoch : 782, loss = 0.03758683277909479\n",
      "epoch : 783, loss = 0.03746607234107187\n",
      "epoch : 784, loss = 0.037344223212515584\n",
      "epoch : 785, loss = 0.0372767899856003\n",
      "epoch : 786, loss = 0.03721747112315408\n",
      "epoch : 787, loss = 0.03709380816161191\n",
      "epoch : 788, loss = 0.0369693081653401\n",
      "epoch : 789, loss = 0.03692585438498505\n",
      "epoch : 790, loss = 0.03684909552005564\n",
      "epoch : 791, loss = 0.03672231479705729\n",
      "epoch : 792, loss = 0.03659848852381754\n",
      "epoch : 793, loss = 0.03658898709267728\n",
      "epoch : 794, loss = 0.03647972225220125\n",
      "epoch : 795, loss = 0.03635304870266472\n",
      "epoch : 796, loss = 0.03627197131771867\n",
      "epoch : 797, loss = 0.03623475958866342\n",
      "epoch : 798, loss = 0.036107372871332116\n",
      "epoch : 799, loss = 0.035985837584594754\n",
      "epoch : 800, loss = 0.035971232997306174\n",
      "epoch : 801, loss = 0.03586483593349979\n",
      "epoch : 802, loss = 0.03574165397725107\n",
      "epoch : 803, loss = 0.03567861783906384\n",
      "epoch : 804, loss = 0.03562313935104656\n",
      "epoch : 805, loss = 0.035498587342791176\n",
      "epoch : 806, loss = 0.03539694877517214\n",
      "epoch : 807, loss = 0.035382278090180866\n",
      "epoch : 808, loss = 0.03525655660915299\n",
      "epoch : 809, loss = 0.03513394900797992\n",
      "epoch : 810, loss = 0.03513030021300886\n",
      "epoch : 811, loss = 0.03501781027838866\n",
      "epoch : 812, loss = 0.034895463000067455\n",
      "epoch : 813, loss = 0.034862287784983494\n",
      "epoch : 814, loss = 0.03477991642728788\n",
      "epoch : 815, loss = 0.03465800218862546\n",
      "epoch : 816, loss = 0.03460267740030091\n",
      "epoch : 817, loss = 0.03454301035698526\n",
      "epoch : 818, loss = 0.03442162974343422\n",
      "epoch : 819, loss = 0.03435061065516578\n",
      "epoch : 820, loss = 0.03430768123746708\n",
      "epoch : 821, loss = 0.03418599424255906\n",
      "epoch : 822, loss = 0.03410530712332153\n",
      "epoch : 823, loss = 0.03407354337048193\n",
      "epoch : 824, loss = 0.033951596157309576\n",
      "epoch : 825, loss = 0.03386618957472222\n",
      "epoch : 826, loss = 0.03384065647833837\n",
      "epoch : 827, loss = 0.033718647832106326\n",
      "epoch : 828, loss = 0.03363268563872895\n",
      "epoch : 829, loss = 0.0336091650064499\n",
      "epoch : 830, loss = 0.03348729128643192\n",
      "epoch : 831, loss = 0.03340428353326398\n",
      "epoch : 832, loss = 0.0333792261085858\n",
      "epoch : 833, loss = 0.033257680632757705\n",
      "epoch : 834, loss = 0.033180523654182094\n",
      "epoch : 835, loss = 0.03315100592138562\n",
      "epoch : 836, loss = 0.03302997761197221\n",
      "epoch : 837, loss = 0.03296099322803348\n",
      "epoch : 838, loss = 0.032924675392613026\n",
      "epoch : 839, loss = 0.03280434706836974\n",
      "epoch : 840, loss = 0.032745321431289096\n",
      "epoch : 841, loss = 0.032700405725014606\n",
      "epoch : 842, loss = 0.032580952499515745\n",
      "epoch : 843, loss = 0.03253317665014686\n",
      "epoch : 844, loss = 0.03247836476774865\n",
      "epoch : 845, loss = 0.032359952209043\n",
      "epoch : 846, loss = 0.03232426455301641\n",
      "epoch : 847, loss = 0.03225871305744745\n",
      "epoch : 848, loss = 0.03214149547069966\n",
      "epoch : 849, loss = 0.032118326159070745\n",
      "epoch : 850, loss = 0.03204160033659784\n",
      "epoch : 851, loss = 0.03192571922983854\n",
      "epoch : 852, loss = 0.03191513645108535\n",
      "epoch : 853, loss = 0.0318271625343115\n",
      "epoch : 854, loss = 0.03171274537483226\n",
      "epoch : 855, loss = 0.031714503094114214\n",
      "epoch : 856, loss = 0.031615501432423296\n",
      "epoch : 857, loss = 0.03150467188718275\n",
      "epoch : 858, loss = 0.031518992480170135\n",
      "epoch : 859, loss = 0.031402734315122346\n",
      "epoch : 860, loss = 0.0313159968700914\n",
      "epoch : 861, loss = 0.03130769637836521\n",
      "epoch : 862, loss = 0.031193693745063603\n",
      "epoch : 863, loss = 0.031128611625500492\n",
      "epoch : 864, loss = 0.031099916744961154\n",
      "epoch : 865, loss = 0.030988094172305188\n",
      "epoch : 866, loss = 0.03094265094220796\n",
      "epoch : 867, loss = 0.030895574026020636\n",
      "epoch : 868, loss = 0.030785944647376953\n",
      "epoch : 869, loss = 0.03075811408496704\n",
      "epoch : 870, loss = 0.03069468019656823\n",
      "epoch : 871, loss = 0.030587249763842462\n",
      "epoch : 872, loss = 0.030575006027079788\n",
      "epoch : 873, loss = 0.03049723573873919\n",
      "epoch : 874, loss = 0.030391995794912346\n",
      "epoch : 875, loss = 0.030393350259834712\n",
      "epoch : 876, loss = 0.030303223241135876\n",
      "epoch : 877, loss = 0.030206655060320556\n",
      "epoch : 878, loss = 0.030209537250590588\n",
      "epoch : 879, loss = 0.030107890557650632\n",
      "epoch : 880, loss = 0.030035723023240774\n",
      "epoch : 881, loss = 0.030016256896631833\n",
      "epoch : 882, loss = 0.02991752398570956\n",
      "epoch : 883, loss = 0.029865179037367923\n",
      "epoch : 884, loss = 0.029827186551663887\n",
      "epoch : 885, loss = 0.029730802270854832\n",
      "epoch : 886, loss = 0.029695479010316824\n",
      "epoch : 887, loss = 0.029642005833030612\n",
      "epoch : 888, loss = 0.029547722019899577\n",
      "epoch : 889, loss = 0.029526693689790454\n",
      "epoch : 890, loss = 0.029460522546499758\n",
      "epoch : 891, loss = 0.029368220509119743\n",
      "epoch : 892, loss = 0.029358977693447624\n",
      "epoch : 893, loss = 0.0292826099455143\n",
      "epoch : 894, loss = 0.029192173131815716\n",
      "epoch : 895, loss = 0.029192475471608668\n",
      "epoch : 896, loss = 0.029108146328119873\n",
      "epoch : 897, loss = 0.029025645453670368\n",
      "epoch : 898, loss = 0.029025108203853577\n",
      "epoch : 899, loss = 0.028932447118819964\n",
      "epoch : 900, loss = 0.02886971902733473\n",
      "epoch : 901, loss = 0.028850795610531742\n",
      "epoch : 902, loss = 0.028761356959952487\n",
      "epoch : 903, loss = 0.028713953498113905\n",
      "epoch : 904, loss = 0.028680551698961786\n",
      "epoch : 905, loss = 0.028593986414551138\n",
      "epoch : 906, loss = 0.028558888364839906\n",
      "epoch : 907, loss = 0.02851395439786654\n",
      "epoch : 908, loss = 0.028430122356725946\n",
      "epoch : 909, loss = 0.02840475848360696\n",
      "epoch : 910, loss = 0.028350803088505874\n",
      "epoch : 911, loss = 0.028269589186424318\n",
      "epoch : 912, loss = 0.02825176000793967\n",
      "epoch : 913, loss = 0.028191140070818445\n",
      "epoch : 914, loss = 0.028111934157137353\n",
      "epoch : 915, loss = 0.028100146179959484\n",
      "epoch : 916, loss = 0.028034928092582696\n",
      "epoch : 917, loss = 0.0279569943044611\n",
      "epoch : 918, loss = 0.02794997108922899\n",
      "epoch : 919, loss = 0.027881730515649635\n",
      "epoch : 920, loss = 0.027804938509497522\n",
      "epoch : 921, loss = 0.027801374643745422\n",
      "epoch : 922, loss = 0.02773141484002351\n",
      "epoch : 923, loss = 0.027657232679119778\n",
      "epoch : 924, loss = 0.02765687212536376\n",
      "epoch : 925, loss = 0.027579448092967612\n",
      "epoch : 926, loss = 0.02751912095524357\n",
      "epoch : 927, loss = 0.027505958162130646\n",
      "epoch : 928, loss = 0.027431659018624585\n",
      "epoch : 929, loss = 0.027381392183600885\n",
      "epoch : 930, loss = 0.0273593824000266\n",
      "epoch : 931, loss = 0.027286314510039378\n",
      "epoch : 932, loss = 0.027244524647120433\n",
      "epoch : 933, loss = 0.02721597504711669\n",
      "epoch : 934, loss = 0.02714394931340563\n",
      "epoch : 935, loss = 0.027108825463028712\n",
      "epoch : 936, loss = 0.02707550379870401\n",
      "epoch : 937, loss = 0.027004379331685407\n",
      "epoch : 938, loss = 0.026975308442339255\n",
      "epoch : 939, loss = 0.02693774583121709\n",
      "epoch : 940, loss = 0.026867460219731586\n",
      "epoch : 941, loss = 0.026843232191965702\n",
      "epoch : 942, loss = 0.0268017729589663\n",
      "epoch : 943, loss = 0.026733071041428746\n",
      "epoch : 944, loss = 0.026712684265841377\n",
      "epoch : 945, loss = 0.026668304008991034\n",
      "epoch : 946, loss = 0.026601495668167736\n",
      "epoch : 947, loss = 0.026582582705346212\n",
      "epoch : 948, loss = 0.026537222193231316\n",
      "epoch : 949, loss = 0.02647278365344166\n",
      "epoch : 950, loss = 0.026454433005137685\n",
      "epoch : 951, loss = 0.026408444791862093\n",
      "epoch : 952, loss = 0.026346318529993654\n",
      "epoch : 953, loss = 0.026327999898744042\n",
      "epoch : 954, loss = 0.02628221633967641\n",
      "epoch : 955, loss = 0.026221613143006667\n",
      "epoch : 956, loss = 0.026203391506673834\n",
      "epoch : 957, loss = 0.026158798719252495\n",
      "epoch : 958, loss = 0.02609834373792492\n",
      "epoch : 959, loss = 0.026080851738057013\n",
      "epoch : 960, loss = 0.026037833032920565\n",
      "epoch : 961, loss = 0.025977100443676408\n",
      "epoch : 962, loss = 0.025960903638778978\n",
      "epoch : 963, loss = 0.025917394099802163\n",
      "epoch : 964, loss = 0.025857823578841702\n",
      "epoch : 965, loss = 0.02584273890948493\n",
      "epoch : 966, loss = 0.025798903096484586\n",
      "epoch : 967, loss = 0.025741310129578393\n",
      "epoch : 968, loss = 0.025724847215481392\n",
      "epoch : 969, loss = 0.02568229738535836\n",
      "epoch : 970, loss = 0.025626928808640675\n",
      "epoch : 971, loss = 0.0256091249108411\n",
      "epoch : 972, loss = 0.025568064599233716\n",
      "epoch : 973, loss = 0.02551376911988593\n",
      "epoch : 974, loss = 0.025495206918977692\n",
      "epoch : 975, loss = 0.025456261966485233\n",
      "epoch : 976, loss = 0.02540175001191662\n",
      "epoch : 977, loss = 0.02538401785356334\n",
      "epoch : 978, loss = 0.02534577415527093\n",
      "epoch : 979, loss = 0.025291453533042355\n",
      "epoch : 980, loss = 0.025274745492346155\n",
      "epoch : 981, loss = 0.025236182467969253\n",
      "epoch : 982, loss = 0.025183855704171594\n",
      "epoch : 983, loss = 0.025165533977412554\n",
      "epoch : 984, loss = 0.02512824725006191\n",
      "epoch : 985, loss = 0.02507811201216095\n",
      "epoch : 986, loss = 0.025058326942254752\n",
      "epoch : 987, loss = 0.025023091584933325\n",
      "epoch : 988, loss = 0.02497274500945023\n",
      "epoch : 989, loss = 0.024953455654189678\n",
      "epoch : 990, loss = 0.02491946064096422\n",
      "epoch : 991, loss = 0.02486884957836788\n",
      "epoch : 992, loss = 0.024850785647160353\n",
      "epoch : 993, loss = 0.02481620779206095\n",
      "epoch : 994, loss = 0.0247675000464776\n",
      "epoch : 995, loss = 0.024748087635076875\n",
      "epoch : 996, loss = 0.024714590178628204\n",
      "epoch : 997, loss = 0.024667726969481673\n",
      "epoch : 998, loss = 0.02464718189187918\n",
      "epoch : 999, loss = 0.02461563309906765\n"
     ]
    }
   ],
   "source": [
    "model = DeepNeuralNet(\n",
    "    input_size=4,\n",
    "    hidden_size=16,\n",
    "    output_size=1,\n",
    "    Lambda=0.05,\n",
    "    learning_rate=0.001,\n",
    "    epoches=1000,\n",
    "    hidden_layer=4\n",
    ")\n",
    "\n",
    "model.fit(X_train,Y_train)\n",
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "276f59b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(343, 1)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test = np.array(Y_test).reshape(-1,1)\n",
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a38be393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       191\n",
      "           1       0.99      0.99      0.99       152\n",
      "\n",
      "    accuracy                           0.99       343\n",
      "   macro avg       0.99      0.99      0.99       343\n",
      "weighted avg       0.99      0.99      0.99       343\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "cmf = confusion_matrix(Y_test,preds)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(cmf,annot=True,fmt='d')\n",
    "plt.xlabel(\"True\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.show()\n",
    "print(classification_report(Y_test,preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07f3074",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
