{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "db8085a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c0631d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self,bias = 0,epochs=100,learning_rate=0.001):\n",
    "        self.bias = np.array([bias])\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def sigmoid(self,z):\n",
    "        return 1 / (1+np.exp(-z))\n",
    "\n",
    "    def log_loss(self, Y_preds, Y):\n",
    "        eps = 1e-15\n",
    "        Y_preds = np.clip(Y_preds, eps, 1 - eps)\n",
    "        p1 = Y * np.log(Y_preds)\n",
    "        p2 = (1 - Y) * np.log(1 - Y_preds)\n",
    "        loss = -np.mean(p1 + p2)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def fit(self,X,Y):\n",
    "        \"\"\"initialize all the variables\"\"\"\n",
    "        X = np.array(X)\n",
    "        m,n = X.shape # here n is number of parameters and m is number of samples \n",
    "        Y = np.array(Y).reshape(m,1)\n",
    "        self.weights = np.zeros(n) # weight vector with same number of elements as n (for each parameter 1 weight element)\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "\n",
    "            \"\"\"computation of linear combinations\"\"\"\n",
    "            z = np.dot(X,self.weights) + self.bias\n",
    "\n",
    "            \"\"\"computation of sigmoid \"\"\"\n",
    "            y_pred = self.sigmoid(z).reshape(m,1) # as we reshpaed Y we will have to convert it too \n",
    "\n",
    "            \"\"\"calculating error\"\"\"\n",
    "            print(f\"epoch : {_} , loss :{self.log_loss(y_pred,Y)}\")\n",
    "            error_in_data = y_pred - Y \n",
    "\n",
    "            \"\"\"computation of gradient\"\"\" \n",
    "            d_weight = (1 / m) * np.dot(X.T, error_in_data).flatten() \n",
    "            d_bias = (1 / m) * np.sum(error_in_data)\n",
    "\n",
    "            \"\"\"updating weight and bias \"\"\"\n",
    "            self.weights = self.weights - self.learning_rate * d_weight\n",
    "            self.bias = self.bias - self.learning_rate * d_bias\n",
    "\n",
    "    def predict(self,X):\n",
    "        z = np.dot(X,self.weights) + self.bias \n",
    "        return (self.sigmoid(z) >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1bb5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_rt = LogisticRegression(epochs=100)\n",
    "train = np.random.randn(3,4)\n",
    "test = [1,0,1]\n",
    "log_rt.fit(train,test)\n",
    "log_rt.predict(train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
